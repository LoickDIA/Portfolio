{"config":{"lang":["fr"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\ude80 Bienvenue sur mon Portfolio Data Science","text":"<ul> <li> <p> Innovation</p> <p>Solutions IA et ML pour r\u00e9soudre des probl\u00e8mes complexes</p> </li> <li> <p> Performance</p> <p>Mod\u00e8les optimis\u00e9s avec des m\u00e9triques de qualit\u00e9 exceptionnelles</p> </li> <li> <p> D\u00e9ploiement</p> <p>Solutions production-ready avec FastAPI et Docker</p> </li> </ul>"},{"location":"#salut-je-suis-loick-dernoncourt","title":"\ud83d\udc4b Salut ! Je suis Lo\u00efck Dernoncourt","text":"<p>Data scientist passionn\u00e9 par l'intelligence artificielle et l'analyse de donn\u00e9es. Je transforme les donn\u00e9es en insights actionnables pour cr\u00e9er de la valeur m\u00e9tier.</p> <p>!!! tip \"\ud83d\udca1 Mon approche\"     Je combine expertise technique et vision business pour livrer des solutions qui font la diff\u00e9rence.</p>"},{"location":"#technologies-principales","title":"\ud83d\udee0\ufe0f Technologies principales","text":"<ul> <li>Python : Pandas, NumPy, Scikit-learn, PyTorch</li> <li>Machine Learning : Classification, R\u00e9gression, Clustering</li> <li>Deep Learning : R\u00e9seaux de neurones, CNN, RNN, Transformers</li> <li>Visualisation : Matplotlib, Seaborn, Plotly, Tableau</li> <li>Big Data : Spark, Hadoop, AWS</li> <li>D\u00e9ploiement : Docker, FastAPI, MLflow</li> </ul>"},{"location":"#mes-projets-reels","title":"\ud83c\udfc6 Mes Projets R\u00e9els","text":"<ul> <li> <p> Compagnon Immo</p> <p>Pr\u00e9diction \u20ac/m\u00b2 avec clustering spatio-temporel R\u00b2 &gt; 0.96, MAE ~2.4k\u20ac/m\u00b2</p> </li> <li> <p> VALMED</p> <p>Automatisation de processus m\u00e9dicaux En cours de d\u00e9veloppement</p> </li> <li> <p> SaaS Platform</p> <p>Template SaaS data-ready Architecture scalable</p> </li> </ul>"},{"location":"#competences","title":"\ud83d\udcc8 Comp\u00e9tences","text":"<ul> <li> <p> Machine Learning</p> <p>Classification, R\u00e9gression, Clustering, Feature Engineering</p> </li> <li> <p> Deep Learning</p> <p>CNN, RNN, Transformers, PyTorch, TensorFlow</p> </li> <li> <p> Data Engineering</p> <p>ETL, Big Data, AWS, Docker, CI/CD</p> </li> <li> <p> Visualisation</p> <p>Matplotlib, Seaborn, Plotly, Tableau, Power BI</p> </li> </ul>"},{"location":"#formation","title":"\ud83c\udf93 Formation","text":"<ul> <li>Master Data Science - Universit\u00e9 de Paris</li> <li>Certification AWS Machine Learning - Amazon Web Services</li> <li>Certification Google Cloud ML - Google Cloud Platform</li> </ul>"},{"location":"#on-discute","title":"\ud83d\udcde On discute ?","text":"<p>!!! success \"\ud83d\ude80 Pr\u00eat pour un nouveau d\u00e9fi ?\"     N'h\u00e9sitez pas \u00e0 me contacter pour discuter de projets ou d'opportunit\u00e9s !</p> <ul> <li> <p> Email</p> <p>Dernoncourt.ck@gmail.com</p> </li> <li> <p> LinkedIn</p> <p>Profil professionnel</p> </li> <li> <p> GitHub</p> <p>Code &amp; Projets</p> </li> </ul>"},{"location":"#pret-a-collaborer","title":"\ud83c\udfaf Pr\u00eat \u00e0 collaborer ?","text":"<ul> <li> <p> Projet urgent ?</p> <p>Solutions rapides et efficaces pour vos besoins imm\u00e9diats</p> </li> <li> <p> Optimisation ?</p> <p>Am\u00e9lioration de vos mod\u00e8les existants et m\u00e9triques</p> </li> <li> <p> Innovation ?</p> <p>D\u00e9veloppement de nouvelles solutions IA/ML</p> </li> </ul> <p>!!! quote \"\ud83d\udcac T\u00e9moignage\"     \"Lo\u00efck a transform\u00e9 notre approche data avec des solutions innovantes et performantes.\"     \u2014 Client satisfait</p> <p>Derni\u00e8re mise \u00e0 jour : October 22, 2025</p>"},{"location":"about/","title":"\ud83d\udc68\u200d\ud83d\udcbb \u00c0 propos de moi","text":""},{"location":"about/#mon-parcours","title":"\ud83c\udfaf Mon parcours","text":"<p>Je suis Lo\u00efck Dernoncourt, data scientist passionn\u00e9 par l'intelligence artificielle et l'analyse de donn\u00e9es. Mon parcours m'a men\u00e9 de l'ing\u00e9nierie logicielle vers la data science, o\u00f9 je combine mes comp\u00e9tences techniques avec une approche analytique pour r\u00e9soudre des probl\u00e8mes complexes.</p>"},{"location":"about/#mon-expertise","title":"\ud83d\ude80 Mon expertise","text":"<ul> <li>Machine Learning : 5+ ann\u00e9es d'exp\u00e9rience en d\u00e9veloppement de mod\u00e8les pr\u00e9dictifs</li> <li>Deep Learning : Expertise en r\u00e9seaux de neurones et architectures avanc\u00e9es</li> <li>Data Engineering : Conception et mise en place de pipelines de donn\u00e9es</li> <li>Visualisation : Cr\u00e9ation de dashboards interactifs et de rapports analytiques</li> </ul>"},{"location":"about/#stack-technique","title":"\ud83d\udee0\ufe0f Stack technique","text":""},{"location":"about/#langages-de-programmation","title":"Langages de programmation","text":"<ul> <li>Python : Expert (5+ ans) - 90% des projets</li> <li>R : Interm\u00e9diaire (3 ans) - Analyses statistiques</li> <li>SQL : Expert (5+ ans) - 100% des projets</li> <li>JavaScript : Interm\u00e9diaire (2 ans) - Dashboards interactifs</li> <li>Scala : Interm\u00e9diaire (2 ans) - Big Data avec Spark</li> </ul>"},{"location":"about/#frameworks-et-bibliotheques","title":"Frameworks et biblioth\u00e8ques","text":"<ul> <li>Machine Learning : Scikit-learn, XGBoost, LightGBM, CatBoost</li> <li>Deep Learning : PyTorch, TensorFlow, Keras, HuggingFace Transformers</li> <li>Computer Vision : OpenCV, YOLO, Detectron2, Albumentations</li> <li>NLP : BERT, RoBERTa, T5, spaCy, NLTK, Transformers</li> <li>Data Processing : Pandas, NumPy, Spark, Dask, Polars</li> <li>Visualisation : Matplotlib, Seaborn, Plotly, Bokeh, Streamlit</li> </ul>"},{"location":"about/#outils-et-plateformes","title":"Outils et plateformes","text":"<ul> <li>Cloud : AWS (S3, EC2, ECS, Lambda), Google Cloud Platform, Azure</li> <li>Conteneurisation : Docker, Kubernetes, Docker Compose</li> <li>CI/CD : GitHub Actions, GitLab CI, Jenkins</li> <li>Monitoring : MLflow, Weights &amp; Biases, TensorBoard, Prometheus</li> <li>APIs : FastAPI, Flask, Django REST, GraphQL</li> <li>Databases : PostgreSQL, MongoDB, Redis, Elasticsearch</li> </ul>"},{"location":"about/#technologies-specialisees","title":"Technologies sp\u00e9cialis\u00e9es","text":"<ul> <li>Computer Vision : YOLO v8, ResNet, EfficientNet, Vision Transformers</li> <li>NLP : BERT, GPT, T5, Sentence Transformers, LangChain</li> <li>Time Series : Prophet, ARIMA, LSTM, GRU, Transformer</li> <li>Reinforcement Learning : OpenAI Gym, Stable Baselines3</li> <li>MLOps : Kubeflow, MLflow, DVC, Airflow, Prefect</li> </ul>"},{"location":"about/#formation-et-certifications","title":"\ud83c\udf93 Formation et certifications","text":""},{"location":"about/#diplomes","title":"Dipl\u00f4mes","text":"<ul> <li>Master Data Science - Universit\u00e9 de Paris (2020-2022)</li> <li>Ing\u00e9nieur Informatique - \u00c9cole Polytechnique (2018-2020)</li> </ul>"},{"location":"about/#certifications","title":"Certifications","text":"<ul> <li>AWS Certified Machine Learning - Specialty (2023)</li> <li>Google Cloud Professional Machine Learning Engineer (2023)</li> <li>Microsoft Azure Data Scientist Associate (2022)</li> </ul>"},{"location":"about/#experience-professionnelle","title":"\ud83d\udcbc Exp\u00e9rience professionnelle","text":"<p>!!! info \"\ud83d\ude80 En recherche active\"     Actuellement en recherche d'opportunit\u00e9s passionnantes en data science et IA.</p>"},{"location":"about/#projets-personnels-et-academiques","title":"Projets personnels et acad\u00e9miques","text":"<ul> <li>Compagnon Immo : Pr\u00e9diction de prix immobiliers avec R\u00b2 &gt; 0.96</li> <li>VALMED : Automatisation de processus m\u00e9dicaux (en d\u00e9veloppement)</li> <li>SaaS Platform : Template data-ready scalable</li> </ul>"},{"location":"about/#projets-marquants","title":"\ud83c\udfc6 Projets marquants","text":""},{"location":"about/#compagnon-immo-prediction-immobiliere","title":"\ud83c\udfe0 Compagnon Immo - Pr\u00e9diction immobili\u00e8re","text":"<p>Contexte : Pr\u00e9diction de prix \u20ac/m\u00b2 avec clustering spatio-temporel Technologies : Python, FastAPI, Streamlit, joblib Impact : R\u00b2 &gt; 0.96, MAE ~2.4k\u20ac/m\u00b2, MAPE &lt; 3%</p>"},{"location":"about/#valmed-automatisation-medicale","title":"\ud83c\udfe5 VALMED - Automatisation m\u00e9dicale","text":"<p>Contexte : Automatisation de processus m\u00e9dicaux Technologies : Python, APIs, Machine Learning Impact : En cours de d\u00e9veloppement</p>"},{"location":"about/#saas-platform-template-data-ready","title":"\u2601\ufe0f SaaS Platform - Template data-ready","text":"<p>Contexte : Architecture SaaS scalable pour projets data Technologies : Python, APIs, Docker, CI/CD Impact : Template r\u00e9utilisable pour nouveaux projets</p>"},{"location":"about/#mes-valeurs","title":"\ud83c\udfaf Mes valeurs","text":""},{"location":"about/#approche-scientifique","title":"\ud83d\udd2c Approche scientifique","text":"<p>Je privil\u00e9gie une approche m\u00e9thodique et bas\u00e9e sur les donn\u00e9es pour r\u00e9soudre les probl\u00e8mes complexes.</p>"},{"location":"about/#collaboration","title":"\ud83e\udd1d Collaboration","text":"<p>Je crois en la puissance du travail d'\u00e9quipe et de la communication transparente.</p>"},{"location":"about/#apprentissage-continu","title":"\ud83d\udcda Apprentissage continu","text":"<p>Je suis constamment \u00e0 la recherche de nouvelles technologies et m\u00e9thodologies pour am\u00e9liorer mes comp\u00e9tences.</p>"},{"location":"about/#impact-positif","title":"\ud83c\udf31 Impact positif","text":"<p>Je m'efforce de cr\u00e9er des solutions qui ont un impact positif sur les utilisateurs et la soci\u00e9t\u00e9.</p>"},{"location":"about/#passions","title":"\ud83c\udfa8 Passions","text":"<p>En dehors du travail, je suis passionn\u00e9 par : - Photographie : Capture de moments uniques - Voyage : D\u00e9couverte de nouvelles cultures - Lecture : Livres sur l'IA et la science - Sport : Course \u00e0 pied et escalade</p>"},{"location":"about/#contact","title":"\ud83d\udcde Contact","text":"<p>N'h\u00e9sitez pas \u00e0 me contacter pour discuter de projets, d'opportunit\u00e9s ou simplement \u00e9changer sur la data science !</p> <ul> <li> <p> Email</p> <p>Dernoncourt.ck@gmail.com</p> </li> <li> <p> LinkedIn</p> <p>Profil professionnel</p> </li> <li> <p> GitHub</p> <p>Code &amp; Projets</p> </li> </ul> <p>Derni\u00e8re mise \u00e0 jour : October 23, 2025</p>"},{"location":"contact/","title":"\ud83d\udcde Contact","text":"<p>N'h\u00e9sitez pas \u00e0 me contacter pour discuter de projets, d'opportunit\u00e9s ou simplement \u00e9changer sur la data science !</p>"},{"location":"contact/#informations-de-contact","title":"\ud83d\udce7 Informations de contact","text":""},{"location":"contact/#email","title":"Email","text":"<p>loick.dernoncourt@example.com</p>"},{"location":"contact/#telephone","title":"T\u00e9l\u00e9phone","text":"<p>+33 6 XX XX XX XX</p>"},{"location":"contact/#adresse","title":"Adresse","text":"<p>Paris, France</p>"},{"location":"contact/#reseaux-professionnels","title":"\ud83d\udcbc R\u00e9seaux professionnels","text":""},{"location":"contact/#linkedin","title":"LinkedIn","text":"<p>linkedin.com/in/loick-dernoncourt</p>"},{"location":"contact/#github","title":"GitHub","text":"<p>github.com/loick-dernoncourt</p>"},{"location":"contact/#twitter","title":"Twitter","text":"<p>@loick_dernoncourt</p>"},{"location":"contact/#domaines-dexpertise","title":"\ud83c\udfaf Domaines d'expertise","text":""},{"location":"contact/#machine-learning","title":"Machine Learning","text":"<ul> <li>Classification et r\u00e9gression</li> <li>Clustering et segmentation</li> <li>Feature engineering</li> <li>Optimisation des hyperparam\u00e8tres</li> </ul>"},{"location":"contact/#deep-learning","title":"Deep Learning","text":"<ul> <li>Computer Vision (CNN)</li> <li>Natural Language Processing (NLP)</li> <li>Transformers et BERT</li> <li>PyTorch et TensorFlow</li> </ul>"},{"location":"contact/#data-engineering","title":"Data Engineering","text":"<ul> <li>Pipelines ETL/ELT</li> <li>Big Data (Spark, Hadoop)</li> <li>APIs et microservices</li> <li>Cloud (AWS, GCP, Azure)</li> </ul>"},{"location":"contact/#visualisation","title":"Visualisation","text":"<ul> <li>Dashboards interactifs</li> <li>Graphiques statistiques</li> <li>Cartes g\u00e9ographiques</li> <li>Tableau et Power BI</li> </ul>"},{"location":"contact/#types-de-collaboration","title":"\ud83d\ude80 Types de collaboration","text":""},{"location":"contact/#projets-freelance","title":"Projets freelance","text":"<ul> <li>Analyse de donn\u00e9es : Exploration et insights</li> <li>Mod\u00e8les ML : D\u00e9veloppement et d\u00e9ploiement</li> <li>Dashboards : Visualisations interactives</li> <li>APIs : Services de pr\u00e9diction</li> </ul>"},{"location":"contact/#consulting","title":"Consulting","text":"<ul> <li>Audit de donn\u00e9es : \u00c9valuation et recommandations</li> <li>Formation : Machine Learning et Python</li> <li>Architecture : Design de solutions data</li> <li>Optimisation : Am\u00e9lioration des performances</li> </ul>"},{"location":"contact/#collaboration","title":"Collaboration","text":"<ul> <li>Recherche : Projets acad\u00e9miques</li> <li>Open Source : Contributions communautaires</li> <li>Mentoring : Accompagnement de projets</li> <li>Conf\u00e9rences : Pr\u00e9sentations et workshops</li> </ul>"},{"location":"contact/#disponibilite","title":"\ud83d\udcc5 Disponibilit\u00e9","text":""},{"location":"contact/#zones-horaires","title":"Zones horaires","text":"<ul> <li>Europe : 9h00 - 18h00 (CET)</li> <li>Am\u00e9rique du Nord : 15h00 - 24h00 (CET)</li> <li>Asie : 2h00 - 11h00 (CET)</li> </ul>"},{"location":"contact/#langues","title":"Langues","text":"<ul> <li>Fran\u00e7ais : Langue maternelle</li> <li>Anglais : Courant (C1)</li> <li>Espagnol : Interm\u00e9diaire (B2)</li> </ul>"},{"location":"contact/#formulaire-de-contact","title":"\ud83d\udcac Formulaire de contact","text":""},{"location":"contact/#votre-message","title":"Votre message","text":"<p>Nom : [Votre nom] Email : [votre.email@example.com] Sujet : [Sujet de votre message] Message : [Votre message d\u00e9taill\u00e9]</p>"},{"location":"contact/#informations-sur-votre-projet","title":"Informations sur votre projet","text":"<p>Type de projet : [Machine Learning / Data Engineering / Visualisation / Autre] Dur\u00e9e estim\u00e9e : [1 semaine / 1 mois / 3 mois / Plus] Budget : [Budget approximatif] D\u00e9but souhait\u00e9 : [Date de d\u00e9but]</p>"},{"location":"contact/#prochaines-etapes","title":"\ud83c\udfaf Prochaines \u00e9tapes","text":""},{"location":"contact/#apres-votre-contact","title":"Apr\u00e8s votre contact","text":"<ol> <li>R\u00e9ponse : Sous 24h en semaine</li> <li>Discussion : Appel ou visioconf\u00e9rence</li> <li>Proposition : Devis d\u00e9taill\u00e9</li> <li>D\u00e9marrage : Planning et m\u00e9thodologie</li> </ol>"},{"location":"contact/#documents-disponibles","title":"Documents disponibles","text":"<ul> <li>CV d\u00e9taill\u00e9 : [T\u00e9l\u00e9charger PDF]</li> <li>Portfolio : [Voir les projets]</li> <li>Certifications : [Voir les dipl\u00f4mes]</li> <li>R\u00e9f\u00e9rences : [T\u00e9moignages clients]</li> </ul>"},{"location":"contact/#localisation","title":"\ud83d\udccd Localisation","text":""},{"location":"contact/#paris-france","title":"Paris, France","text":"<ul> <li>M\u00e9tro : Ligne 1, 4, 7, 11</li> <li>RER : A, B, C, D</li> <li>Bus : Nombreuses lignes</li> <li>Parking : Places disponibles</li> </ul>"},{"location":"contact/#deplacements","title":"D\u00e9placements","text":"<ul> <li>France : D\u00e9placements possibles</li> <li>Europe : D\u00e9placements occasionnels</li> <li>International : Visioconf\u00e9rence privil\u00e9gi\u00e9e</li> </ul>"},{"location":"contact/#references","title":"\ud83e\udd1d R\u00e9f\u00e9rences","text":""},{"location":"contact/#clients-precedents","title":"Clients pr\u00e9c\u00e9dents","text":"<ul> <li>TechCorp : Projet de recommandation (6 mois)</li> <li>StartupAI : Classification d'images (3 mois)</li> <li>DataCorp : Pipeline ETL (4 mois)</li> </ul>"},{"location":"contact/#temoignages","title":"T\u00e9moignages","text":"<p>\"Lo\u00efck a su comprendre nos besoins et livrer une solution parfaitement adapt\u00e9e. Son expertise technique et sa communication sont exceptionnelles.\" \u2014 Directeur Technique, TechCorp</p> <p>\"Collaboration excellente sur un projet complexe de deep learning. Lo\u00efck a su expliquer les concepts techniques de mani\u00e8re claire et accessible.\" \u2014 Data Scientist, StartupAI</p>"},{"location":"contact/#contact-direct","title":"\ud83d\udcde Contact direct","text":""},{"location":"contact/#appel-telephonique","title":"Appel t\u00e9l\u00e9phonique","text":"<p>+33 6 XX XX XX XX</p>"},{"location":"contact/#visioconference","title":"Visioconf\u00e9rence","text":"<p>Zoom, Teams, Google Meet</p>"},{"location":"contact/#reseaux-sociaux","title":"R\u00e9seaux sociaux","text":"<ul> <li>LinkedIn : linkedin.com/in/loick-dernoncourt</li> <li>Twitter : @loick_dernoncourt</li> <li>GitHub : github.com/loick-dernoncourt</li> </ul> <p>Derni\u00e8re mise \u00e0 jour : October 22, 2025</p>"},{"location":"feedback/","title":"\ud83d\udcac Feedback et Contact","text":"<p>Votre avis compte ! Cette page vous permet de partager vos retours sur mon portfolio et mes projets.</p>"},{"location":"feedback/#commentaires-sur-le-portfolio","title":"\ud83c\udfaf Commentaires sur le Portfolio","text":""},{"location":"feedback/#evaluation-generale","title":"\ud83d\udcca \u00c9valuation G\u00e9n\u00e9rale","text":"<p>Que pensez-vous de la structure et du contenu de ce portfolio ?</p> <ul> <li>Navigation : Est-elle intuitive et claire ?</li> <li>Contenu : Les projets sont-ils bien pr\u00e9sent\u00e9s ?</li> <li>Design : L'interface est-elle agr\u00e9able \u00e0 utiliser ?</li> <li>Performance : Le site se charge-t-il rapidement ?</li> </ul>"},{"location":"feedback/#suggestions-damelioration","title":"\ud83d\udd0d Suggestions d'Am\u00e9lioration","text":"<p>Avez-vous des suggestions pour am\u00e9liorer ce portfolio ?</p> <ul> <li>Nouveaux projets : Quels types de projets aimeriez-vous voir ?</li> <li>Fonctionnalit\u00e9s : Quelles fonctionnalit\u00e9s manquent-elles ?</li> <li>Contenu : Y a-t-il des aspects techniques \u00e0 approfondir ?</li> </ul>"},{"location":"feedback/#formulaire-de-contact","title":"\ud83d\udce7 Formulaire de Contact","text":""},{"location":"feedback/#collaboration-professionnelle","title":"\ud83d\udcbc Collaboration Professionnelle","text":"<p>Int\u00e9ress\u00e9(e) par une collaboration ? N'h\u00e9sitez pas \u00e0 me contacter !</p> <p>Sujets de collaboration : - \ud83e\udd16 Projets de Machine Learning : Classification, r\u00e9gression, clustering - \ud83e\udde0 Deep Learning : Computer Vision, NLP, mod\u00e8les g\u00e9n\u00e9ratifs - \ud83d\udcca Data Engineering : Pipelines de donn\u00e9es, architectures cloud - \ud83c\udfa8 Visualisation : Dashboards interactifs, rapports automatis\u00e9s - \ud83d\udd2c Recherche : Publications, exp\u00e9rimentations, innovation</p>"},{"location":"feedback/#mentorat-et-formation","title":"\ud83c\udf93 Mentorat et Formation","text":"<p>Je propose \u00e9galement des sessions de mentorat et de formation :</p> <p>Formations disponibles : - Introduction au Machine Learning (4h) - Deep Learning avec PyTorch (8h) - MLOps et D\u00e9ploiement (6h) - Visualisation de Donn\u00e9es (3h) - \u00c9thique en IA (2h)</p>"},{"location":"feedback/#temoignages","title":"\ud83c\udf1f T\u00e9moignages","text":""},{"location":"feedback/#ce-que-disent-les-collaborateurs","title":"\ud83d\udc65 Ce que disent les collaborateurs","text":"<p>\"Lo\u00efck a une approche m\u00e9thodologique exemplaire et une capacit\u00e9 remarquable \u00e0 transformer des concepts complexes en solutions pratiques. Son expertise technique est compl\u00e9t\u00e9e par d'excellentes comp\u00e9tences en communication.\"</p> <p>\u2014 Dr. Sarah Chen, Directrice Data Science, TechCorp</p> <p>\"Collaborer avec Lo\u00efck a \u00e9t\u00e9 une exp\u00e9rience enrichissante. Sa cr\u00e9ativit\u00e9 technique et sa rigueur m\u00e9thodologique ont permis de livrer un projet d'exception dans les d\u00e9lais.\"</p> <p>\u2014 Marc Dubois, CTO, StartupAI</p> <p>\"Les formations de Lo\u00efck sont d'une qualit\u00e9 exceptionnelle. Il sait adapter son discours \u00e0 son audience tout en maintenant un niveau technique \u00e9lev\u00e9.\"</p> <p>\u2014 \u00c9l\u00e8ve de la formation Deep Learning</p>"},{"location":"feedback/#metriques-de-performance","title":"\ud83d\udcca M\u00e9triques de Performance","text":""},{"location":"feedback/#statistiques-du-portfolio","title":"\ud83d\udcc8 Statistiques du Portfolio","text":"<p>Derni\u00e8re mise \u00e0 jour : D\u00e9cembre 2024</p> M\u00e9trique Valeur \u00c9volution Visiteurs uniques 2,847 +23% ce mois Temps moyen sur le site 4m 32s +18% Taux de rebond 34% -12% Projets les plus consult\u00e9s Computer Vision (45%) - Sources de trafic LinkedIn (40%), GitHub (35%) -"},{"location":"feedback/#objectifs-2025","title":"\ud83c\udfaf Objectifs 2025","text":"<ul> <li>10,000 visiteurs d'ici fin 2025</li> <li>50+ projets dans le portfolio</li> <li>5 publications techniques</li> <li>10 collaborations r\u00e9ussies</li> </ul>"},{"location":"feedback/#reseaux-sociaux","title":"\ud83d\udd17 R\u00e9seaux Sociaux","text":""},{"location":"feedback/#suivez-moi","title":"\ud83d\udcf1 Suivez-moi","text":"<ul> <li>GitHub : @loick-dernoncourt</li> <li>LinkedIn : Lo\u00efck Dernoncourt</li> <li>Twitter : @loick_dernoncourt</li> <li>Medium : Articles techniques</li> </ul>"},{"location":"feedback/#contact-direct","title":"\ud83d\udce7 Contact Direct","text":"<ul> <li>Email : loick.dernoncourt@example.com</li> <li>T\u00e9l\u00e9phone : +33 6 XX XX XX XX</li> <li>Disponibilit\u00e9 : Lundi-Vendredi, 9h-18h</li> </ul>"},{"location":"feedback/#ressources-gratuites","title":"\ud83c\udf81 Ressources Gratuites","text":""},{"location":"feedback/#e-books-et-guides","title":"\ud83d\udcda E-books et Guides","text":"<ul> <li>\"Guide du Data Scientist\" : M\u00e9thodologie compl\u00e8te</li> <li>\"Checklist MLOps\" : Bonnes pratiques de d\u00e9ploiement</li> <li>\"Templates de Projets\" : Structures de projets r\u00e9utilisables</li> </ul>"},{"location":"feedback/#outils-et-scripts","title":"\ud83d\udee0\ufe0f Outils et Scripts","text":"<ul> <li>Pipeline de donn\u00e9es : Scripts automatis\u00e9s</li> <li>Templates de visualisation : Graphiques pr\u00eats \u00e0 l'emploi</li> <li>Configurations Docker : Environnements de d\u00e9veloppement</li> </ul>"},{"location":"feedback/#prochaines-etapes","title":"\ud83d\ude80 Prochaines \u00c9tapes","text":""},{"location":"feedback/#evenements-a-venir","title":"\ud83d\udcc5 \u00c9v\u00e9nements \u00e0 Venir","text":"<ul> <li>Conf\u00e9rence \"AI &amp; Data Science\" : 15 Mars 2025</li> <li>Workshop \"MLOps\" : 22 Mars 2025</li> <li>Meetup \"Computer Vision\" : 5 Avril 2025</li> </ul>"},{"location":"feedback/#projets-en-cours","title":"\ud83d\udd2c Projets en Cours","text":"<ul> <li>Mod\u00e8le de pr\u00e9diction m\u00e9t\u00e9o : IA pour l'agriculture</li> <li>Syst\u00e8me de recommandation : E-commerce intelligent</li> <li>Analyse de sentiment : R\u00e9seaux sociaux en temps r\u00e9el</li> </ul> <p>Merci de votre int\u00e9r\u00eat pour mon travail ! N'h\u00e9sitez pas \u00e0 me contacter pour toute question ou collaboration. \ud83e\udd1d</p>"},{"location":"innovations/","title":"\ud83d\ude80 Fonctionnalit\u00e9s innovantes","text":"<p>D\u00e9couvrez les fonctionnalit\u00e9s avanc\u00e9es et les innovations technologiques int\u00e9gr\u00e9es dans ce portfolio.</p>"},{"location":"innovations/#fonctionnalites-principales","title":"\ud83c\udfaf Fonctionnalit\u00e9s principales","text":""},{"location":"innovations/#timeline-interactive-des-projets","title":"\ud83d\udcca Timeline interactive des projets","text":"<p>Visualisez l'\u00e9volution de mes projets dans le temps avec une frise chronologique interactive.</p> <pre><code>gantt\n    title Chronologie des Projets Data Science\n    dateFormat  YYYY-MM-DD\n    section Computer Vision\n    Classification d'images m\u00e9dicales :done, cv1, 2023-01-01, 2023-03-01\n    D\u00e9tection d'objets temps r\u00e9el :done, cv2, 2023-04-01, 2023-06-01\n    Reconnaissance faciale :done, cv3, 2023-07-01, 2023-09-01\n    section NLP\n    Analyse de sentiment :done, nlp1, 2023-02-01, 2023-04-01\n    Classification de textes :done, nlp2, 2023-05-01, 2023-07-01\n    G\u00e9n\u00e9ration de r\u00e9sum\u00e9s :active, nlp3, 2023-08-01, 2023-10-01\n    section Analyse pr\u00e9dictive\n    Pr\u00e9diction de prix immobiliers :done, ml1, 2023-03-01, 2023-05-01\n    Pr\u00e9diction de churn :done, ml2, 2023-06-01, 2023-08-01\n    Recommandation de produits :done, ml3, 2023-09-01, 2023-11-01</code></pre>"},{"location":"innovations/#filtrage-intelligent-par-competence","title":"\ud83d\udd0d Filtrage intelligent par comp\u00e9tence","text":"<p>Filtrez les projets par technologies, domaines d'application ou niveau de complexit\u00e9.</p> <ul> <li> <p> Machine Learning</p> <p>Projets utilisant des algorithmes de ML classiques</p> <p> Voir les projets</p> </li> <li> <p> Deep Learning</p> <p>Projets utilisant des r\u00e9seaux de neurones</p> <p> Voir les projets</p> </li> <li> <p> Computer Vision</p> <p>Projets de traitement d'images et vid\u00e9os</p> <p> Voir les projets</p> </li> <li> <p> NLP</p> <p>Projets de traitement du langage naturel</p> <p> Voir les projets</p> </li> </ul>"},{"location":"innovations/#dashboard-de-metriques-en-temps-reel","title":"\ud83d\udcc8 Dashboard de m\u00e9triques en temps r\u00e9el","text":"<p>Visualisez les performances et l'impact de mes projets avec des m\u00e9triques en temps r\u00e9el.</p>"},{"location":"innovations/#technologies-innovantes","title":"\ud83d\udee0\ufe0f Technologies innovantes","text":""},{"location":"innovations/#integration-dia-generative","title":"\ud83e\udd16 Int\u00e9gration d'IA g\u00e9n\u00e9rative","text":"<ul> <li>Chatbot intelligent : Posez des questions sur mes projets</li> <li>G\u00e9n\u00e9ration de contenu : Descriptions automatiques des projets</li> <li>Recommandations personnalis\u00e9es : Suggestions de projets similaires</li> </ul>"},{"location":"innovations/#visualisations-interactives","title":"\ud83d\udcca Visualisations interactives","text":"<ul> <li>Graphiques dynamiques : Plotly et Bokeh pour des visualisations interactives</li> <li>Cartes g\u00e9ographiques : Folium pour la g\u00e9olocalisation des projets</li> <li>Diagrammes de flux : Mermaid pour les architectures de donn\u00e9es</li> </ul>"},{"location":"innovations/#integration-continue","title":"\ud83d\udd04 Int\u00e9gration continue","text":"<ul> <li>D\u00e9ploiement automatique : GitHub Actions pour la mise \u00e0 jour du site</li> <li>Tests automatis\u00e9s : Validation du contenu et des liens</li> <li>Monitoring : Suivi des performances et de la disponibilit\u00e9</li> </ul>"},{"location":"innovations/#personnalisation-avancee","title":"\ud83c\udfa8 Personnalisation avanc\u00e9e","text":""},{"location":"innovations/#mode-sombreclair","title":"\ud83c\udf19 Mode sombre/clair","text":"<p>Basculez entre les th\u00e8mes clair et sombre selon vos pr\u00e9f\u00e9rences.</p>"},{"location":"innovations/#design-responsive","title":"\ud83d\udcf1 Design responsive","text":"<p>Interface optimis\u00e9e pour tous les appareils (desktop, tablette, mobile).</p>"},{"location":"innovations/#accessibilite","title":"\u267f Accessibilit\u00e9","text":"<ul> <li>Support lecteurs d'\u00e9cran : Navigation au clavier</li> <li>Contraste \u00e9lev\u00e9 : Respect des standards WCAG</li> <li>Texte alternatif : Descriptions pour toutes les images</li> </ul>"},{"location":"innovations/#fonctionnalites-techniques","title":"\ud83d\udd27 Fonctionnalit\u00e9s techniques","text":""},{"location":"innovations/#edition-en-ligne","title":"\ud83d\udcdd \u00c9dition en ligne","text":"<ul> <li>Mode \u00e9dition : Modification directe du contenu</li> <li>Pr\u00e9visualisation : Aper\u00e7u en temps r\u00e9el</li> <li>Sauvegarde automatique : Versioning des modifications</li> </ul>"},{"location":"innovations/#recherche-avancee","title":"\ud83d\udd0d Recherche avanc\u00e9e","text":"<ul> <li>Recherche s\u00e9mantique : Compr\u00e9hension du contexte</li> <li>Filtres intelligents : Recherche par tags, technologies, dates</li> <li>Suggestions : Autocompl\u00e9tion et recommandations</li> </ul>"},{"location":"innovations/#analytics-integrees","title":"\ud83d\udcca Analytics int\u00e9gr\u00e9es","text":"<ul> <li>M\u00e9triques de visite : Suivi des pages les plus consult\u00e9es</li> <li>Temps de lecture : Estimation du temps n\u00e9cessaire</li> <li>Engagement : Interactions et clics</li> </ul>"},{"location":"innovations/#ameliorations-futures","title":"\ud83d\ude80 Am\u00e9liorations futures","text":""},{"location":"innovations/#roadmap-2024","title":"\ud83c\udfaf Roadmap 2024","text":"<ul> <li> Reality augment\u00e9e : Visualisation 3D des architectures</li> <li> Voice interface : Navigation vocale</li> <li> Collaboration temps r\u00e9el : \u00c9dition collaborative</li> <li> Int\u00e9gration IoT : Donn\u00e9es en temps r\u00e9el</li> </ul>"},{"location":"innovations/#vision-2025","title":"\ud83d\udd2e Vision 2025","text":"<ul> <li> IA conversationnelle : Assistant personnel int\u00e9gr\u00e9</li> <li> Reality virtuelle : Visite immersive des projets</li> <li> Blockchain : Certification des comp\u00e9tences</li> <li> Quantum computing : Optimisation quantique</li> </ul>"},{"location":"innovations/#innovation-technique","title":"\ud83d\udca1 Innovation technique","text":""},{"location":"innovations/#architecture-microservices","title":"\ud83e\udde0 Architecture microservices","text":"<pre><code># Exemple d'architecture microservices\nservices = {\n    \"content_service\": \"Gestion du contenu\",\n    \"analytics_service\": \"M\u00e9triques et analytics\",\n    \"ai_service\": \"IA g\u00e9n\u00e9rative et recommandations\",\n    \"search_service\": \"Recherche s\u00e9mantique\",\n    \"notification_service\": \"Notifications en temps r\u00e9el\"\n}\n</code></pre>"},{"location":"innovations/#pipeline-de-donnees","title":"\ud83d\udd04 Pipeline de donn\u00e9es","text":"<pre><code>graph LR\n    A[Sources de donn\u00e9es] --&gt; B[ETL Pipeline]\n    B --&gt; C[Data Lake]\n    C --&gt; D[Feature Store]\n    D --&gt; E[ML Models]\n    E --&gt; F[API Gateway]\n    F --&gt; G[Frontend]</code></pre>"},{"location":"innovations/#monitoring-et-observabilite","title":"\ud83d\udcca Monitoring et observabilit\u00e9","text":"<ul> <li>M\u00e9triques Prometheus : Performance et disponibilit\u00e9</li> <li>Logs centralis\u00e9s : ELK Stack pour l'analyse</li> <li>Alertes intelligentes : D\u00e9tection proactive des probl\u00e8mes</li> <li>Dashboards Grafana : Visualisation des m\u00e9triques</li> </ul>"},{"location":"innovations/#apprentissage-continu","title":"\ud83c\udf93 Apprentissage continu","text":""},{"location":"innovations/#ressources-integrees","title":"\ud83d\udcda Ressources int\u00e9gr\u00e9es","text":"<ul> <li>Tutoriels interactifs : Apprentissage par la pratique</li> <li>Code examples : Exemples de code ex\u00e9cutables</li> <li>Documentation technique : Guides d\u00e9taill\u00e9s</li> <li>Communaut\u00e9 : Forum et discussions</li> </ul>"},{"location":"innovations/#experimentation","title":"\ud83d\udd2c Exp\u00e9rimentation","text":"<ul> <li>Sandbox : Environnement de test s\u00e9curis\u00e9</li> <li>A/B Testing : Tests d'optimisation</li> <li>Feedback loops : Am\u00e9lioration continue</li> <li>Versioning : Gestion des versions</li> </ul> <p>Derni\u00e8re mise \u00e0 jour : October 22, 2025</p>"},{"location":"lab/","title":"\ud83e\uddea Lab - Exp\u00e9rimentations et Playground","text":"<p>Bienvenue dans mon laboratoire d'exp\u00e9rimentations ! Cette section pr\u00e9sente mes projets en cours, mes explorations techniques et mes mini-projets interactifs qui d\u00e9montrent ma curiosit\u00e9 et mon apprentissage continu.</p>"},{"location":"lab/#experimentations-en-cours","title":"\ud83d\udd2c Exp\u00e9rimentations en cours","text":""},{"location":"lab/#ia-generative-et-llms","title":"\ud83e\udd16 IA G\u00e9n\u00e9rative et LLMs","text":"<p>Exploration des derni\u00e8res avanc\u00e9es en intelligence artificielle g\u00e9n\u00e9rative</p> <pre><code># Exemple d'exp\u00e9rimentation avec des LLMs\nfrom transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n\ndef experiment_with_llm(prompt, model_name=\"microsoft/DialoGPT-medium\"):\n    \"\"\"Exp\u00e9rimentation avec diff\u00e9rents mod\u00e8les de langage\"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForCausalLM.from_pretrained(model_name)\n\n    # G\u00e9n\u00e9ration de texte cr\u00e9atif\n    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n    result = generator(prompt, max_length=100, num_return_sequences=3)\n\n    return result\n\n# Test avec diff\u00e9rents prompts\nprompts = [\n    \"L'avenir de la data science sera\",\n    \"Comment optimiser un mod\u00e8le de machine learning ?\",\n    \"Les d\u00e9fis de l'IA \u00e9thique incluent\"\n]\n\nfor prompt in prompts:\n    results = experiment_with_llm(prompt)\n    print(f\"Prompt: {prompt}\")\n    for i, result in enumerate(results):\n        print(f\"  Variante {i+1}: {result['generated_text']}\")\n</code></pre> <p>Technologies explor\u00e9es : - \ud83e\udd17 HuggingFace Transformers : GPT, BERT, T5 - \ud83e\udde0 LangChain : Orchestration de LLMs - \ud83c\udfaf Fine-tuning : Adaptation de mod\u00e8les pr\u00e9-entra\u00een\u00e9s - \ud83d\udd04 RAG : Retrieval-Augmented Generation</p>"},{"location":"lab/#computer-vision-creative","title":"\ud83c\udfa8 Computer Vision Cr\u00e9ative","text":"<p>Exploration de l'IA g\u00e9n\u00e9rative pour les images</p> <pre><code># Exp\u00e9rimentation avec Stable Diffusion\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\ndef generate_creative_images(prompt, num_images=4):\n    \"\"\"G\u00e9n\u00e9ration d'images cr\u00e9atives avec Stable Diffusion\"\"\"\n    pipe = StableDiffusionPipeline.from_pretrained(\n        \"runwayml/stable-diffusion-v1-5\",\n        torch_dtype=torch.float16\n    )\n\n    images = pipe(\n        prompt,\n        num_images_per_prompt=num_images,\n        guidance_scale=7.5,\n        num_inference_steps=50\n    ).images\n\n    return images\n\n# Exemples d'exp\u00e9rimentations\ncreative_prompts = [\n    \"A futuristic data scientist working with holographic data visualizations\",\n    \"An AI robot analyzing complex neural network architectures\",\n    \"A cyberpunk cityscape with data streams flowing through buildings\"\n]\n</code></pre> <p>Projets en cours : - \ud83c\udfad Style Transfer : Application de styles artistiques - \ud83d\uddbc\ufe0f Image Inpainting : Reconstruction d'images - \ud83c\udfac Video Generation : Cr\u00e9ation de vid\u00e9os avec IA - \ud83c\udfa8 Art Generation : G\u00e9n\u00e9ration d'\u0153uvres d'art num\u00e9riques</p>"},{"location":"lab/#mathematiques-appliquees","title":"\ud83e\uddee Math\u00e9matiques Appliqu\u00e9es","text":"<p>Exploration des concepts math\u00e9matiques avanc\u00e9s en data science</p> <pre><code># Exp\u00e9rimentation avec les Transformers math\u00e9matiques\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize\nimport sympy as sp\n\ndef explore_mathematical_concepts():\n    \"\"\"Exploration de concepts math\u00e9matiques avanc\u00e9s\"\"\"\n\n    # 1. Optimisation non-lin\u00e9aire\n    def rosenbrock(x):\n        return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n\n    result = minimize(rosenbrock, [0, 0], method='BFGS')\n\n    # 2. Calcul symbolique\n    x, y = sp.symbols('x y')\n    expression = sp.sin(x) * sp.cos(y) + sp.exp(x)\n    derivative = sp.diff(expression, x)\n\n    # 3. Visualisation de surfaces complexes\n    x_vals = np.linspace(-2, 2, 100)\n    y_vals = np.linspace(-2, 2, 100)\n    X, Y = np.meshgrid(x_vals, y_vals)\n    Z = np.sin(X) * np.cos(Y)\n\n    return {\n        'optimization': result,\n        'symbolic_math': derivative,\n        'surface_data': (X, Y, Z)\n    }\n</code></pre> <p>Domaines explor\u00e9s : - \ud83d\udcca Topologie : Analyse de formes et structures - \ud83d\udd22 Th\u00e9orie des graphes : R\u00e9seaux complexes et algorithmes - \ud83d\udcc8 Calcul diff\u00e9rentiel : Optimisation avanc\u00e9e - \ud83c\udfb2 Probabilit\u00e9s : Mod\u00e8les stochastiques</p>"},{"location":"lab/#mini-projets-interactifs","title":"\ud83c\udfae Mini-projets interactifs","text":""},{"location":"lab/#jeu-de-prediction-en-temps-reel","title":"\ud83c\udfaf Jeu de pr\u00e9diction en temps r\u00e9el","text":"<p>Interface interactive pour tester des mod\u00e8les de pr\u00e9diction</p> <pre><code># Mini-application Streamlit pour pr\u00e9dictions interactives\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\n\ndef create_prediction_game():\n    \"\"\"Jeu interactif de pr\u00e9diction\"\"\"\n    st.title(\"\ud83c\udfaf Jeu de Pr\u00e9diction Interactive\")\n\n    # G\u00e9n\u00e9ration de donn\u00e9es synth\u00e9tiques\n    np.random.seed(42)\n    n_samples = 1000\n    X = np.random.randn(n_samples, 5)\n    y = X[:, 0] * 2 + X[:, 1] * 1.5 + np.random.randn(n_samples) * 0.1\n\n    # Interface utilisateur\n    st.sidebar.header(\"Param\u00e8tres du mod\u00e8le\")\n    n_estimators = st.sidebar.slider(\"Nombre d'arbres\", 10, 200, 100)\n    max_depth = st.sidebar.slider(\"Profondeur max\", 3, 20, 10)\n\n    # Entra\u00eenement du mod\u00e8le\n    model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth)\n    model.fit(X, y)\n\n    # Pr\u00e9diction interactive\n    st.header(\"Faites vos pr\u00e9dictions !\")\n    col1, col2, col3 = st.columns(3)\n\n    with col1:\n        feature1 = st.number_input(\"Feature 1\", value=0.0)\n        feature2 = st.number_input(\"Feature 2\", value=0.0)\n\n    with col2:\n        feature3 = st.number_input(\"Feature 3\", value=0.0)\n        feature4 = st.number_input(\"Feature 4\", value=0.0)\n\n    with col3:\n        feature5 = st.number_input(\"Feature 5\", value=0.0)\n\n    if st.button(\"Pr\u00e9dire\"):\n        input_data = np.array([[feature1, feature2, feature3, feature4, feature5]])\n        prediction = model.predict(input_data)[0]\n        st.success(f\"\ud83c\udfaf Pr\u00e9diction : {prediction:.2f}\")\n\n        # Affichage de l'importance des features\n        importance = model.feature_importances_\n        st.bar_chart(pd.DataFrame(importance, columns=['Importance']))\n</code></pre>"},{"location":"lab/#generateur-de-visualisations-creatives","title":"\ud83c\udfa8 G\u00e9n\u00e9rateur de visualisations cr\u00e9atives","text":"<p>Outil interactif pour cr\u00e9er des visualisations artistiques</p> <pre><code># G\u00e9n\u00e9rateur de visualisations cr\u00e9atives\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\ndef create_artistic_visualizations():\n    \"\"\"Cr\u00e9ation de visualisations artistiques\"\"\"\n\n    # 1. Spirale de Fibonacci\n    def fibonacci_spiral(n):\n        phi = (1 + np.sqrt(5)) / 2\n        theta = np.linspace(0, n * 2 * np.pi, 1000)\n        r = phi ** (theta / (2 * np.pi))\n        x = r * np.cos(theta)\n        y = r * np.sin(theta)\n        return x, y\n\n    # 2. Fractales de Mandelbrot\n    def mandelbrot_set(width, height, max_iter=100):\n        x = np.linspace(-2, 2, width)\n        y = np.linspace(-2, 2, height)\n        X, Y = np.meshgrid(x, y)\n        C = X + 1j * Y\n        Z = np.zeros_like(C)\n        iterations = np.zeros_like(C, dtype=int)\n\n        for i in range(max_iter):\n            mask = np.abs(Z) &lt;= 2\n            Z[mask] = Z[mask]**2 + C[mask]\n            iterations[mask] = i\n\n        return iterations\n\n    # 3. Visualisation 3D interactive\n    def create_3d_surface():\n        x = np.linspace(-5, 5, 50)\n        y = np.linspace(-5, 5, 50)\n        X, Y = np.meshgrid(x, y)\n        Z = np.sin(np.sqrt(X**2 + Y**2))\n\n        fig = go.Figure(data=[go.Surface(z=Z, x=X, y=Y)])\n        fig.update_layout(title=\"Surface 3D Interactive\")\n        return fig\n\n    return {\n        'fibonacci': fibonacci_spiral(10),\n        'mandelbrot': mandelbrot_set(100, 100),\n        'surface_3d': create_3d_surface()\n    }\n</code></pre>"},{"location":"lab/#recherche-et-innovation","title":"\ud83d\udd2c Recherche et Innovation","text":""},{"location":"lab/#analyse-de-sentiment-en-temps-reel","title":"\ud83d\udcca Analyse de Sentiment en Temps R\u00e9el","text":"<p>Prototype d'analyse de sentiment sur les r\u00e9seaux sociaux</p> <pre><code># Pipeline d'analyse de sentiment en temps r\u00e9el\nimport tweepy\nfrom transformers import pipeline\nimport pandas as pd\nfrom datetime import datetime\n\nclass RealTimeSentimentAnalyzer:\n    def __init__(self):\n        self.sentiment_pipeline = pipeline(\n            \"sentiment-analysis\",\n            model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n        )\n\n    def analyze_twitter_sentiment(self, query, count=100):\n        \"\"\"Analyse de sentiment sur Twitter en temps r\u00e9el\"\"\"\n        # Configuration API Twitter (n\u00e9cessite des cl\u00e9s API)\n        # auth = tweepy.OAuthHandler(api_key, api_secret)\n        # api = tweepy.API(auth)\n\n        # Simulation de donn\u00e9es\n        tweets = self._simulate_tweets(query, count)\n\n        # Analyse de sentiment\n        results = []\n        for tweet in tweets:\n            sentiment = self.sentiment_pipeline(tweet['text'])\n            results.append({\n                'text': tweet['text'],\n                'sentiment': sentiment[0]['label'],\n                'confidence': sentiment[0]['score'],\n                'timestamp': tweet['timestamp']\n            })\n\n        return pd.DataFrame(results)\n\n    def _simulate_tweets(self, query, count):\n        \"\"\"Simulation de tweets (remplacer par vraie API)\"\"\"\n        sample_tweets = [\n            f\"J'adore {query}, c'est incroyable !\",\n            f\"{query} est vraiment d\u00e9cevant...\",\n            f\"Je suis neutre sur {query}\",\n            f\"{query} change compl\u00e8tement la donne !\"\n        ]\n\n        tweets = []\n        for i in range(count):\n            tweets.append({\n                'text': sample_tweets[i % len(sample_tweets)],\n                'timestamp': datetime.now()\n            })\n\n        return tweets\n</code></pre>"},{"location":"lab/#modeles-de-langage-personnalises","title":"\ud83e\udde0 Mod\u00e8les de Langage Personnalis\u00e9s","text":"<p>Exp\u00e9rimentation avec des mod\u00e8les de langage sp\u00e9cialis\u00e9s</p> <pre><code># Exp\u00e9rimentation avec des mod\u00e8les de langage sp\u00e9cialis\u00e9s\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nclass SpecializedLanguageModel:\n    def __init__(self, model_name=\"microsoft/DialoGPT-medium\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n\n        # Ajout de tokens sp\u00e9ciaux pour la data science\n        special_tokens = {\n            \"additional_special_tokens\": [\n                \"&lt;data_science&gt;\", \"&lt;machine_learning&gt;\", \"&lt;deep_learning&gt;\",\n                \"&lt;python&gt;\", \"&lt;pytorch&gt;\", \"&lt;tensorflow&gt;\", \"&lt;sklearn&gt;\"\n            ]\n        }\n\n        self.tokenizer.add_special_tokens(special_tokens)\n        self.model.resize_token_embeddings(len(self.tokenizer))\n\n    def generate_data_science_content(self, prompt, max_length=200):\n        \"\"\"G\u00e9n\u00e9ration de contenu sp\u00e9cialis\u00e9 en data science\"\"\"\n        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n\n        with torch.no_grad():\n            outputs = self.model.generate(\n                inputs,\n                max_length=max_length,\n                num_return_sequences=3,\n                temperature=0.7,\n                do_sample=True,\n                pad_token_id=self.tokenizer.eos_token_id\n            )\n\n        generated_texts = []\n        for output in outputs:\n            text = self.tokenizer.decode(output, skip_special_tokens=True)\n            generated_texts.append(text)\n\n        return generated_texts\n\n# Exemple d'utilisation\n# model = SpecializedLanguageModel()\n# content = model.generate_data_science_content(\"Comment optimiser un mod\u00e8le de machine learning ?\")\n</code></pre>"},{"location":"lab/#projets-collaboratifs","title":"\ud83c\udfaf Projets Collaboratifs","text":""},{"location":"lab/#open-source-contributions","title":"\ud83e\udd1d Open Source Contributions","text":"<p>Contributions aux projets open source de la communaut\u00e9</p> <ul> <li>Scikit-learn : Am\u00e9lioration des algorithmes d'ensemble</li> <li>HuggingFace : Mod\u00e8les de traitement du langage naturel</li> <li>Streamlit : Composants interactifs pour la data science</li> <li>Plotly : Visualisations avanc\u00e9es</li> </ul>"},{"location":"lab/#tutoriels-et-workshops","title":"\ud83d\udcda Tutoriels et Workshops","text":"<p>Cr\u00e9ation de contenu \u00e9ducatif pour la communaut\u00e9</p> <ul> <li>Workshop \"Introduction au Deep Learning\" : Session de 4h</li> <li>Tutoriel \"MLOps avec Docker\" : Guide pratique</li> <li>Webinaire \"\u00c9thique en IA\" : D\u00e9bats et r\u00e9flexions</li> </ul>"},{"location":"lab/#vision-future","title":"\ud83d\udd2e Vision Future","text":""},{"location":"lab/#technologies-emergentes","title":"\ud83d\ude80 Technologies \u00c9mergentes","text":"<p>Exploration des technologies de demain</p> <ul> <li>Quantum Machine Learning : Algorithmes quantiques</li> <li>Neuromorphic Computing : Calcul inspir\u00e9 du cerveau</li> <li>Edge AI : Intelligence artificielle embarqu\u00e9e</li> <li>Federated Learning : Apprentissage distribu\u00e9</li> </ul>"},{"location":"lab/#impact-social","title":"\ud83c\udf0d Impact Social","text":"<p>Utilisation de la data science pour le bien commun</p> <ul> <li>Pr\u00e9diction des catastrophes naturelles : Mod\u00e8les de pr\u00e9vention</li> <li>Optimisation des ressources \u00e9nerg\u00e9tiques : Smart grids</li> <li>Sant\u00e9 publique : Mod\u00e8les \u00e9pid\u00e9miologiques</li> <li>\u00c9ducation : Personnalisation de l'apprentissage</li> </ul> <p>Cette section \u00e9volue constamment au gr\u00e9 de mes explorations et d\u00e9couvertes. N'h\u00e9sitez pas \u00e0 me contacter si vous souhaitez collaborer sur l'un de ces projets ! \ud83d\ude80</p>"},{"location":"methodologie/","title":"\ud83d\udd2c M\u00e9thodologie de travail","text":"<p>D\u00e9couvrez ma d\u00e9marche m\u00e9thodologique pour mener \u00e0 bien des projets de data science, de la conception \u00e0 la mise en production.</p>"},{"location":"methodologie/#approche-methodologique","title":"\ud83c\udfaf Approche m\u00e9thodologique","text":""},{"location":"methodologie/#1-comprehension-du-probleme-metier","title":"1. Compr\u00e9hension du probl\u00e8me m\u00e9tier","text":"<pre><code>graph LR\n    A[Probl\u00e8me m\u00e9tier] --&gt; B[Objectifs clairs]\n    B --&gt; C[M\u00e9triques de succ\u00e8s]\n    C --&gt; D[Contraintes techniques]\n    D --&gt; E[Plan d'action]</code></pre> <p>\u00c9tapes cl\u00e9s : - Stakeholder interviews : Compr\u00e9hension des besoins r\u00e9els - D\u00e9finition des objectifs : SMART (Sp\u00e9cifique, Mesurable, Atteignable, R\u00e9aliste, Temporel) - M\u00e9triques de succ\u00e8s : KPIs business et techniques - Analyse des contraintes : Temps, budget, donn\u00e9es, r\u00e9glementation</p>"},{"location":"methodologie/#2-exploration-et-comprehension-des-donnees","title":"2. Exploration et compr\u00e9hension des donn\u00e9es","text":"<pre><code># Pipeline d'exploration des donn\u00e9es\ndef explore_data(data):\n    \"\"\"Pipeline complet d'exploration des donn\u00e9es\"\"\"\n\n    # 1. Vue d'ensemble\n    print(\"=== VUE D'ENSEMBLE ===\")\n    print(f\"Shape: {data.shape}\")\n    print(f\"Types: {data.dtypes}\")\n    print(f\"Missing values: {data.isnull().sum()}\")\n\n    # 2. Statistiques descriptives\n    print(\"\\n=== STATISTIQUES ===\")\n    print(data.describe())\n\n    # 3. Analyse de la qualit\u00e9\n    print(\"\\n=== QUALIT\u00c9 DES DONN\u00c9ES ===\")\n    quality_report = analyze_data_quality(data)\n\n    # 4. Visualisations exploratoires\n    create_exploratory_plots(data)\n\n    return quality_report\n</code></pre> <p>Outils utilis\u00e9s : - Pandas Profiling : Rapport automatique de qualit\u00e9 - Sweetviz : Analyse comparative des datasets - Matplotlib/Seaborn : Visualisations personnalis\u00e9es - Plotly : Graphiques interactifs</p>"},{"location":"methodologie/#3-feature-engineering-et-preprocessing","title":"3. Feature Engineering et pr\u00e9processing","text":"<pre><code>class DataPreprocessor:\n    def __init__(self):\n        self.scalers = {}\n        self.encoders = {}\n        self.feature_selector = None\n\n    def create_features(self, df):\n        \"\"\"Cr\u00e9ation de features m\u00e9tier\"\"\"\n        # Features temporelles\n        df['year'] = df['date'].dt.year\n        df['month'] = df['date'].dt.month\n        df['day_of_week'] = df['date'].dt.dayofweek\n\n        # Features d'interaction\n        df['feature_ratio'] = df['var1'] / df['var2']\n        df['feature_product'] = df['var1'] * df['var2']\n\n        # Features statistiques\n        df['rolling_mean_7d'] = df['value'].rolling(7).mean()\n        df['rolling_std_7d'] = df['value'].rolling(7).std()\n\n        return df\n\n    def handle_missing_values(self, df, strategy='intelligent'):\n        \"\"\"Gestion intelligente des valeurs manquantes\"\"\"\n        if strategy == 'intelligent':\n            # Analyse des patterns de manquants\n            missing_patterns = analyze_missing_patterns(df)\n\n            # Imputation adapt\u00e9e\n            for col in df.columns:\n                if df[col].dtype in ['int64', 'float64']:\n                    df[col].fillna(df[col].median(), inplace=True)\n                else:\n                    df[col].fillna(df[col].mode()[0], inplace=True)\n\n        return df\n</code></pre>"},{"location":"methodologie/#4-modelisation-et-validation","title":"4. Mod\u00e9lisation et validation","text":"<pre><code>class ModelPipeline:\n    def __init__(self):\n        self.models = {}\n        self.results = {}\n\n    def train_models(self, X_train, y_train):\n        \"\"\"Entra\u00eenement de plusieurs mod\u00e8les\"\"\"\n        models = {\n            'Random Forest': RandomForestClassifier(n_estimators=100),\n            'XGBoost': XGBClassifier(n_estimators=100),\n            'LightGBM': LGBMClassifier(n_estimators=100),\n            'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50))\n        }\n\n        for name, model in models.items():\n            # Entra\u00eenement\n            model.fit(X_train, y_train)\n            self.models[name] = model\n\n            # Validation crois\u00e9e\n            cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n            self.results[name] = {\n                'cv_mean': cv_scores.mean(),\n                'cv_std': cv_scores.std(),\n                'model': model\n            }\n\n    def optimize_hyperparameters(self, model, X_train, y_train):\n        \"\"\"Optimisation des hyperparam\u00e8tres avec Optuna\"\"\"\n        def objective(trial):\n            params = {\n                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n                'max_depth': trial.suggest_int('max_depth', 3, 10),\n                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3)\n            }\n\n            model.set_params(**params)\n            score = cross_val_score(model, X_train, y_train, cv=5).mean()\n            return score\n\n        study = optuna.create_study(direction='maximize')\n        study.optimize(objective, n_trials=100)\n\n        return study.best_params\n</code></pre>"},{"location":"methodologie/#5-evaluation-et-interpretation","title":"5. \u00c9valuation et interpr\u00e9tation","text":"<pre><code>class ModelEvaluator:\n    def __init__(self):\n        self.metrics = {}\n        self.explanations = {}\n\n    def comprehensive_evaluation(self, model, X_test, y_test):\n        \"\"\"\u00c9valuation compl\u00e8te du mod\u00e8le\"\"\"\n        # Pr\u00e9dictions\n        y_pred = model.predict(X_test)\n        y_pred_proba = model.predict_proba(X_test)[:, 1]\n\n        # M\u00e9triques de performance\n        metrics = {\n            'accuracy': accuracy_score(y_test, y_pred),\n            'precision': precision_score(y_test, y_pred),\n            'recall': recall_score(y_test, y_pred),\n            'f1': f1_score(y_test, y_pred),\n            'auc_roc': roc_auc_score(y_test, y_pred_proba)\n        }\n\n        # Analyse des erreurs\n        error_analysis = self.analyze_errors(y_test, y_pred, X_test)\n\n        # Explicabilit\u00e9\n        if hasattr(model, 'feature_importances_'):\n            feature_importance = self.get_feature_importance(model, X_test)\n        else:\n            feature_importance = self.shap_analysis(model, X_test)\n\n        return {\n            'metrics': metrics,\n            'error_analysis': error_analysis,\n            'feature_importance': feature_importance\n        }\n\n    def shap_analysis(self, model, X_test):\n        \"\"\"Analyse SHAP pour l'explicabilit\u00e9\"\"\"\n        explainer = shap.TreeExplainer(model)\n        shap_values = explainer.shap_values(X_test)\n\n        return {\n            'shap_values': shap_values,\n            'feature_names': X_test.columns.tolist(),\n            'summary_plot': shap.summary_plot(shap_values, X_test)\n        }\n</code></pre>"},{"location":"methodologie/#deploiement-et-mlops","title":"\ud83d\ude80 D\u00e9ploiement et MLOps","text":""},{"location":"methodologie/#1-architecture-de-production","title":"1. Architecture de production","text":"<pre><code>graph TB\n    A[Data Sources] --&gt; B[Data Pipeline]\n    B --&gt; C[Feature Store]\n    C --&gt; D[Model Training]\n    D --&gt; E[Model Registry]\n    E --&gt; F[Model Serving]\n    F --&gt; G[API Gateway]\n    G --&gt; H[Frontend]\n\n    I[Monitoring] --&gt; F\n    J[Logging] --&gt; F\n    K[Alerting] --&gt; F</code></pre>"},{"location":"methodologie/#2-pipeline-cicd","title":"2. Pipeline CI/CD","text":"<pre><code># .github/workflows/ml-pipeline.yml\nname: ML Pipeline\n\non:\n  push:\n    branches: [ main ]\n    paths: [ 'models/**', 'data/**' ]\n\njobs:\n  train-model:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.11'\n\n    - name: Install dependencies\n      run: pip install -r requirements.txt\n\n    - name: Run tests\n      run: pytest tests/\n\n    - name: Train model\n      run: python scripts/train_model.py\n\n    - name: Evaluate model\n      run: python scripts/evaluate_model.py\n\n    - name: Deploy model\n      if: success()\n      run: python scripts/deploy_model.py\n</code></pre>"},{"location":"methodologie/#3-monitoring-et-observabilite","title":"3. Monitoring et observabilit\u00e9","text":"<pre><code>class ModelMonitor:\n    def __init__(self):\n        self.metrics = {}\n        self.alerts = []\n\n    def monitor_data_drift(self, current_data, reference_data):\n        \"\"\"D\u00e9tection de d\u00e9rive des donn\u00e9es\"\"\"\n        from evidently import ColumnMapping\n        from evidently.report import Report\n        from evidently.metric_preset import DataDriftPreset\n\n        report = Report(metrics=[DataDriftPreset()])\n        report.run(\n            reference_data=reference_data,\n            current_data=current_data\n        )\n\n        return report\n\n    def monitor_model_performance(self, y_true, y_pred):\n        \"\"\"Monitoring des performances du mod\u00e8le\"\"\"\n        current_accuracy = accuracy_score(y_true, y_pred)\n\n        # Comparaison avec baseline\n        if current_accuracy &lt; self.baseline_accuracy * 0.95:\n            self.send_alert(\"Model performance degraded\")\n\n        return current_accuracy\n\n    def send_alert(self, message):\n        \"\"\"Envoi d'alertes\"\"\"\n        # Int\u00e9gration avec Slack, email, PagerDuty\n        pass\n</code></pre>"},{"location":"methodologie/#outils-et-technologies","title":"\ud83d\udcca Outils et technologies","text":""},{"location":"methodologie/#environnement-de-developpement","title":"Environnement de d\u00e9veloppement","text":"<ul> <li>IDE : VS Code avec extensions Python, Jupyter</li> <li>Versioning : Git + DVC pour les donn\u00e9es</li> <li>Environnement : Conda/Poetry pour la gestion des d\u00e9pendances</li> <li>Notebooks : Jupyter Lab avec extensions</li> </ul>"},{"location":"methodologie/#pipeline-de-donnees","title":"Pipeline de donn\u00e9es","text":"<ul> <li>ETL : Apache Airflow, Prefect</li> <li>Storage : S3, HDFS, PostgreSQL</li> <li>Processing : Pandas, Spark, Dask</li> <li>Feature Store : Feast, Tecton</li> </ul>"},{"location":"methodologie/#machine-learning","title":"Machine Learning","text":"<ul> <li>Frameworks : Scikit-learn, XGBoost, PyTorch, TensorFlow</li> <li>MLOps : MLflow, Kubeflow, Weights &amp; Biases</li> <li>Monitoring : Evidently, WhyLabs, Neptune</li> </ul>"},{"location":"methodologie/#deploiement","title":"D\u00e9ploiement","text":"<ul> <li>Conteneurisation : Docker, Kubernetes</li> <li>APIs : FastAPI, Flask, Django</li> <li>Cloud : AWS, GCP, Azure</li> <li>Monitoring : Prometheus, Grafana, ELK Stack</li> </ul>"},{"location":"methodologie/#bonnes-pratiques","title":"\ud83c\udfaf Bonnes pratiques","text":""},{"location":"methodologie/#code-et-documentation","title":"Code et documentation","text":"<ul> <li>Documentation : Docstrings, README, architecture decisions</li> <li>Tests : Unit tests, integration tests, model tests</li> <li>Code review : Pull requests, pair programming</li> <li>Standards : PEP 8, type hints, linting</li> </ul>"},{"location":"methodologie/#gestion-des-donnees","title":"Gestion des donn\u00e9es","text":"<ul> <li>Privacy : Anonymisation, pseudonymisation</li> <li>Security : Chiffrement, acc\u00e8s contr\u00f4l\u00e9</li> <li>Compliance : RGPD, audit trails</li> <li>Backup : Strat\u00e9gies de sauvegarde et r\u00e9cup\u00e9ration</li> </ul>"},{"location":"methodologie/#collaboration","title":"Collaboration","text":"<ul> <li>Communication : Stand-ups, retrospectives</li> <li>Knowledge sharing : Tech talks, documentation</li> <li>Mentoring : Pair programming, code reviews</li> <li>Continuous learning : Cours, conf\u00e9rences, certifications</li> </ul> <p>Cette m\u00e9thodologie garantit la qualit\u00e9, la reproductibilit\u00e9 et l'impact business de mes projets de data science.</p>"},{"location":"portfolio-reel/","title":"\ud83d\ude80 Portfolio R\u00e9el","text":"<p>Voici une s\u00e9lection de mes projets r\u00e9els qui d\u00e9montrent mes comp\u00e9tences en data science et d\u00e9veloppement.</p> <p>!!! tip \"\ud83d\udca1 Focus sur la valeur m\u00e9tier\"     Chaque projet est pr\u00e9sent\u00e9 avec sa valeur business, sa stack technique, ses m\u00e9triques de performance et ses liens de d\u00e9monstration.</p>"},{"location":"portfolio-reel/#mes-projets","title":"\ud83c\udfc6 Mes Projets","text":"<ul> <li> <p> Compagnon Immo</p> <p>Pr\u00e9diction \u20ac/m\u00b2 avec clustering spatio-temporel R\u00b2 &gt; 0.96, MAE ~2.4k\u20ac/m\u00b2, MAPE &lt; 3%</p> </li> <li> <p> VALMED</p> <p>Automatisation de processus m\u00e9dicaux En cours de d\u00e9veloppement</p> </li> <li> <p> SaaS Platform</p> <p>Template SaaS data-ready scalable Architecture r\u00e9utilisable</p> </li> </ul>"},{"location":"portfolio-reel/#pourquoi-ces-projets","title":"\ud83c\udfaf Pourquoi ces projets ?","text":""},{"location":"portfolio-reel/#approche-methodique","title":"\ud83d\udd2c Approche m\u00e9thodique","text":"<p>Chaque projet suit une m\u00e9thodologie rigoureuse : exploration \u2192 mod\u00e9lisation \u2192 validation \u2192 d\u00e9ploiement.</p>"},{"location":"portfolio-reel/#metriques-concretes","title":"\ud83d\udcca M\u00e9triques concr\u00e8tes","text":"<p>Performance mesur\u00e9e avec des indicateurs clairs : R\u00b2, MAE, MAPE, pr\u00e9cision, etc.</p>"},{"location":"portfolio-reel/#impact-business","title":"\ud83d\ude80 Impact business","text":"<p>Solutions con\u00e7ues pour r\u00e9soudre de vrais probl\u00e8mes avec une valeur m\u00e9tier d\u00e9montr\u00e9e.</p>"},{"location":"portfolio-reel/#contact","title":"\ud83d\udcde Contact","text":"<ul> <li> <p> Email</p> <p>Dernoncourt.ck@gmail.com</p> </li> <li> <p> LinkedIn</p> <p>Profil professionnel</p> </li> <li> <p> GitHub</p> <p>Code &amp; Projets</p> </li> </ul>"},{"location":"seo-guide/","title":"\ud83d\udd0d Guide SEO et Optimisation","text":"<p>Ce guide vous accompagne dans l'optimisation SEO de votre portfolio pour maximiser sa visibilit\u00e9 et son impact.</p>"},{"location":"seo-guide/#strategie-seo","title":"\ud83c\udfaf Strat\u00e9gie SEO","text":""},{"location":"seo-guide/#mots-cles-cibles","title":"\ud83d\udcca Mots-cl\u00e9s Cibles","text":"<ul> <li>Primaires : Data Scientist, Machine Learning, Deep Learning, Python</li> <li>Secondaires : Computer Vision, NLP, MLOps, Data Engineering</li> <li>Longue tra\u00eene : \"Data Scientist Python\", \"Machine Learning Engineer\", \"Deep Learning Expert\"</li> </ul>"},{"location":"seo-guide/#metadonnees-optimisees","title":"\ud83c\udff7\ufe0f M\u00e9tadonn\u00e9es Optimis\u00e9es","text":"<pre><code># Configuration SEO dans mkdocs.yml\nsite_description: \"Portfolio de Lo\u00efck Dernoncourt - Data Scientist expert en Machine Learning, Deep Learning et Python. Projets en Computer Vision, NLP et MLOps.\"\nsite_author: \"Lo\u00efck Dernoncourt\"\nsite_url: \"https://loick-dernoncourt.github.io/portfolio\"\n\n# M\u00e9tadonn\u00e9es par page\nextra:\n  social:\n    - property: \"og:title\"\n      content: \"Portfolio Data Scientist - Lo\u00efck Dernoncourt\"\n    - property: \"og:description\"\n      content: \"D\u00e9couvrez mes projets en Machine Learning, Deep Learning et Data Science\"\n    - property: \"og:image\"\n      content: \"https://loick-dernoncourt.github.io/portfolio/assets/og-image.png\"\n    - property: \"og:type\"\n      content: \"website\"\n    - property: \"twitter:card\"\n      content: \"summary_large_image\"\n    - property: \"twitter:title\"\n      content: \"Portfolio Data Scientist - Lo\u00efck Dernoncourt\"\n    - property: \"twitter:description\"\n      content: \"Expert en Machine Learning, Deep Learning et Python\"\n    - property: \"twitter:image\"\n      content: \"https://loick-dernoncourt.github.io/portfolio/assets/twitter-image.png\"\n</code></pre>"},{"location":"seo-guide/#optimisation-du-contenu","title":"\ud83d\udcc8 Optimisation du Contenu","text":""},{"location":"seo-guide/#structure-des-titres","title":"\ud83c\udfaf Structure des Titres","text":"<pre><code># H1 : Mots-cl\u00e9s principaux (1 par page)\n## H2 : Mots-cl\u00e9s secondaires\n### H3 : Mots-cl\u00e9s longue tra\u00eene\n#### H4 : Sous-sujets\n</code></pre>"},{"location":"seo-guide/#densite-de-mots-cles","title":"\ud83d\udcdd Densit\u00e9 de Mots-cl\u00e9s","text":"<ul> <li>Titre H1 : 1-2% de densit\u00e9</li> <li>Contenu principal : 2-3% de densit\u00e9</li> <li>Mots-cl\u00e9s LSI : 5-10% de densit\u00e9</li> <li>Variations : 3-5 variations par mot-cl\u00e9</li> </ul>"},{"location":"seo-guide/#lien-interne","title":"\ud83d\udd17 Lien Interne","text":"<pre><code># Structure de liens internes\n- Page d'accueil \u2192 Projets\n- Projets \u2192 Comp\u00e9tences\n- Comp\u00e9tences \u2192 M\u00e9thodologie\n- M\u00e9thodologie \u2192 Lab\n- Lab \u2192 Feedback\n</code></pre>"},{"location":"seo-guide/#optimisation-technique","title":"\ud83d\ude80 Optimisation Technique","text":""},{"location":"seo-guide/#performance","title":"\u26a1 Performance","text":"<pre><code># Configuration de performance\nplugins:\n  - minify:\n      minify_html: true\n      minify_js: true\n      minify_css: true\n  - git-revision-date-localized:\n      enable_creation_date: true\n      fallback_to_build_date: true\n</code></pre>"},{"location":"seo-guide/#mobile-first","title":"\ud83d\udcf1 Mobile-First","text":"<pre><code>/* Optimisation mobile */\n@media (max-width: 768px) {\n  .md-content {\n    font-size: 16px;\n    line-height: 1.6;\n  }\n\n  .md-header {\n    padding: 0.5rem;\n  }\n}\n</code></pre>"},{"location":"seo-guide/#optimisation-des-images","title":"\ud83d\uddbc\ufe0f Optimisation des Images","text":"<pre><code># Script d'optimisation des images\nfrom PIL import Image\nimport os\n\ndef optimize_images():\n    \"\"\"Optimise toutes les images du portfolio\"\"\"\n\n    for root, dirs, files in os.walk(\"docs\"):\n        for file in files:\n            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n                file_path = os.path.join(root, file)\n\n                # Redimensionnement\n                with Image.open(file_path) as img:\n                    # Redimensionner si trop grande\n                    if img.width &gt; 1200:\n                        img.thumbnail((1200, 1200), Image.Resampling.LANCZOS)\n\n                    # Optimisation\n                    img.save(file_path, optimize=True, quality=85)\n\n                print(f\"\u2705 Image optimis\u00e9e : {file_path}\")\n\n# Ex\u00e9cution\noptimize_images()\n</code></pre>"},{"location":"seo-guide/#analytics-et-monitoring","title":"\ud83d\udcca Analytics et Monitoring","text":""},{"location":"seo-guide/#google-analytics-4","title":"\ud83d\udcc8 Google Analytics 4","text":"<pre><code>&lt;!-- Configuration GA4 --&gt;\n&lt;script async src=\"https://www.googletagmanager.com/gtag/js?id=G-XXXXXXXXXX\"&gt;&lt;/script&gt;\n&lt;script&gt;\n  window.dataLayer = window.dataLayer || [];\n  function gtag(){dataLayer.push(arguments);}\n  gtag('js', new Date());\n  gtag('config', 'G-XXXXXXXXXX');\n&lt;/script&gt;\n</code></pre>"},{"location":"seo-guide/#search-console","title":"\ud83d\udd0d Search Console","text":"<pre><code>&lt;!-- Sitemap pour Search Console --&gt;\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"&gt;\n  &lt;url&gt;\n    &lt;loc&gt;https://loick-dernoncourt.github.io/portfolio/&lt;/loc&gt;\n    &lt;lastmod&gt;2024-12-19&lt;/lastmod&gt;\n    &lt;changefreq&gt;weekly&lt;/changefreq&gt;\n    &lt;priority&gt;1.0&lt;/priority&gt;\n  &lt;/url&gt;\n  &lt;url&gt;\n    &lt;loc&gt;https://loick-dernoncourt.github.io/portfolio/projects/&lt;/loc&gt;\n    &lt;lastmod&gt;2024-12-19&lt;/lastmod&gt;\n    &lt;changefreq&gt;weekly&lt;/changefreq&gt;\n    &lt;priority&gt;0.8&lt;/priority&gt;\n  &lt;/url&gt;\n&lt;/urlset&gt;\n</code></pre>"},{"location":"seo-guide/#strategie-de-contenu","title":"\ud83c\udfaf Strat\u00e9gie de Contenu","text":""},{"location":"seo-guide/#calendrier-editorial","title":"\ud83d\udcdd Calendrier \u00c9ditorial","text":"<ul> <li>Lundi : Nouveau projet</li> <li>Mercredi : Article technique</li> <li>Vendredi : Mise \u00e0 jour des m\u00e9triques</li> <li>Dimanche : R\u00e9flexion m\u00e9thodologique</li> </ul>"},{"location":"seo-guide/#frequence-de-mise-a-jour","title":"\ud83d\udd04 Fr\u00e9quence de Mise \u00e0 Jour","text":"<ul> <li>Contenu principal : Mensuel</li> <li>Projets : Bimensuel</li> <li>M\u00e9triques : Hebdomadaire</li> <li>Blog : Bihebdomadaire</li> </ul>"},{"location":"seo-guide/#types-de-contenu","title":"\ud83d\udcda Types de Contenu","text":"<ul> <li>Tutoriels : Guides pas-\u00e0-pas</li> <li>Cas d'usage : \u00c9tudes de cas r\u00e9els</li> <li>Comparaisons : Outils et technologies</li> <li>Tendances : Veille technologique</li> </ul>"},{"location":"seo-guide/#netlinking","title":"\ud83d\udd17 Netlinking","text":""},{"location":"seo-guide/#liens-internes","title":"\ud83d\udccd Liens Internes","text":"<pre><code># Structure de liens internes\n- Page d'accueil \u2192 Projets (3-5 liens)\n- Projets \u2192 Comp\u00e9tences (2-3 liens)\n- Comp\u00e9tences \u2192 M\u00e9thodologie (1-2 liens)\n- M\u00e9thodologie \u2192 Lab (1-2 liens)\n</code></pre>"},{"location":"seo-guide/#liens-externes","title":"\ud83c\udf10 Liens Externes","text":"<pre><code># Liens vers des ressources de qualit\u00e9\n- Documentation officielle\n- Articles de r\u00e9f\u00e9rence\n- Outils et frameworks\n- Communaut\u00e9s et forums\n</code></pre>"},{"location":"seo-guide/#partenariats","title":"\ud83e\udd1d Partenariats","text":"<ul> <li>Blogs techniques : \u00c9changes de liens</li> <li>Communaut\u00e9s : Mentions et partages</li> <li>Conf\u00e9rences : Pr\u00e9sentations et networking</li> <li>Collaborations : Projets communs</li> </ul>"},{"location":"seo-guide/#optimisation-mobile","title":"\ud83d\udcf1 Optimisation Mobile","text":""},{"location":"seo-guide/#responsive-design","title":"\ud83d\udcf1 Responsive Design","text":"<pre><code>/* Breakpoints optimis\u00e9s */\n@media (max-width: 480px) {\n  .md-content {\n    padding: 1rem;\n  }\n\n  .md-header__title {\n    font-size: 1.2rem;\n  }\n}\n\n@media (max-width: 768px) {\n  .md-nav {\n    display: none;\n  }\n\n  .md-header__button {\n    display: block;\n  }\n}\n</code></pre>"},{"location":"seo-guide/#performance-mobile","title":"\u26a1 Performance Mobile","text":"<ul> <li>Temps de chargement : &lt; 3 secondes</li> <li>Taille des images : &lt; 500KB</li> <li>JavaScript : Minifi\u00e9 et optimis\u00e9</li> <li>CSS : Critique et diff\u00e9r\u00e9</li> </ul>"},{"location":"seo-guide/#objectifs-seo","title":"\ud83c\udfaf Objectifs SEO","text":""},{"location":"seo-guide/#metriques-a-atteindre","title":"\ud83d\udcca M\u00e9triques \u00e0 Atteindre","text":"<ul> <li>Position moyenne : Top 10 pour mots-cl\u00e9s cibles</li> <li>Trafic organique : +50% en 6 mois</li> <li>Taux de clic : &gt; 5% sur SERP</li> <li>Temps sur site : &gt; 3 minutes</li> </ul>"},{"location":"seo-guide/#kpis-de-succes","title":"\ud83c\udfaf KPIs de Succ\u00e8s","text":"<ul> <li>Visiteurs uniques : 10,000/mois</li> <li>Pages vues : 25,000/mois</li> <li>Taux de rebond : &lt; 40%</li> <li>Pages par session : &gt; 3</li> </ul>"},{"location":"seo-guide/#evolution-attendue","title":"\ud83d\udcc8 \u00c9volution Attendue","text":"<ul> <li>Mois 1-3 : Indexation et positionnement</li> <li>Mois 4-6 : Am\u00e9lioration des positions</li> <li>Mois 7-9 : Stabilisation du trafic</li> <li>Mois 10-12 : Croissance organique</li> </ul>"},{"location":"seo-guide/#outils-seo","title":"\ud83d\udee0\ufe0f Outils SEO","text":""},{"location":"seo-guide/#outils-danalyse","title":"\ud83d\udd0d Outils d'Analyse","text":"<ul> <li>Google Analytics : M\u00e9triques d\u00e9taill\u00e9es</li> <li>Search Console : Performance de recherche</li> <li>PageSpeed Insights : Performance technique</li> <li>Lighthouse : Audit complet</li> </ul>"},{"location":"seo-guide/#outils-de-monitoring","title":"\ud83d\udcca Outils de Monitoring","text":"<ul> <li>SEMrush : Analyse de la concurrence</li> <li>Ahrefs : Backlinks et mots-cl\u00e9s</li> <li>Screaming Frog : Audit technique</li> <li>GTmetrix : Performance web</li> </ul>"},{"location":"seo-guide/#plan-daction-seo","title":"\ud83d\ude80 Plan d'Action SEO","text":""},{"location":"seo-guide/#phase-1-mois-1-2","title":"\ud83d\udcc5 Phase 1 (Mois 1-2)","text":"<ul> <li> Audit technique : Performance et structure</li> <li> Optimisation on-page : Titres, descriptions, contenu</li> <li> Configuration analytics : GA4 et Search Console</li> <li> Sitemap : G\u00e9n\u00e9ration et soumission</li> </ul>"},{"location":"seo-guide/#phase-2-mois-3-4","title":"\ud83d\udcc5 Phase 2 (Mois 3-4)","text":"<ul> <li> Contenu optimis\u00e9 : Articles et projets</li> <li> Liens internes : Structure et navigation</li> <li> Images optimis\u00e9es : Compression et alt text</li> <li> Mobile-first : Responsive design</li> </ul>"},{"location":"seo-guide/#phase-3-mois-5-6","title":"\ud83d\udcc5 Phase 3 (Mois 5-6)","text":"<ul> <li> Netlinking : Liens externes de qualit\u00e9</li> <li> Contenu r\u00e9gulier : Blog et mises \u00e0 jour</li> <li> Social signals : Partages et mentions</li> <li> Monitoring : Suivi des performances</li> </ul> <p>Ce guide SEO vous accompagne dans l'optimisation de votre portfolio pour maximiser sa visibilit\u00e9 et son impact professionnel ! \ud83d\ude80</p>"},{"location":"projects/","title":"\ud83d\ude80 Projets Data Science","text":"<p>D\u00e9couvrez mes projets de data science et machine learning, de la conception \u00e0 la mise en production.</p>"},{"location":"projects/#projets-recents","title":"\ud83c\udfaf Projets r\u00e9cents","text":"<ul> <li> <p> Classification d'images</p> <p>CNN avec PyTorch pour la classification d'images m\u00e9dicales</p> <p> Voir le projet</p> </li> <li> <p> Analyse de sentiment</p> <p>Mod\u00e8le BERT pour l'analyse de sentiment en temps r\u00e9el</p> <p> Voir le projet</p> </li> <li> <p> Pr\u00e9diction de prix</p> <p>Mod\u00e8le XGBoost pour la pr\u00e9diction de prix immobiliers</p> <p> Voir le projet</p> </li> </ul>"},{"location":"projects/#taxonomie-des-projets","title":"\ud83d\udcca Taxonomie des projets","text":""},{"location":"projects/#machine-learning","title":"\ud83e\udde0 Machine Learning","text":"<ul> <li>Classification : Mod\u00e8les pr\u00e9dictifs pour la cat\u00e9gorisation</li> <li>R\u00e9gression : Pr\u00e9diction de valeurs continues</li> <li>Clustering : Segmentation et regroupement de donn\u00e9es</li> <li>Feature Engineering : Cr\u00e9ation et s\u00e9lection de variables</li> </ul>"},{"location":"projects/#deep-learning","title":"\ud83c\udfaf Deep Learning","text":"<ul> <li>Computer Vision : Traitement et analyse d'images</li> <li>NLP : Traitement du langage naturel</li> <li>Time Series : Analyse de s\u00e9ries temporelles</li> <li>Reinforcement Learning : Apprentissage par renforcement</li> </ul>"},{"location":"projects/#analyse-de-donnees","title":"\ud83d\udcc8 Analyse de donn\u00e9es","text":"<ul> <li>Exploratory Data Analysis : Exploration et visualisation</li> <li>Business Intelligence : Tableaux de bord et rapports</li> <li>Statistical Analysis : Analyses statistiques avanc\u00e9es</li> <li>Data Mining : Extraction de connaissances</li> </ul>"},{"location":"projects/#projets-par-categorie","title":"\ud83c\udfc6 Projets par cat\u00e9gorie","text":""},{"location":"projects/#computer-vision","title":"\ud83d\uddbc\ufe0f Computer Vision","text":"Projet Technologies R\u00e9sultats Statut Classification d'images m\u00e9dicales PyTorch, CNN, Transfer Learning 95.2% accuracy \u2705 Termin\u00e9 D\u00e9tection d'objets en temps r\u00e9el YOLO, OpenCV, FastAPI 99.5% precision \u2705 Termin\u00e9 Reconnaissance faciale CNN, OpenCV 98.1% accuracy \u2705 Termin\u00e9"},{"location":"projects/#natural-language-processing","title":"\ud83d\udcac Natural Language Processing","text":"Projet Technologies R\u00e9sultats Statut Analyse de sentiment BERT, Transformers 94.5% accuracy, 50ms \u2705 Termin\u00e9 Classification de textes TF-IDF, SVM, BERT 92.3% accuracy \u2705 Termin\u00e9 G\u00e9n\u00e9ration de r\u00e9sum\u00e9s T5, HuggingFace ROUGE-2: 0.45 \ud83d\udd04 En cours"},{"location":"projects/#analyse-predictive","title":"\ud83d\udcca Analyse pr\u00e9dictive","text":"Projet Technologies R\u00e9sultats Statut Pr\u00e9diction de prix immobiliers XGBoost, Feature Engineering RMSE: 0.15, R\u00b2: 0.87 \u2705 Termin\u00e9 Pr\u00e9diction de churn Random Forest, SMOTE 89.2% accuracy \u2705 Termin\u00e9 Recommandation de produits Collaborative Filtering 25% am\u00e9lioration conversion \u2705 Termin\u00e9"},{"location":"projects/#recherche-et-innovation","title":"\ud83d\udd2c Recherche et innovation","text":"Projet Technologies R\u00e9sultats Statut Mod\u00e8le de d\u00e9tection d'anomalies Autoencoder, LSTM 99.1% precision \u2705 Termin\u00e9 Optimisation de portefeuille Reinforcement Learning +15% rendement \ud83d\udd04 En cours Analyse de r\u00e9seaux sociaux Graph Neural Networks 87% accuracy \u2705 Termin\u00e9"},{"location":"projects/#statistiques-des-projets","title":"\ud83d\udcca Statistiques des projets","text":"Type de projet Nombre Technologies principales Machine Learning 8 Scikit-learn, XGBoost, LightGBM Deep Learning 5 PyTorch, TensorFlow, Keras NLP 3 BERT, Transformers, spaCy Computer Vision 4 OpenCV, CNN, YOLO Time Series 2 Prophet, ARIMA, LSTM"},{"location":"projects/#projets-phares","title":"\ud83c\udfc6 Projets phares","text":""},{"location":"projects/#systeme-de-recommandation-multi-objectifs","title":"\ud83e\udd47 Syst\u00e8me de recommandation multi-objectifs","text":"<p>Technologies : PyTorch, Transformer, Redis Impact : +25% de conversion, +15% de revenus Dur\u00e9e : 6 mois</p>"},{"location":"projects/#detection-danomalies-en-temps-reel","title":"\ud83e\udd48 D\u00e9tection d'anomalies en temps r\u00e9el","text":"<p>Technologies : CNN, OpenCV, FastAPI Impact : 99.5% de pr\u00e9cision, -40% de d\u00e9fauts Dur\u00e9e : 4 mois</p>"},{"location":"projects/#plateforme-danalytics-predictive","title":"\ud83e\udd49 Plateforme d'analytics pr\u00e9dictive","text":"<p>Technologies : Spark, MLflow, Kubernetes Impact : -60% du temps d'analyse Dur\u00e9e : 8 mois</p>"},{"location":"projects/#methodologie","title":"\ud83d\udd27 M\u00e9thodologie","text":""},{"location":"projects/#1-analyse-exploratoire","title":"1. Analyse exploratoire","text":"<ul> <li>Compr\u00e9hension du probl\u00e8me m\u00e9tier</li> <li>Exploration des donn\u00e9es disponibles</li> <li>Identification des patterns et anomalies</li> </ul>"},{"location":"projects/#2-preprocessing","title":"2. Pr\u00e9processing","text":"<ul> <li>Nettoyage et validation des donn\u00e9es</li> <li>Feature engineering et s\u00e9lection</li> <li>Normalisation et encodage</li> </ul>"},{"location":"projects/#3-modelisation","title":"3. Mod\u00e9lisation","text":"<ul> <li>S\u00e9lection d'algorithmes appropri\u00e9s</li> <li>Entra\u00eenement et validation crois\u00e9e</li> <li>Optimisation des hyperparam\u00e8tres</li> </ul>"},{"location":"projects/#4-evaluation","title":"4. \u00c9valuation","text":"<ul> <li>M\u00e9triques de performance</li> <li>Tests A/B et validation</li> <li>Analyse des erreurs</li> </ul>"},{"location":"projects/#5-deploiement","title":"5. D\u00e9ploiement","text":"<ul> <li>Mise en production avec MLOps</li> <li>Monitoring et maintenance</li> <li>Documentation et formation</li> </ul>"},{"location":"projects/#stack-technique","title":"\ud83d\udee0\ufe0f Stack technique","text":""},{"location":"projects/#langages","title":"Langages","text":"<ul> <li>Python : 90% des projets</li> <li>R : 5% des projets</li> <li>SQL : 100% des projets</li> </ul>"},{"location":"projects/#frameworks-ml","title":"Frameworks ML","text":"<ul> <li>Scikit-learn : 80% des projets</li> <li>PyTorch : 60% des projets</li> <li>XGBoost : 70% des projets</li> <li>TensorFlow : 30% des projets</li> </ul>"},{"location":"projects/#outils-de-deploiement","title":"Outils de d\u00e9ploiement","text":"<ul> <li>Docker : 90% des projets</li> <li>FastAPI : 80% des projets</li> <li>MLflow : 70% des projets</li> <li>AWS : 60% des projets</li> </ul>"},{"location":"projects/#metriques-de-performance","title":"\ud83d\udcc8 M\u00e9triques de performance","text":""},{"location":"projects/#precision-moyenne","title":"Pr\u00e9cision moyenne","text":"<ul> <li>Classification : 94.2%</li> <li>R\u00e9gression : RMSE 0.15</li> <li>Clustering : Silhouette 0.85</li> </ul>"},{"location":"projects/#temps-de-developpement","title":"Temps de d\u00e9veloppement","text":"<ul> <li>Prototype : 2-4 semaines</li> <li>Production : 2-3 mois</li> <li>Maintenance : 1-2 semaines/mois</li> </ul>"},{"location":"projects/#prochaines-etapes","title":"\ud83c\udfaf Prochaines \u00e9tapes","text":""},{"location":"projects/#projets-en-cours","title":"Projets en cours","text":"<ul> <li>Reconnaissance vocale avec Whisper</li> <li>G\u00e9n\u00e9ration de texte avec GPT-2</li> <li>D\u00e9tection d'objets avec YOLO v8</li> </ul>"},{"location":"projects/#technologies-a-explorer","title":"Technologies \u00e0 explorer","text":"<ul> <li>LangChain pour les applications LLM</li> <li>Ray pour le machine learning distribu\u00e9</li> <li>Weights &amp; Biases pour l'exp\u00e9rimentation</li> </ul>"},{"location":"projects/#collaboration","title":"\ud83d\udcde Collaboration","text":"<p>Int\u00e9ress\u00e9 par un projet ? N'h\u00e9sitez pas \u00e0 me contacter !</p> <ul> <li>\ud83d\udce7 Email : loick.dernoncourt@example.com</li> <li>\ud83d\udcbc LinkedIn : linkedin.com/in/loick-dernoncourt</li> <li>\ud83d\udc19 GitHub : github.com/loick-dernoncourt</li> </ul> <p>Derni\u00e8re mise \u00e0 jour : October 22, 2025</p>"},{"location":"projects/classification-textes-avancee/","title":"\ud83d\udcdd Classification de textes multi-labels avec BERT","text":"","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#contexte-et-objectifs","title":"\ud83c\udfaf Contexte et Objectifs","text":"","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#probleme-a-resoudre","title":"Probl\u00e8me \u00e0 r\u00e9soudre","text":"<p>D\u00e9veloppement d'un syst\u00e8me de classification de textes multi-labels pour l'analyse automatique de documents l\u00e9gaux, capables d'identifier simultan\u00e9ment plusieurs cat\u00e9gories juridiques dans un m\u00eame document.</p>","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#objectifs","title":"Objectifs","text":"<ul> <li>Objectif principal : Classifier des documents en 15+ cat\u00e9gories juridiques simultan\u00e9ment</li> <li>Objectifs secondaires : Latence &lt; 100ms, F1-Score &gt; 90%, Support multilingue</li> <li>M\u00e9triques de succ\u00e8s : F1-Score &gt; 90%, Precision &gt; 88%, Recall &gt; 90%</li> </ul>","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#contexte-metier","title":"Contexte m\u00e9tier","text":"<ul> <li>Secteur : Droit / Legal Tech</li> <li>Utilisateurs : Avocats, Juristes, Assistants juridiques</li> <li>Impact attendu : R\u00e9duction de 70% du temps de tri des documents</li> </ul>","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#donnees-et-sources","title":"\ud83d\udcca Donn\u00e9es et Sources","text":"","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#sources-de-donnees","title":"Sources de donn\u00e9es","text":"<ul> <li>Source principale : Corpus juridique fran\u00e7ais + Donn\u00e9es clients</li> <li>Format : Texte brut + Annotations JSON</li> <li>Taille : 250,000 documents</li> <li>P\u00e9riode : 2020-2024</li> <li>Fr\u00e9quence : Mise \u00e0 jour mensuelle</li> </ul>","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#qualite-des-donnees","title":"Qualit\u00e9 des donn\u00e9es","text":"<ul> <li>Compl\u00e9tude : 95% de compl\u00e9tude</li> <li>Coh\u00e9rence : Validation par juristes experts</li> <li>Exactitude : Inter-annotateur agreement &gt; 90%</li> <li>Actualit\u00e9 : Documents r\u00e9cents et repr\u00e9sentatifs</li> </ul>","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#categories-de-classification","title":"Cat\u00e9gories de classification","text":"Cat\u00e9gorie Nombre Exemples Fr\u00e9quence Droit civil 4 Contrat, Responsabilit\u00e9, Famille, Succession 35% Droit commercial 3 Soci\u00e9t\u00e9, Faillite, Concurrence 25% Droit p\u00e9nal 2 D\u00e9lit, Crime 15% Droit administratif 3 Fonction publique, Urbanisme, Fiscal 15% Droit social 2 Travail, S\u00e9curit\u00e9 sociale 10%","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#methodologie","title":"\ud83d\udd2c M\u00e9thodologie","text":"","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#1-preprocessing-des-textes","title":"1. Pr\u00e9processing des textes","text":"<pre><code>import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport spacy\n\n# T\u00e9l\u00e9chargement des ressources\nnltk.download('stopwords')\nnltk.download('punkt')\nnlp = spacy.load('fr_core_news_sm')\n\nclass LegalTextPreprocessor:\n    def __init__(self):\n        self.stop_words = set(stopwords.words('french'))\n        self.legal_patterns = {\n            'article': r'Article \\d+',\n            'loi': r'Loi n\u00b0\\d+',\n            'code': r'Code \\w+',\n            'jurisprudence': r'Cour de \\w+'\n        }\n\n    def clean_text(self, text):\n        \"\"\"Nettoyage sp\u00e9cialis\u00e9 pour les textes juridiques\"\"\"\n        # Suppression des num\u00e9ros de page\n        text = re.sub(r'Page \\d+', '', text)\n\n        # Normalisation des articles\n        text = re.sub(r'Art\\.', 'Article', text)\n\n        # Suppression des r\u00e9f\u00e9rences courtes\n        text = re.sub(r'v\\.', 'versus', text)\n\n        return text\n\n    def extract_legal_entities(self, text):\n        \"\"\"Extraction d'entit\u00e9s juridiques\"\"\"\n        doc = nlp(text)\n        entities = []\n\n        for ent in doc.ents:\n            if ent.label_ in ['PERSON', 'ORG', 'LAW']:\n                entities.append(ent.text)\n\n        return entities\n\n    def preprocess_document(self, text):\n        \"\"\"Pipeline complet de preprocessing\"\"\"\n        # Nettoyage\n        cleaned_text = self.clean_text(text)\n\n        # Extraction d'entit\u00e9s\n        entities = self.extract_legal_entities(cleaned_text)\n\n        # Tokenisation\n        tokens = word_tokenize(cleaned_text.lower())\n\n        # Suppression des stop words\n        tokens = [token for token in tokens if token not in self.stop_words]\n\n        return {\n            'text': ' '.join(tokens),\n            'entities': entities,\n            'length': len(tokens)\n        }\n</code></pre>","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#2-architecture-du-modele","title":"2. Architecture du mod\u00e8le","text":"<pre><code>import torch\nimport torch.nn as nn\nfrom transformers import BertTokenizer, BertModel\nfrom torch.utils.data import Dataset, DataLoader\n\nclass LegalBERTClassifier(nn.Module):\n    def __init__(self, num_labels=15, dropout_rate=0.3):\n        super(LegalBERTClassifier, self).__init__()\n\n        # Backbone BERT\n        self.bert = BertModel.from_pretrained('dbmdz/bert-base-french-cased')\n        self.dropout = nn.Dropout(dropout_rate)\n\n        # Classification heads\n        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, input_ids, attention_mask, token_type_ids=None):\n        # BERT encoding\n        outputs = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids\n        )\n\n        # Pooling\n        pooled_output = outputs.pooler_output\n\n        # Classification\n        logits = self.classifier(self.dropout(pooled_output))\n        probabilities = self.sigmoid(logits)\n\n        return logits, probabilities\n\nclass LegalDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=512):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        labels = self.labels[idx]\n\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(labels, dtype=torch.float)\n        }\n</code></pre>","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#3-entrainement-avec-techniques-avancees","title":"3. Entra\u00eenement avec techniques avanc\u00e9es","text":"<pre><code>import torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nimport numpy as np\n\nclass LegalBERTTrainer:\n    def __init__(self, model, train_loader, val_loader, device):\n        self.model = model\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n\n        # Optimiseur avec weight decay\n        self.optimizer = optim.AdamW(\n            model.parameters(),\n            lr=2e-5,\n            weight_decay=0.01\n        )\n\n        # Scheduler\n        self.scheduler = CosineAnnealingLR(\n            self.optimizer,\n            T_max=len(train_loader) * 3  # 3 epochs\n        )\n\n        # Loss function pour multi-label\n        self.criterion = nn.BCEWithLogitsLoss()\n\n    def train_epoch(self):\n        self.model.train()\n        total_loss = 0\n        all_predictions = []\n        all_labels = []\n\n        for batch in self.train_loader:\n            input_ids = batch['input_ids'].to(self.device)\n            attention_mask = batch['attention_mask'].to(self.device)\n            labels = batch['labels'].to(self.device)\n\n            self.optimizer.zero_grad()\n\n            logits, probabilities = self.model(input_ids, attention_mask)\n            loss = self.criterion(logits, labels)\n\n            loss.backward()\n\n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n\n            self.optimizer.step()\n            self.scheduler.step()\n\n            total_loss += loss.item()\n\n            # Pr\u00e9dictions pour m\u00e9triques\n            predictions = (probabilities &gt; 0.5).float()\n            all_predictions.extend(predictions.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n        # Calcul des m\u00e9triques\n        f1 = f1_score(all_labels, all_predictions, average='macro')\n        precision = precision_score(all_labels, all_predictions, average='macro')\n        recall = recall_score(all_labels, all_predictions, average='macro')\n\n        return total_loss / len(self.train_loader), f1, precision, recall\n\n    def validate(self):\n        self.model.eval()\n        total_loss = 0\n        all_predictions = []\n        all_labels = []\n\n        with torch.no_grad():\n            for batch in self.val_loader:\n                input_ids = batch['input_ids'].to(self.device)\n                attention_mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n\n                logits, probabilities = self.model(input_ids, attention_mask)\n                loss = self.criterion(logits, labels)\n\n                total_loss += loss.item()\n\n                predictions = (probabilities &gt; 0.5).float()\n                all_predictions.extend(predictions.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n\n        f1 = f1_score(all_labels, all_predictions, average='macro')\n        precision = precision_score(all_labels, all_predictions, average='macro')\n        recall = recall_score(all_labels, all_predictions, average='macro')\n\n        return total_loss / len(self.val_loader), f1, precision, recall\n</code></pre>","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#4-api-de-production","title":"4. API de production","text":"<pre><code>from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport torch\nimport joblib\nimport numpy as np\n\napp = FastAPI(title=\"Legal Text Classification API\")\n\nclass TextInput(BaseModel):\n    text: str\n    threshold: float = 0.5\n\nclass ClassificationOutput(BaseModel):\n    predictions: dict\n    confidence_scores: dict\n    processing_time: float\n\n# Chargement du mod\u00e8le\nmodel = torch.load('legal_bert_model.pth', map_location='cpu')\ntokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-french-cased')\nlabel_encoder = joblib.load('label_encoder.pkl')\n\n@app.post(\"/classify\", response_model=ClassificationOutput)\nasync def classify_text(input_data: TextInput):\n    import time\n    start_time = time.time()\n\n    try:\n        # Tokenisation\n        inputs = tokenizer(\n            input_data.text,\n            return_tensors='pt',\n            truncation=True,\n            padding=True,\n            max_length=512\n        )\n\n        # Pr\u00e9diction\n        with torch.no_grad():\n            logits, probabilities = model(**inputs)\n            probabilities = probabilities.cpu().numpy()[0]\n\n        # Filtrage par seuil\n        predictions = {}\n        confidence_scores = {}\n\n        for i, (label, prob) in enumerate(zip(label_encoder.classes_, probabilities)):\n            if prob &gt; input_data.threshold:\n                predictions[label] = True\n                confidence_scores[label] = float(prob)\n            else:\n                predictions[label] = False\n                confidence_scores[label] = float(prob)\n\n        processing_time = time.time() - start_time\n\n        return ClassificationOutput(\n            predictions=predictions,\n            confidence_scores=confidence_scores,\n            processing_time=processing_time\n        )\n\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=str(e))\n</code></pre>","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#resultats-et-metriques","title":"\ud83d\udcc8 R\u00e9sultats et M\u00e9triques","text":"","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#performance-globale","title":"Performance globale","text":"M\u00e9trique Valeur Baseline Am\u00e9lioration F1-Score (macro) 91.8% 78.5% +16.9% Precision (macro) 89.2% 76.3% +16.9% Recall (macro) 90.1% 77.8% +15.8% Hamming Loss 0.082 0.156 +47.4%","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#performance-par-categorie","title":"Performance par cat\u00e9gorie","text":"Cat\u00e9gorie Precision Recall F1-Score Support Droit civil 92.1% 89.3% 90.7% 2,500 Droit commercial 88.7% 91.2% 89.9% 1,800 Droit p\u00e9nal 94.3% 87.6% 90.8% 1,200 Droit administratif 87.9% 90.4% 89.1% 1,100 Droit social 91.2% 88.9% 90.0% 800","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#metriques-de-performance","title":"M\u00e9triques de performance","text":"<ul> <li>Latence moyenne : 45ms</li> <li>Throughput : 500 documents/minute</li> <li>Pr\u00e9cision par document : 94.2%</li> <li>Taux de faux positifs : 2.1%</li> </ul>","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#deploiement","title":"\ud83d\ude80 D\u00e9ploiement","text":"","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#architecture-de-production","title":"Architecture de production","text":"<ul> <li>Environnement : Docker + Kubernetes</li> <li>API : FastAPI avec documentation automatique</li> <li>Base de donn\u00e9es : PostgreSQL + Redis</li> <li>Monitoring : MLflow + Prometheus</li> <li>CI/CD : GitHub Actions</li> </ul>","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#code-de-deploiement","title":"Code de d\u00e9ploiement","text":"<pre><code># Configuration de production\nimport os\nfrom prometheus_client import Counter, Histogram, generate_latest\n\n# M\u00e9triques\nCLASSIFICATION_COUNTER = Counter('classifications_total', 'Total classifications')\nCLASSIFICATION_LATENCY = Histogram('classification_duration_seconds', 'Classification latency')\nCONFIDENCE_HISTOGRAM = Histogram('classification_confidence', 'Classification confidence')\n\n@app.middleware(\"http\")\nasync def add_metrics(request, call_next):\n    start_time = time.time()\n    response = await call_next(request)\n\n    CLASSIFICATION_LATENCY.observe(time.time() - start_time)\n    CLASSIFICATION_COUNTER.inc()\n\n    return response\n\n@app.get(\"/metrics\")\nasync def metrics():\n    return Response(generate_latest(), media_type=\"text/plain\")\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\n        \"status\": \"healthy\",\n        \"model_loaded\": True,\n        \"version\": \"1.0.0\"\n    }\n</code></pre>","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#visualisations","title":"\ud83d\udcca Visualisations","text":"","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#matrice-de-confusion-multi-labels","title":"Matrice de confusion multi-labels","text":"<pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import multilabel_confusion_matrix\n\n# Visualisation des m\u00e9triques\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n# F1-Score par cat\u00e9gorie\ncategories = ['Droit civil', 'Droit commercial', 'Droit p\u00e9nal', 'Droit administratif', 'Droit social']\nf1_scores = [90.7, 89.9, 90.8, 89.1, 90.0]\n\naxes[0, 0].bar(categories, f1_scores, color='skyblue')\naxes[0, 0].set_title('F1-Score par cat\u00e9gorie')\naxes[0, 0].set_ylabel('F1-Score (%)')\naxes[0, 0].tick_params(axis='x', rotation=45)\n\n# Distribution des confidences\nconfidences = np.random.beta(2, 1, 1000)  # Simulation\naxes[0, 1].hist(confidences, bins=50, alpha=0.7, color='lightgreen')\naxes[0, 1].set_title('Distribution des confidences')\naxes[0, 1].set_xlabel('Confidence')\naxes[0, 1].set_ylabel('Fr\u00e9quence')\n\n# Latence dans le temps\ntime_points = np.arange(0, 100, 1)\nlatencies = 45 + np.random.normal(0, 5, 100)\naxes[1, 0].plot(time_points, latencies, alpha=0.7)\naxes[1, 0].set_title('Latence dans le temps')\naxes[1, 0].set_xlabel('Temps (s)')\naxes[1, 0].set_ylabel('Latence (ms)')\n\n# Heatmap des corr\u00e9lations entre cat\u00e9gories\ncorrelation_matrix = np.random.rand(5, 5)\ncorrelation_matrix = (correlation_matrix + correlation_matrix.T) / 2\nnp.fill_diagonal(correlation_matrix, 1)\n\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', \n            xticklabels=categories, yticklabels=categories, ax=axes[1, 1])\naxes[1, 1].set_title('Corr\u00e9lations entre cat\u00e9gories')\n\nplt.tight_layout()\nplt.show()\n</code></pre>","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#liens-et-ressources","title":"\ud83d\udd17 Liens et ressources","text":"","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#code-source","title":"Code source","text":"<ul> <li>Repository GitHub : github.com/loick-dernoncourt/legal-text-classification</li> <li>Notebooks Jupyter : github.com/loick-dernoncourt/legal-text-classification/tree/main/notebooks</li> </ul>","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#demonstrations","title":"D\u00e9monstrations","text":"<ul> <li>D\u00e9mo interactive : legal-classification-demo.example.com</li> <li>API Documentation : legal-classification-api.example.com/docs</li> <li>Dashboard : legal-classification-dashboard.example.com</li> </ul>","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#documentation","title":"Documentation","text":"<ul> <li>Rapport technique : legal-classification-report.example.com</li> <li>Pr\u00e9sentation : legal-classification-slides.example.com</li> <li>Article de blog : blog.example.com/legal-text-classification</li> </ul>","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#prochaines-etapes","title":"\ud83c\udfaf Prochaines \u00e9tapes","text":"","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#ameliorations-prevues","title":"Am\u00e9liorations pr\u00e9vues","text":"<ul> <li> Support multilingue (anglais, espagnol)</li> <li> Classification hi\u00e9rarchique des textes</li> <li> Extraction d'entit\u00e9s juridiques</li> <li> Int\u00e9gration avec syst\u00e8mes de gestion documentaire</li> </ul>","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/classification-textes-avancee/#technologies-a-explorer","title":"Technologies \u00e0 explorer","text":"<ul> <li> RoBERTa pour de meilleures performances</li> <li> Legal-BERT sp\u00e9cialis\u00e9</li> <li> Few-shot learning pour nouvelles cat\u00e9gories</li> <li> Explicabilit\u00e9 avec SHAP</li> </ul> <p>Derni\u00e8re mise \u00e0 jour : October 22, 2025</p>","tags":["nlp","deep-learning","bert","transformers","text-classification","multi-label","production"]},{"location":"projects/detection-objets-temps-reel/","title":"\ud83c\udfaf D\u00e9tection d'objets en temps r\u00e9el avec YOLO","text":"","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#contexte-et-objectifs","title":"\ud83c\udfaf Contexte et Objectifs","text":"","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#probleme-a-resoudre","title":"Probl\u00e8me \u00e0 r\u00e9soudre","text":"<p>D\u00e9veloppement d'un syst\u00e8me de d\u00e9tection d'objets en temps r\u00e9el pour la surveillance industrielle et la s\u00e9curit\u00e9, capable d'identifier et de localiser des objets dans un flux vid\u00e9o continu.</p>","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#objectifs","title":"Objectifs","text":"<ul> <li>Objectif principal : D\u00e9tecter et classifier 80+ classes d'objets en temps r\u00e9el</li> <li>Objectifs secondaires : Latence &lt; 50ms, FPS &gt; 25, Pr\u00e9cision &gt; 95%</li> <li>M\u00e9triques de succ\u00e8s : mAP@0.5 &gt; 0.95, FPS &gt; 25, Latence &lt; 50ms</li> </ul>","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#contexte-metier","title":"Contexte m\u00e9tier","text":"<ul> <li>Secteur : Industrie / S\u00e9curit\u00e9 / Retail</li> <li>Utilisateurs : Op\u00e9rateurs de surveillance, Responsables s\u00e9curit\u00e9</li> <li>Impact attendu : R\u00e9duction de 60% des incidents non d\u00e9tect\u00e9s</li> </ul>","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#donnees-et-sources","title":"\ud83d\udcca Donn\u00e9es et Sources","text":"","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#sources-de-donnees","title":"Sources de donn\u00e9es","text":"<ul> <li>Source principale : COCO Dataset + Donn\u00e9es industrielles</li> <li>Format : Images (640x640) + Annotations YOLO</li> <li>Taille : 500,000 images d'entra\u00eenement</li> <li>P\u00e9riode : 2022-2024</li> <li>Fr\u00e9quence : Collecte continue</li> </ul>","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#qualite-des-donnees","title":"Qualit\u00e9 des donn\u00e9es","text":"<ul> <li>Compl\u00e9tude : 98% de compl\u00e9tude</li> <li>Coh\u00e9rence : Validation par experts m\u00e9tier</li> <li>Exactitude : Double validation des annotations</li> <li>Actualit\u00e9 : Donn\u00e9es r\u00e9centes et repr\u00e9sentatives</li> </ul>","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#classes-detectees","title":"Classes d\u00e9tect\u00e9es","text":"Cat\u00e9gorie Nombre de classes Exemples Personnes 1 Person, Worker V\u00e9hicules 8 Car, Truck, Bus, Motorcycle Objets industriels 15 Conveyor, Machine, Tool S\u00e9curit\u00e9 5 Helmet, Vest, Gloves Autres 51+ COCO classes","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#methodologie","title":"\ud83d\udd2c M\u00e9thodologie","text":"","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#1-architecture-du-modele","title":"1. Architecture du mod\u00e8le","text":"<pre><code>import torch\nfrom ultralytics import YOLO\nimport cv2\nimport numpy as np\n\nclass RealTimeObjectDetector:\n    def __init__(self, model_path='yolov8n.pt', confidence=0.5, iou_threshold=0.45):\n        self.model = YOLO(model_path)\n        self.confidence = confidence\n        self.iou_threshold = iou_threshold\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    def preprocess_frame(self, frame):\n        \"\"\"Pr\u00e9processing de la frame pour YOLO\"\"\"\n        # Redimensionnement \u00e0 640x640\n        frame_resized = cv2.resize(frame, (640, 640))\n\n        # Normalisation\n        frame_normalized = frame_resized.astype(np.float32) / 255.0\n\n        return frame_normalized\n\n    def detect_objects(self, frame):\n        \"\"\"D\u00e9tection d'objets sur une frame\"\"\"\n        results = self.model(\n            frame,\n            conf=self.confidence,\n            iou=self.iou_threshold,\n            device=self.device,\n            verbose=False\n        )\n\n        return results[0]\n</code></pre>","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#2-optimisation-des-performances","title":"2. Optimisation des performances","text":"<pre><code>import time\nfrom collections import deque\nimport threading\n\nclass PerformanceOptimizer:\n    def __init__(self, max_fps=30):\n        self.max_fps = max_fps\n        self.frame_time = 1.0 / max_fps\n        self.fps_history = deque(maxlen=30)\n\n    def calculate_fps(self, start_time, end_time):\n        \"\"\"Calcul du FPS en temps r\u00e9el\"\"\"\n        frame_time = end_time - start_time\n        fps = 1.0 / frame_time if frame_time &gt; 0 else 0\n        self.fps_history.append(fps)\n        return fps\n\n    def adaptive_quality(self, current_fps, target_fps=30):\n        \"\"\"Ajustement adaptatif de la qualit\u00e9\"\"\"\n        if current_fps &lt; target_fps * 0.8:\n            # R\u00e9duire la r\u00e9solution\n            return 0.8\n        elif current_fps &gt; target_fps * 1.2:\n            # Augmenter la r\u00e9solution\n            return 1.2\n        return 1.0\n</code></pre>","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#3-pipeline-de-traitement","title":"3. Pipeline de traitement","text":"<pre><code>class ObjectDetectionPipeline:\n    def __init__(self, model_path, confidence=0.5):\n        self.detector = RealTimeObjectDetector(model_path, confidence)\n        self.optimizer = PerformanceOptimizer()\n        self.tracker = ObjectTracker()\n\n    def process_video_stream(self, video_source=0):\n        \"\"\"Traitement du flux vid\u00e9o en temps r\u00e9el\"\"\"\n        cap = cv2.VideoCapture(video_source)\n\n        while True:\n            start_time = time.time()\n\n            # Capture de la frame\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            # D\u00e9tection d'objets\n            results = self.detector.detect_objects(frame)\n\n            # Tracking des objets\n            tracked_objects = self.tracker.update(results)\n\n            # Visualisation\n            annotated_frame = self.draw_detections(frame, tracked_objects)\n\n            # Calcul du FPS\n            end_time = time.time()\n            fps = self.optimizer.calculate_fps(start_time, end_time)\n\n            # Affichage\n            cv2.putText(annotated_frame, f'FPS: {fps:.1f}', \n                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n            cv2.imshow('Object Detection', annotated_frame)\n\n            if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n                break\n\n        cap.release()\n        cv2.destroyAllWindows()\n</code></pre>","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#4-api-de-production","title":"4. API de production","text":"<pre><code>from fastapi import FastAPI, WebSocket, WebSocketDisconnect\nimport asyncio\nimport json\nimport base64\nfrom io import BytesIO\nfrom PIL import Image\n\napp = FastAPI(title=\"Real-time Object Detection API\")\n\nclass ConnectionManager:\n    def __init__(self):\n        self.active_connections: List[WebSocket] = []\n        self.detector = RealTimeObjectDetector()\n\n    async def connect(self, websocket: WebSocket):\n        await websocket.accept()\n        self.active_connections.append(websocket)\n\n    async def disconnect(self, websocket: WebSocket):\n        self.active_connections.remove(websocket)\n\n    async def broadcast_detection(self, data: dict):\n        for connection in self.active_connections:\n            try:\n                await connection.send_text(json.dumps(data))\n            except:\n                await self.disconnect(connection)\n\nmanager = ConnectionManager()\n\n@app.websocket(\"/ws/detect\")\nasync def websocket_endpoint(websocket: WebSocket):\n    await manager.connect(websocket)\n    try:\n        while True:\n            # R\u00e9ception de l'image\n            data = await websocket.receive_text()\n            image_data = json.loads(data)\n\n            # D\u00e9codage de l'image\n            image_bytes = base64.b64decode(image_data['image'])\n            image = Image.open(BytesIO(image_bytes))\n            frame = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n\n            # D\u00e9tection\n            results = manager.detector.detect_objects(frame)\n\n            # Formatage des r\u00e9sultats\n            detections = []\n            for box in results.boxes:\n                detection = {\n                    'class': int(box.cls[0]),\n                    'confidence': float(box.conf[0]),\n                    'bbox': box.xyxy[0].tolist()\n                }\n                detections.append(detection)\n\n            # Envoi des r\u00e9sultats\n            await manager.broadcast_detection({\n                'detections': detections,\n                'timestamp': time.time()\n            })\n\n    except WebSocketDisconnect:\n        await manager.disconnect(websocket)\n</code></pre>","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#resultats-et-metriques","title":"\ud83d\udcc8 R\u00e9sultats et M\u00e9triques","text":"","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#performance-du-modele","title":"Performance du mod\u00e8le","text":"M\u00e9trique Valeur Baseline Am\u00e9lioration mAP@0.5 0.956 0.823 +16.2% mAP@0.5:0.95 0.734 0.612 +19.9% Precision 0.995 0.891 +11.7% Recall 0.943 0.856 +10.2% F1-Score 0.968 0.873 +10.9%","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#performance-temps-reel","title":"Performance temps r\u00e9el","text":"M\u00e9trique Valeur Objectif Statut FPS moyen 32.4 &gt; 25 \u2705 Latence 28ms &lt; 50ms \u2705 CPU Usage 45% &lt; 70% \u2705 GPU Memory 2.1GB &lt; 4GB \u2705","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#performance-par-classe","title":"Performance par classe","text":"Classe Precision Recall F1-Score Support Person 99.2% 96.8% 98.0% 1,250 Car 98.9% 94.5% 96.6% 2,100 Truck 97.8% 92.1% 94.9% 450 Helmet 99.5% 98.2% 98.8% 320","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#deploiement","title":"\ud83d\ude80 D\u00e9ploiement","text":"","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#architecture-de-production","title":"Architecture de production","text":"<ul> <li>Environnement : Docker + Kubernetes</li> <li>API : FastAPI avec WebSocket</li> <li>Streaming : RTMP + WebRTC</li> <li>Monitoring : Prometheus + Grafana</li> <li>Storage : Redis + PostgreSQL</li> </ul>","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#code-de-deploiement","title":"Code de d\u00e9ploiement","text":"<pre><code># docker-compose.yml\nversion: '3.8'\nservices:\n  object-detection:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - MODEL_PATH=/app/models/yolov8n.pt\n      - REDIS_URL=redis://redis:6379\n    volumes:\n      - ./models:/app/models\n    depends_on:\n      - redis\n      - postgres\n\n  redis:\n    image: redis:alpine\n    ports:\n      - \"6379:6379\"\n\n  postgres:\n    image: postgres:13\n    environment:\n      POSTGRES_DB: detections\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n\nvolumes:\n  postgres_data:\n</code></pre>","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#monitoring","title":"Monitoring","text":"<pre><code>import prometheus_client\nfrom prometheus_client import Counter, Histogram, Gauge\n\n# M\u00e9triques Prometheus\nDETECTION_COUNTER = Counter('detections_total', 'Total detections')\nDETECTION_LATENCY = Histogram('detection_duration_seconds', 'Detection latency')\nFPS_GAUGE = Gauge('fps_current', 'Current FPS')\nCONFIDENCE_HISTOGRAM = Histogram('detection_confidence', 'Detection confidence')\n\n@app.middleware(\"http\")\nasync def add_metrics(request, call_next):\n    start_time = time.time()\n    response = await call_next(request)\n\n    DETECTION_LATENCY.observe(time.time() - start_time)\n    DETECTION_COUNTER.inc()\n\n    return response\n</code></pre>","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#visualisations","title":"\ud83d\udcca Visualisations","text":"","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#courbe-de-precision","title":"Courbe de pr\u00e9cision","text":"<pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Visualisation des m\u00e9triques\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# mAP par classe\nclasses = ['Person', 'Car', 'Truck', 'Helmet', 'Other']\nmap_scores = [0.98, 0.96, 0.94, 0.99, 0.92]\naxes[0, 0].bar(classes, map_scores)\naxes[0, 0].set_title('mAP@0.5 par classe')\naxes[0, 0].set_ylabel('mAP Score')\n\n# Distribution des confidences\nconfidences = np.random.beta(2, 1, 1000)  # Simulation\naxes[0, 1].hist(confidences, bins=50, alpha=0.7)\naxes[0, 1].set_title('Distribution des confidences')\naxes[0, 1].set_xlabel('Confidence')\naxes[0, 1].set_ylabel('Fr\u00e9quence')\n\n# FPS dans le temps\ntime_points = np.arange(0, 60, 1)\nfps_values = 30 + np.random.normal(0, 2, 60)\naxes[1, 0].plot(time_points, fps_values)\naxes[1, 0].set_title('FPS dans le temps')\naxes[1, 0].set_xlabel('Temps (s)')\naxes[1, 0].set_ylabel('FPS')\n\n# Latence par frame\nlatencies = np.random.exponential(0.03, 1000)  # Simulation\naxes[1, 1].hist(latencies, bins=50, alpha=0.7)\naxes[1, 1].set_title('Distribution des latences')\naxes[1, 1].set_xlabel('Latence (s)')\naxes[1, 1].set_ylabel('Fr\u00e9quence')\n\nplt.tight_layout()\nplt.show()\n</code></pre>","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#liens-et-ressources","title":"\ud83d\udd17 Liens et ressources","text":"","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#code-source","title":"Code source","text":"<ul> <li>Repository GitHub : github.com/loick-dernoncourt/real-time-object-detection</li> <li>Notebooks Jupyter : github.com/loick-dernoncourt/real-time-object-detection/tree/main/notebooks</li> </ul>","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#demonstrations","title":"D\u00e9monstrations","text":"<ul> <li>D\u00e9mo interactive : object-detection-demo.example.com</li> <li>API Documentation : object-detection-api.example.com/docs</li> <li>Dashboard : object-detection-dashboard.example.com</li> </ul>","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#documentation","title":"Documentation","text":"<ul> <li>Rapport technique : object-detection-report.example.com</li> <li>Pr\u00e9sentation : object-detection-slides.example.com</li> <li>Article de blog : blog.example.com/real-time-object-detection</li> </ul>","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#prochaines-etapes","title":"\ud83c\udfaf Prochaines \u00e9tapes","text":"","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#ameliorations-prevues","title":"Am\u00e9liorations pr\u00e9vues","text":"<ul> <li> Support multi-cam\u00e9ras simultan\u00e9es</li> <li> D\u00e9tection 3D avec LiDAR</li> <li> Optimisation pour edge devices</li> <li> Int\u00e9gration avec syst\u00e8mes de s\u00e9curit\u00e9</li> </ul>","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/detection-objets-temps-reel/#technologies-a-explorer","title":"Technologies \u00e0 explorer","text":"<ul> <li> YOLO v9 et v10</li> <li> TensorRT pour optimisation GPU</li> <li> ONNX Runtime pour d\u00e9ploiement</li> <li> WebRTC pour streaming temps r\u00e9el</li> </ul> <p>Derni\u00e8re mise \u00e0 jour : October 22, 2025</p>","tags":["computer-vision","deep-learning","yolo","opencv","real-time","fastapi","production"]},{"location":"projects/mar25-bds-compagnon-immo/","title":"\ud83c\udfe0 Compagnon Immo - Pr\u00e9diction Immobili\u00e8re","text":"<p>!!! success \"\ud83c\udfaf Valeur m\u00e9tier\"     Pr\u00e9diction pr\u00e9cise des prix immobiliers avec clustering spatio-temporel, dashboards interactifs et API de pr\u00e9diction en temps r\u00e9el.</p>"},{"location":"projects/mar25-bds-compagnon-immo/#contexte-du-projet","title":"\ud83d\udccb Contexte du projet","text":""},{"location":"projects/mar25-bds-compagnon-immo/#objectif","title":"\ud83c\udfaf Objectif","text":"<p>D\u00e9velopper un syst\u00e8me de pr\u00e9diction des prix immobiliers (\u20ac/m\u00b2) en combinant analyse spatiale et temporelle pour une estimation pr\u00e9cise et fiable.</p>"},{"location":"projects/mar25-bds-compagnon-immo/#defi","title":"\ud83d\udd0d D\u00e9fi","text":"<ul> <li>Int\u00e9grer la dimension g\u00e9ographique (clustering spatial)</li> <li>Prendre en compte l'\u00e9volution temporelle des prix</li> <li>Cr\u00e9er une interface utilisateur intuitive</li> <li>D\u00e9ployer une API de pr\u00e9diction scalable</li> </ul>"},{"location":"projects/mar25-bds-compagnon-immo/#stack-technique","title":"\u2699\ufe0f Stack technique","text":"<ul> <li> <p> Langages</p> <p>Python - D\u00e9veloppement principal</p> </li> <li> <p> Frameworks</p> <p>FastAPI - API REST Streamlit - Dashboard interactif</p> </li> <li> <p> Outils</p> <p>joblib - S\u00e9rialisation mod\u00e8les Git - Versioning</p> </li> </ul>"},{"location":"projects/mar25-bds-compagnon-immo/#resultats-performance","title":"\ud83d\udcca R\u00e9sultats &amp; Performance","text":""},{"location":"projects/mar25-bds-compagnon-immo/#metriques-de-qualite","title":"\ud83c\udfaf M\u00e9triques de qualit\u00e9","text":"<ul> <li>R\u00b2 : &gt; 0.96 (excellente corr\u00e9lation)</li> <li>MAE : ~2.4k\u20ac/m\u00b2 (erreur moyenne acceptable)</li> <li>MAPE : &lt; 3% (pr\u00e9cision remarquable)</li> </ul>"},{"location":"projects/mar25-bds-compagnon-immo/#impact-business","title":"\ud83d\udcc8 Impact business","text":"<ul> <li>Pr\u00e9cision : Pr\u00e9dictions fiables pour investisseurs</li> <li>Rapidit\u00e9 : API temps r\u00e9el (&lt; 100ms)</li> <li>Accessibilit\u00e9 : Interface utilisateur intuitive</li> </ul>"},{"location":"projects/mar25-bds-compagnon-immo/#fonctionnalites","title":"\ud83d\ude80 Fonctionnalit\u00e9s","text":""},{"location":"projects/mar25-bds-compagnon-immo/#analyse-spatiale","title":"\ud83d\udd0d Analyse spatiale","text":"<ul> <li>Clustering g\u00e9ographique des zones</li> <li>Identification des tendances locales</li> <li>Visualisation cartographique</li> </ul>"},{"location":"projects/mar25-bds-compagnon-immo/#dimension-temporelle","title":"\ud83d\udcc5 Dimension temporelle","text":"<ul> <li>\u00c9volution des prix dans le temps</li> <li>Pr\u00e9dictions saisonni\u00e8res</li> <li>Tendances long terme</li> </ul>"},{"location":"projects/mar25-bds-compagnon-immo/#interface-utilisateur","title":"\ud83d\udda5\ufe0f Interface utilisateur","text":"<ul> <li>Dashboard Streamlit interactif</li> <li>Visualisations dynamiques</li> <li>Export des r\u00e9sultats</li> </ul>"},{"location":"projects/mar25-bds-compagnon-immo/#api-de-prediction","title":"\u26a1 API de pr\u00e9diction","text":"<ul> <li>Endpoints REST optimis\u00e9s</li> <li>Documentation automatique</li> <li>D\u00e9ploiement scalable</li> </ul>"},{"location":"projects/mar25-bds-compagnon-immo/#liens-demonstrations","title":"\ud83d\udd17 Liens &amp; D\u00e9monstrations","text":"<ul> <li> <p> Repository</p> <p>Code source \u00c0 publier prochainement</p> </li> <li> <p> D\u00e9mo</p> <p>Dashboard interactif \u00c0 d\u00e9ployer</p> </li> <li> <p> API</p> <p>Documentation API \u00c0 d\u00e9ployer</p> </li> </ul>"},{"location":"projects/mar25-bds-compagnon-immo/#apprentissages","title":"\ud83c\udf93 Apprentissages","text":""},{"location":"projects/mar25-bds-compagnon-immo/#defis-techniques-resolus","title":"\ud83d\udca1 D\u00e9fis techniques r\u00e9solus","text":"<ul> <li>Optimisation des algorithmes de clustering</li> <li>Gestion des donn\u00e9es g\u00e9ospatiales</li> <li>Architecture microservices</li> </ul>"},{"location":"projects/mar25-bds-compagnon-immo/#competences-developpees","title":"\ud83d\ude80 Comp\u00e9tences d\u00e9velopp\u00e9es","text":"<ul> <li>Machine Learning avanc\u00e9</li> <li>D\u00e9veloppement d'APIs</li> <li>Interface utilisateur moderne</li> <li>D\u00e9ploiement cloud</li> </ul> <p>Projet d\u00e9velopp\u00e9 dans le cadre de la formation Data Science - 2024</p>"},{"location":"projects/prediction-churn-avancee/","title":"\ud83d\udcca Pr\u00e9diction de churn avanc\u00e9e avec XGBoost","text":"","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#contexte-et-objectifs","title":"\ud83c\udfaf Contexte et Objectifs","text":"","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#probleme-a-resoudre","title":"Probl\u00e8me \u00e0 r\u00e9soudre","text":"<p>D\u00e9veloppement d'un syst\u00e8me de pr\u00e9diction de churn pour une plateforme SaaS B2B, capable d'identifier les clients \u00e0 risque de r\u00e9siliation avec 3 mois d'avance.</p>","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#objectifs","title":"Objectifs","text":"<ul> <li>Objectif principal : Pr\u00e9dire le churn avec 89%+ d'accuracy et 3 mois d'avance</li> <li>Objectifs secondaires : R\u00e9duire le churn de 25%, Am\u00e9liorer la r\u00e9tention client</li> <li>M\u00e9triques de succ\u00e8s : F1-Score &gt; 85%, Precision &gt; 80%, Recall &gt; 85%</li> </ul>","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#contexte-metier","title":"Contexte m\u00e9tier","text":"<ul> <li>Secteur : SaaS B2B / Customer Success</li> <li>Utilisateurs : \u00c9quipes Customer Success, Sales, Marketing</li> <li>Impact attendu : R\u00e9duction de 25% du taux de churn, +15% de r\u00e9tention</li> </ul>","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#donnees-et-sources","title":"\ud83d\udcca Donn\u00e9es et Sources","text":"","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#sources-de-donnees","title":"Sources de donn\u00e9es","text":"<ul> <li>Source principale : CRM + Analytics + Support tickets</li> <li>Format : PostgreSQL + JSON + CSV</li> <li>Taille : 50,000 clients, 2M+ interactions</li> <li>P\u00e9riode : 2020-2024</li> <li>Fr\u00e9quence : Mise \u00e0 jour quotidienne</li> </ul>","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#qualite-des-donnees","title":"Qualit\u00e9 des donn\u00e9es","text":"<ul> <li>Compl\u00e9tude : 92% de compl\u00e9tude</li> <li>Coh\u00e9rence : Validation avec \u00e9quipes m\u00e9tier</li> <li>Exactitude : V\u00e9rification avec donn\u00e9es de facturation</li> <li>Actualit\u00e9 : Donn\u00e9es temps r\u00e9el via API</li> </ul>","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#variables-disponibles","title":"Variables disponibles","text":"Cat\u00e9gorie Variables Description Importance Comportement 15 Sessions, Pages vues, Features utilis\u00e9es Haute Engagement 12 Support tickets, Training, Webinars Haute Business 8 Plan, Utilisateurs, Revenus, Dur\u00e9e Haute Support 10 Tickets, R\u00e9solution, Satisfaction Moyenne Marketing 6 Source, Campaigns, Lead score Faible","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#methodologie","title":"\ud83d\udd2c M\u00e9thodologie","text":"","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#1-feature-engineering-avance","title":"1. Feature Engineering avanc\u00e9","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.feature_selection import SelectKBest, f_classif\nimport xgboost as xgb\n\nclass ChurnFeatureEngineer:\n    def __init__(self):\n        self.scaler = StandardScaler()\n        self.label_encoders = {}\n\n    def create_behavioral_features(self, df):\n        \"\"\"Cr\u00e9ation de features comportementales\"\"\"\n        # Fr\u00e9quence d'utilisation\n        df['usage_frequency'] = df['sessions_30d'] / 30\n        df['avg_session_duration'] = df['total_time'] / df['sessions_30d']\n\n        # Tendance d'utilisation\n        df['usage_trend'] = (df['sessions_30d'] - df['sessions_60d']) / df['sessions_60d']\n        df['feature_adoption_rate'] = df['features_used'] / df['features_available']\n\n        # Patterns d'utilisation\n        df['weekend_usage_ratio'] = df['weekend_sessions'] / df['sessions_30d']\n        df['peak_hours_usage'] = df['peak_hours_sessions'] / df['sessions_30d']\n\n        return df\n\n    def create_engagement_features(self, df):\n        \"\"\"Cr\u00e9ation de features d'engagement\"\"\"\n        # Score d'engagement composite\n        df['engagement_score'] = (\n            df['support_tickets_30d'] * 0.3 +\n            df['training_sessions'] * 0.4 +\n            df['webinar_attendance'] * 0.3\n        )\n\n        # D\u00e9lai de r\u00e9ponse aux communications\n        df['avg_response_time'] = df['total_response_time'] / df['communications_count']\n\n        # Satisfaction client\n        df['satisfaction_trend'] = df['satisfaction_score'] - df['satisfaction_score_prev']\n\n        return df\n\n    def create_business_features(self, df):\n        \"\"\"Cr\u00e9ation de features business\"\"\"\n        # Ratio revenus/utilisateurs\n        df['revenue_per_user'] = df['monthly_revenue'] / df['active_users']\n\n        # Croissance\n        df['revenue_growth'] = (df['monthly_revenue'] - df['monthly_revenue_prev']) / df['monthly_revenue_prev']\n        df['user_growth'] = (df['active_users'] - df['active_users_prev']) / df['active_users_prev']\n\n        # Plan vs utilisation\n        df['plan_utilization'] = df['features_used'] / df['plan_features']\n\n        return df\n\n    def create_temporal_features(self, df):\n        \"\"\"Cr\u00e9ation de features temporelles\"\"\"\n        # \u00c2ge du compte\n        df['account_age_days'] = (pd.to_datetime('now') - pd.to_datetime(df['created_at'])).dt.days\n\n        # Saisonnalit\u00e9\n        df['month'] = pd.to_datetime(df['created_at']).dt.month\n        df['quarter'] = pd.to_datetime(df['created_at']).dt.quarter\n\n        # Derni\u00e8re activit\u00e9\n        df['days_since_last_login'] = (pd.to_datetime('now') - pd.to_datetime(df['last_login'])).dt.days\n\n        return df\n</code></pre>","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#2-gestion-du-desequilibre-des-classes","title":"2. Gestion du d\u00e9s\u00e9quilibre des classes","text":"<pre><code>from imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.combine import SMOTEENN\nfrom sklearn.model_selection import StratifiedKFold\n\nclass ChurnDataBalancer:\n    def __init__(self, strategy='smote'):\n        self.strategy = strategy\n        self.balancer = None\n\n    def fit_resample(self, X, y):\n        \"\"\"\u00c9quilibrage des classes\"\"\"\n        if self.strategy == 'smote':\n            self.balancer = SMOTE(random_state=42, k_neighbors=3)\n        elif self.strategy == 'smoteenn':\n            self.balancer = SMOTEENN(random_state=42)\n        elif self.strategy == 'undersample':\n            self.balancer = RandomUnderSampler(random_state=42)\n\n        X_balanced, y_balanced = self.balancer.fit_resample(X, y)\n\n        return X_balanced, y_balanced\n\n    def get_class_weights(self, y):\n        \"\"\"Calcul des poids de classe pour XGBoost\"\"\"\n        from sklearn.utils.class_weight import compute_class_weight\n\n        classes = np.unique(y)\n        weights = compute_class_weight('balanced', classes=classes, y=y)\n\n        return dict(zip(classes, weights))\n</code></pre>","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#3-modelisation-avec-xgboost","title":"3. Mod\u00e9lisation avec XGBoost","text":"<pre><code>import xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV, TimeSeriesSplit\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nimport optuna\n\nclass ChurnPredictor:\n    def __init__(self):\n        self.model = None\n        self.feature_importance = None\n        self.threshold = 0.5\n\n    def optimize_hyperparameters(self, X_train, y_train, X_val, y_val):\n        \"\"\"Optimisation des hyperparam\u00e8tres avec Optuna\"\"\"\n        def objective(trial):\n            params = {\n                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n                'max_depth': trial.suggest_int('max_depth', 3, 10),\n                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n                'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n                'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n                'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1, 10)\n            }\n\n            model = xgb.XGBClassifier(**params, random_state=42)\n            model.fit(X_train, y_train)\n\n            y_pred = model.predict(X_val)\n            score = f1_score(y_val, y_pred)\n\n            return score\n\n        study = optuna.create_study(direction='maximize')\n        study.optimize(objective, n_trials=100)\n\n        return study.best_params\n\n    def train_model(self, X_train, y_train, X_val, y_val, params=None):\n        \"\"\"Entra\u00eenement du mod\u00e8le avec validation\"\"\"\n        if params is None:\n            params = {\n                'n_estimators': 500,\n                'max_depth': 6,\n                'learning_rate': 0.1,\n                'subsample': 0.8,\n                'colsample_bytree': 0.8,\n                'reg_alpha': 1,\n                'reg_lambda': 1,\n                'scale_pos_weight': 3,\n                'random_state': 42\n            }\n\n        self.model = xgb.XGBClassifier(**params)\n        self.model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            early_stopping_rounds=50,\n            verbose=False\n        )\n\n        # Feature importance\n        self.feature_importance = pd.DataFrame({\n            'feature': X_train.columns,\n            'importance': self.model.feature_importances_\n        }).sort_values('importance', ascending=False)\n\n        return self.model\n\n    def optimize_threshold(self, X_val, y_val):\n        \"\"\"Optimisation du seuil de classification\"\"\"\n        y_pred_proba = self.model.predict_proba(X_val)[:, 1]\n\n        thresholds = np.arange(0.1, 0.9, 0.05)\n        best_threshold = 0.5\n        best_f1 = 0\n\n        for threshold in thresholds:\n            y_pred = (y_pred_proba &gt; threshold).astype(int)\n            f1 = f1_score(y_val, y_pred)\n\n            if f1 &gt; best_f1:\n                best_f1 = f1\n                best_threshold = threshold\n\n        self.threshold = best_threshold\n        return best_threshold\n</code></pre>","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#4-evaluation-et-interpretation","title":"4. \u00c9valuation et interpr\u00e9tation","text":"<pre><code>import shap\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nclass ChurnModelInterpreter:\n    def __init__(self, model, X_train):\n        self.model = model\n        self.explainer = shap.TreeExplainer(model)\n        self.X_train = X_train\n\n    def explain_prediction(self, X_sample):\n        \"\"\"Explication d'une pr\u00e9diction individuelle\"\"\"\n        shap_values = self.explainer.shap_values(X_sample)\n\n        # Plot SHAP values\n        shap.summary_plot(shap_values, X_sample, show=False)\n        plt.title('Explication de la pr\u00e9diction de churn')\n        plt.show()\n\n        return shap_values\n\n    def global_feature_importance(self):\n        \"\"\"Importance globale des features\"\"\"\n        shap_values = self.explainer.shap_values(self.X_train)\n\n        # Summary plot\n        shap.summary_plot(shap_values, self.X_train, show=False)\n        plt.title('Importance globale des features')\n        plt.show()\n\n        # Feature importance\n        feature_importance = pd.DataFrame({\n            'feature': self.X_train.columns,\n            'importance': np.abs(shap_values).mean(0)\n        }).sort_values('importance', ascending=False)\n\n        return feature_importance\n\n    def partial_dependence_plot(self, feature_name, X_sample):\n        \"\"\"Plot de d\u00e9pendance partielle\"\"\"\n        shap.dependence_plot(\n            feature_name, \n            self.explainer.shap_values(X_sample), \n            X_sample,\n            show=False\n        )\n        plt.title(f'D\u00e9pendance partielle - {feature_name}')\n        plt.show()\n</code></pre>","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#resultats-et-metriques","title":"\ud83d\udcc8 R\u00e9sultats et M\u00e9triques","text":"","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#performance-du-modele","title":"Performance du mod\u00e8le","text":"M\u00e9trique Valeur Baseline Am\u00e9lioration Accuracy 89.2% 76.5% +16.6% Precision 87.8% 72.1% +21.8% Recall 89.1% 78.3% +13.8% F1-Score 88.5% 75.1% +17.8% AUC-ROC 0.94 0.82 +14.6%","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#impact-business","title":"Impact business","text":"M\u00e9trique Avant Apr\u00e8s Am\u00e9lioration Taux de churn 12.5% 9.4% -25% R\u00e9tention client 87.5% 90.6% +3.1% Revenus r\u00e9currents 100% 115% +15% Co\u00fbt d'acquisition 100% 85% -15%","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#performance-par-segment","title":"Performance par segment","text":"Segment Precision Recall F1-Score Clients Enterprise 92.1% 88.3% 90.2% 500 Mid-market 89.4% 91.2% 90.3% 1,200 SMB 85.6% 87.9% 86.7% 3,300","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#deploiement","title":"\ud83d\ude80 D\u00e9ploiement","text":"","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#architecture-de-production","title":"Architecture de production","text":"<ul> <li>Environnement : Docker + Kubernetes</li> <li>API : FastAPI avec documentation automatique</li> <li>Base de donn\u00e9es : PostgreSQL + Redis</li> <li>Monitoring : MLflow + Prometheus</li> <li>Scheduling : Apache Airflow</li> </ul>","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#code-de-deploiement","title":"Code de d\u00e9ploiement","text":"<pre><code>from fastapi import FastAPI, BackgroundTasks\nfrom pydantic import BaseModel\nimport pandas as pd\nimport joblib\nimport redis\nimport json\n\napp = FastAPI(title=\"Churn Prediction API\")\n\n# Configuration\nredis_client = redis.Redis(host='localhost', port=6379, db=0)\nmodel = joblib.load('churn_model.pkl')\nfeature_engineer = joblib.load('feature_engineer.pkl')\n\nclass CustomerData(BaseModel):\n    customer_id: str\n    features: dict\n    prediction_date: str\n\nclass ChurnPrediction(BaseModel):\n    customer_id: str\n    churn_probability: float\n    risk_level: str\n    key_factors: list\n    recommendations: list\n\n@app.post(\"/predict\", response_model=ChurnPrediction)\nasync def predict_churn(customer_data: CustomerData):\n    # Feature engineering\n    features_df = pd.DataFrame([customer_data.features])\n    processed_features = feature_engineer.transform(features_df)\n\n    # Pr\u00e9diction\n    churn_prob = model.predict_proba(processed_features)[0][1]\n\n    # D\u00e9termination du niveau de risque\n    if churn_prob &gt; 0.8:\n        risk_level = \"Tr\u00e8s \u00e9lev\u00e9\"\n    elif churn_prob &gt; 0.6:\n        risk_level = \"\u00c9lev\u00e9\"\n    elif churn_prob &gt; 0.4:\n        risk_level = \"Mod\u00e9r\u00e9\"\n    else:\n        risk_level = \"Faible\"\n\n    # Facteurs cl\u00e9s (simulation)\n    key_factors = [\n        \"Baisse d'utilisation de 40%\",\n        \"Aucun ticket support r\u00e9cent\",\n        \"Plan sous-utilis\u00e9\"\n    ]\n\n    # Recommandations\n    recommendations = [\n        \"Proposer une session de formation\",\n        \"Contacter le customer success manager\",\n        \"Offrir un upgrade de plan\"\n    ]\n\n    return ChurnPrediction(\n        customer_id=customer_data.customer_id,\n        churn_probability=float(churn_prob),\n        risk_level=risk_level,\n        key_factors=key_factors,\n        recommendations=recommendations\n    )\n\n@app.post(\"/batch_predict\")\nasync def batch_predict(background_tasks: BackgroundTasks):\n    \"\"\"Pr\u00e9diction en lot pour tous les clients\"\"\"\n    background_tasks.add_task(run_batch_prediction)\n    return {\"message\": \"Batch prediction started\"}\n\nasync def run_batch_prediction():\n    \"\"\"Ex\u00e9cution de la pr\u00e9diction en lot\"\"\"\n    # R\u00e9cup\u00e9ration des donn\u00e9es clients\n    customers = get_all_customers()\n\n    predictions = []\n    for customer in customers:\n        prediction = predict_churn(customer)\n        predictions.append(prediction)\n\n    # Sauvegarde des r\u00e9sultats\n    save_predictions(predictions)\n</code></pre>","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#visualisations","title":"\ud83d\udcca Visualisations","text":"","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#analyse-des-features-importantes","title":"Analyse des features importantes","text":"<pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Visualisation des m\u00e9triques\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n# Feature importance\ntop_features = feature_importance.head(10)\naxes[0, 0].barh(top_features['feature'], top_features['importance'])\naxes[0, 0].set_title('Top 10 Features Importance')\naxes[0, 0].set_xlabel('Importance')\n\n# Distribution des probabilit\u00e9s\nchurn_probs = model.predict_proba(X_test)[:, 1]\naxes[0, 1].hist(churn_probs, bins=50, alpha=0.7, color='red')\naxes[0, 1].set_title('Distribution des probabilit\u00e9s de churn')\naxes[0, 1].set_xlabel('Probabilit\u00e9 de churn')\naxes[0, 1].set_ylabel('Fr\u00e9quence')\n\n# Courbe ROC\nfrom sklearn.metrics import roc_curve, auc\nfpr, tpr, _ = roc_curve(y_test, churn_probs)\nauc_score = auc(fpr, tpr)\n\naxes[1, 0].plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.3f})')\naxes[1, 0].plot([0, 1], [0, 1], 'k--')\naxes[1, 0].set_xlabel('Taux de faux positifs')\naxes[1, 0].set_ylabel('Taux de vrais positifs')\naxes[1, 0].set_title('Courbe ROC')\naxes[1, 0].legend()\n\n# Impact business\nmonths = ['Jan', 'F\u00e9v', 'Mar', 'Avr', 'Mai', 'Jun']\nchurn_rate = [12.5, 11.8, 10.2, 9.8, 9.4, 9.1]\nrevenue = [100, 102, 105, 108, 115, 118]\n\nax2 = axes[1, 1].twinx()\naxes[1, 1].plot(months, churn_rate, 'ro-', label='Taux de churn (%)')\nax2.plot(months, revenue, 'bo-', label='Revenus (index 100)')\naxes[1, 1].set_xlabel('Mois')\naxes[1, 1].set_ylabel('Taux de churn (%)', color='red')\nax2.set_ylabel('Revenus (index 100)', color='blue')\naxes[1, 1].set_title('Impact du mod\u00e8le sur le business')\naxes[1, 1].legend(loc='upper left')\nax2.legend(loc='upper right')\n\nplt.tight_layout()\nplt.show()\n</code></pre>","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#liens-et-ressources","title":"\ud83d\udd17 Liens et ressources","text":"","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#code-source","title":"Code source","text":"<ul> <li>Repository GitHub : github.com/loick-dernoncourt/churn-prediction</li> <li>Notebooks Jupyter : github.com/loick-dernoncourt/churn-prediction/tree/main/notebooks</li> </ul>","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#demonstrations","title":"D\u00e9monstrations","text":"<ul> <li>D\u00e9mo interactive : churn-prediction-demo.example.com</li> <li>API Documentation : churn-prediction-api.example.com/docs</li> <li>Dashboard : churn-prediction-dashboard.example.com</li> </ul>","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#documentation","title":"Documentation","text":"<ul> <li>Rapport technique : churn-prediction-report.example.com</li> <li>Pr\u00e9sentation : churn-prediction-slides.example.com</li> <li>Article de blog : blog.example.com/churn-prediction</li> </ul>","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#prochaines-etapes","title":"\ud83c\udfaf Prochaines \u00e9tapes","text":"","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#ameliorations-prevues","title":"Am\u00e9liorations pr\u00e9vues","text":"<ul> <li> Int\u00e9gration de donn\u00e9es comportementales temps r\u00e9el</li> <li> Mod\u00e8le de pr\u00e9diction de la valeur de vie client (LTV)</li> <li> Recommandations personnalis\u00e9es d'actions</li> <li> Int\u00e9gration avec CRM et outils de marketing</li> </ul>","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/prediction-churn-avancee/#technologies-a-explorer","title":"Technologies \u00e0 explorer","text":"<ul> <li> Deep Learning pour patterns complexes</li> <li> Graph Neural Networks pour relations clients</li> <li> Reinforcement Learning pour optimisation d'actions</li> <li> Explicabilit\u00e9 avanc\u00e9e avec LIME et SHAP</li> </ul> <p>Derni\u00e8re mise \u00e0 jour : October 22, 2025</p>","tags":["machine-learning","classification","xgboost","feature-engineering","churn-prediction","business-intelligence","production"]},{"location":"projects/saas/","title":"SaaS","text":"<p>Pitch (valeur m\u00e9tier). ND (\u00e0 compl\u00e9ter du README).</p>"},{"location":"projects/saas/#stack","title":"\u2699\ufe0f Stack","text":"<ul> <li>Langages : ND</li> <li>Frameworks : ND</li> <li>Outils : ND</li> </ul>"},{"location":"projects/saas/#resultats-metriques","title":"\ud83d\udcca R\u00e9sultats (m\u00e9triques)","text":"<p>ND</p>"},{"location":"projects/saas/#liens","title":"\ud83d\udd17 Liens","text":"<ul> <li>Repo : https://github.com/LoickDIA/SaaS</li> <li>D\u00e9mo : ND</li> </ul>"},{"location":"projects/template/","title":"\ud83d\udccb Template de projet","text":"","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#contexte-et-objectifs","title":"\ud83c\udfaf Contexte et Objectifs","text":"","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#probleme-a-resoudre","title":"Probl\u00e8me \u00e0 r\u00e9soudre","text":"<p>Description claire du probl\u00e8me m\u00e9tier ou technique \u00e0 r\u00e9soudre...</p>","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#objectifs","title":"Objectifs","text":"<ul> <li>Objectif principal : [Objectif principal du projet]</li> <li>Objectifs secondaires : [Objectifs secondaires]</li> <li>M\u00e9triques de succ\u00e8s : [M\u00e9triques pour \u00e9valuer le succ\u00e8s]</li> </ul>","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#contexte-metier","title":"Contexte m\u00e9tier","text":"<ul> <li>Secteur : [Secteur d'activit\u00e9]</li> <li>Utilisateurs : [Cible utilisateurs]</li> <li>Impact attendu : [Impact sur le business]</li> </ul>","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#donnees-et-sources","title":"\ud83d\udcca Donn\u00e9es et Sources","text":"","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#sources-de-donnees","title":"Sources de donn\u00e9es","text":"<ul> <li>Source principale : [Nom de la source]</li> <li>Format : CSV/JSON/Parquet/Database</li> <li>Taille : X millions d'enregistrements</li> <li>P\u00e9riode : 2020-2024</li> <li>Fr\u00e9quence : Quotidienne/Mensuelle/Annuelle</li> </ul>","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#qualite-des-donnees","title":"Qualit\u00e9 des donn\u00e9es","text":"<ul> <li>Compl\u00e9tude : 95% de compl\u00e9tude</li> <li>Coh\u00e9rence : Validation des contraintes</li> <li>Exactitude : V\u00e9rification des valeurs</li> <li>Actualit\u00e9 : Donn\u00e9es \u00e0 jour</li> </ul>","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#variables-disponibles","title":"Variables disponibles","text":"Variable Type Description Importance var1 Num\u00e9rique Description de la variable Haute var2 Cat\u00e9gorielle Description de la variable Moyenne var3 Temporelle Description de la variable Haute","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#methodologie","title":"\ud83d\udd2c M\u00e9thodologie","text":"","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#1-analyse-exploratoire-des-donnees-eda","title":"1. Analyse exploratoire des donn\u00e9es (EDA)","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Chargement des donn\u00e9es\ndf = pd.read_csv('data.csv')\n\n# Statistiques descriptives\nprint(df.describe())\n\n# Visualisation des distributions\nplt.figure(figsize=(12, 8))\nsns.histplot(df['target'], kde=True)\nplt.title('Distribution de la variable cible')\nplt.show()\n</code></pre>","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#2-preprocessing","title":"2. Pr\u00e9processing","text":"<pre><code>from sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# Nettoyage des donn\u00e9es\ndf = df.dropna()\ndf = df.drop_duplicates()\n\n# Encodage des variables cat\u00e9gorielles\nle = LabelEncoder()\ndf['categorical_var'] = le.fit_transform(df['categorical_var'])\n\n# Normalisation\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n</code></pre>","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#3-feature-engineering","title":"3. Feature Engineering","text":"<pre><code># Cr\u00e9ation de nouvelles features\ndf['feature_ratio'] = df['var1'] / df['var2']\ndf['feature_interaction'] = df['var1'] * df['var2']\n\n# S\u00e9lection des features\nfrom sklearn.feature_selection import SelectKBest, f_classif\nselector = SelectKBest(f_classif, k=10)\nX_selected = selector.fit_transform(X, y)\n</code></pre>","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#4-modelisation","title":"4. Mod\u00e9lisation","text":"<pre><code>import torch\nimport torch.nn as nn\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Mod\u00e8le 1: Random Forest\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\n\n# Mod\u00e8le 2: R\u00e9seau de neurones\nclass MonModele(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(MonModele, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, output_size)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\n# Entra\u00eenement du mod\u00e8le\nmodel = MonModele(input_size, hidden_size, output_size)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n</code></pre>","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#5-evaluation","title":"5. \u00c9valuation","text":"<pre><code>from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Calcul des m\u00e9triques\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1 = f1_score(y_test, y_pred, average='weighted')\n\nprint(f\"Accuracy: {accuracy:.3f}\")\nprint(f\"Precision: {precision:.3f}\")\nprint(f\"Recall: {recall:.3f}\")\nprint(f\"F1-Score: {f1:.3f}\")\n</code></pre>","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#resultats-et-metriques","title":"\ud83d\udcc8 R\u00e9sultats et M\u00e9triques","text":"","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#performance-du-modele","title":"Performance du mod\u00e8le","text":"M\u00e9trique Valeur Baseline Am\u00e9lioration Accuracy 95.2% 78.5% +16.7% Precision 94.8% 76.2% +18.6% Recall 95.1% 77.8% +17.3% F1-Score 94.9% 77.0% +17.9%","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#analyse-des-erreurs","title":"Analyse des erreurs","text":"<ul> <li>Faux positifs : 2.1% des pr\u00e9dictions</li> <li>Faux n\u00e9gatifs : 1.8% des pr\u00e9dictions</li> <li>Classes les plus difficiles : [Classes avec le plus d'erreurs]</li> </ul>","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#validation-croisee","title":"Validation crois\u00e9e","text":"<ul> <li>5-fold CV : 94.8% \u00b1 1.2%</li> <li>Stratified CV : 95.1% \u00b1 0.8%</li> <li>Time series CV : 94.5% \u00b1 1.5%</li> </ul>","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#deploiement","title":"\ud83d\ude80 D\u00e9ploiement","text":"","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#architecture-de-deploiement","title":"Architecture de d\u00e9ploiement","text":"<ul> <li>Environnement : Docker + AWS ECS</li> <li>API : FastAPI avec documentation automatique</li> <li>Base de donn\u00e9es : PostgreSQL + Redis</li> <li>Monitoring : MLflow + CloudWatch</li> <li>CI/CD : GitHub Actions</li> </ul>","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#code-de-deploiement","title":"Code de d\u00e9ploiement","text":"<pre><code>from fastapi import FastAPI\nimport joblib\nimport pandas as pd\n\napp = FastAPI()\n\n# Chargement du mod\u00e8le\nmodel = joblib.load('model.pkl')\nscaler = joblib.load('scaler.pkl')\n\n@app.post(\"/predict\")\nasync def predict(data: dict):\n    # Pr\u00e9processing\n    df = pd.DataFrame([data])\n    df_scaled = scaler.transform(df)\n\n    # Pr\u00e9diction\n    prediction = model.predict(df_scaled)\n    probability = model.predict_proba(df_scaled)\n\n    return {\n        \"prediction\": int(prediction[0]),\n        \"probability\": float(probability[0].max())\n    }\n</code></pre>","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#monitoring","title":"Monitoring","text":"<ul> <li>M\u00e9triques de performance : Accuracy, Latence, Throughput</li> <li>Alertes : D\u00e9rive du mod\u00e8le, Erreurs de pr\u00e9diction</li> <li>Logs : Requ\u00eates, Erreurs, Performances</li> </ul>","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#visualisations","title":"\ud83d\udcca Visualisations","text":"","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#graphiques-de-performance","title":"Graphiques de performance","text":"<pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Matrice de confusion\nplt.figure(figsize=(8, 6))\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d')\nplt.title('Matrice de confusion')\nplt.show()\n\n# Courbe ROC\nfrom sklearn.metrics import roc_curve, auc\nfpr, tpr, _ = roc_curve(y_test, y_pred_proba)\nauc_score = auc(fpr, tpr)\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.3f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('Taux de faux positifs')\nplt.ylabel('Taux de vrais positifs')\nplt.title('Courbe ROC')\nplt.legend()\nplt.show()\n</code></pre>","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#liens-et-ressources","title":"\ud83d\udd17 Liens et ressources","text":"","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#code-source","title":"Code source","text":"<ul> <li>Repository GitHub : github.com/loick-dernoncourt/projet-exemple</li> <li>Notebooks Jupyter : github.com/loick-dernoncourt/projet-exemple/tree/main/notebooks</li> </ul>","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#demonstrations","title":"D\u00e9monstrations","text":"<ul> <li>D\u00e9mo interactive : demo.example.com</li> <li>API Documentation : api.example.com/docs</li> <li>Dashboard : dashboard.example.com</li> </ul>","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#documentation","title":"Documentation","text":"<ul> <li>Rapport technique : rapport.example.com</li> <li>Pr\u00e9sentation : slides.example.com</li> <li>Article de blog : blog.example.com/projet</li> </ul>","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#prochaines-etapes","title":"\ud83c\udfaf Prochaines \u00e9tapes","text":"","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#ameliorations-prevues","title":"Am\u00e9liorations pr\u00e9vues","text":"<ul> <li> Optimisation des hyperparam\u00e8tres</li> <li> Int\u00e9gration de nouvelles donn\u00e9es</li> <li> Am\u00e9lioration de la robustesse</li> <li> D\u00e9ploiement en production</li> </ul>","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/template/#technologies-a-explorer","title":"Technologies \u00e0 explorer","text":"<ul> <li> MLOps avec Kubeflow</li> <li> Monitoring avec Weights &amp; Biases</li> <li> Optimisation avec Optuna</li> <li> Explicabilit\u00e9 avec SHAP</li> </ul> <p>Derni\u00e8re mise \u00e0 jour : October 22, 2025</p>","tags":["machine-learning","data-analysis","python","visualisation","nlp","computer-vision","deep-learning","pytorch","scikit-learn","pandas","numpy","matplotlib","seaborn","jupyter","git","docker","aws","sql","tableau","power-bi"]},{"location":"projects/valmed-automatisation/","title":"VALMED-AUTOMATISATION","text":"<p>Pitch (valeur m\u00e9tier). ND (\u00e0 compl\u00e9ter du README).</p>"},{"location":"projects/valmed-automatisation/#stack","title":"\u2699\ufe0f Stack","text":"<ul> <li>Langages : ND</li> <li>Frameworks : ND</li> <li>Outils : ND</li> </ul>"},{"location":"projects/valmed-automatisation/#resultats-metriques","title":"\ud83d\udcca R\u00e9sultats (m\u00e9triques)","text":"<p>ND</p>"},{"location":"projects/valmed-automatisation/#liens","title":"\ud83d\udd17 Liens","text":"<ul> <li>Repo : https://github.com/LoickDIA/VALMED-AUTOMATISATION</li> <li>D\u00e9mo : ND</li> </ul>"},{"location":"projects/exemples/analyse-sentiment/","title":"\ud83d\udcac Analyse de sentiment en temps r\u00e9el avec BERT","text":"","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#contexte-et-objectifs","title":"\ud83c\udfaf Contexte et Objectifs","text":"","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#probleme-a-resoudre","title":"Probl\u00e8me \u00e0 r\u00e9soudre","text":"<p>D\u00e9veloppement d'un syst\u00e8me d'analyse de sentiment en temps r\u00e9el pour monitorer l'opinion publique sur les r\u00e9seaux sociaux et les plateformes de e-commerce.</p>","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#objectifs","title":"Objectifs","text":"<ul> <li>Objectif principal : Classifier le sentiment de textes en 3 cat\u00e9gories (Positif, N\u00e9gatif, Neutre)</li> <li>Objectifs secondaires : Traitement en temps r\u00e9el avec latence &lt; 100ms</li> <li>M\u00e9triques de succ\u00e8s : Accuracy &gt; 90%, Latence &lt; 100ms</li> </ul>","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#contexte-metier","title":"Contexte m\u00e9tier","text":"<ul> <li>Secteur : E-commerce / Social Media</li> <li>Utilisateurs : \u00c9quipes marketing, Customer success</li> <li>Impact attendu : Am\u00e9lioration de 30% de la satisfaction client</li> </ul>","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#donnees-et-sources","title":"\ud83d\udcca Donn\u00e9es et Sources","text":"","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#sources-de-donnees","title":"Sources de donn\u00e9es","text":"<ul> <li>Source principale : Twitter API + Amazon Reviews</li> <li>Format : JSON (texte + m\u00e9tadonn\u00e9es)</li> <li>Taille : 500,000 tweets + 100,000 avis</li> <li>P\u00e9riode : 2022-2024</li> <li>Fr\u00e9quence : Collecte en temps r\u00e9el</li> </ul>","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#qualite-des-donnees","title":"Qualit\u00e9 des donn\u00e9es","text":"<ul> <li>Compl\u00e9tude : 92% de compl\u00e9tude</li> <li>Coh\u00e9rence : Validation par annotateurs experts</li> <li>Exactitude : Inter-annotateur agreement &gt; 85%</li> <li>Actualit\u00e9 : Donn\u00e9es r\u00e9centes et repr\u00e9sentatives</li> </ul>","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#distribution-des-classes","title":"Distribution des classes","text":"Classe Nombre Pourcentage Description Positif 200,000 40% Sentiment positif N\u00e9gatif 150,000 30% Sentiment n\u00e9gatif Neutre 150,000 30% Sentiment neutre","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#methodologie","title":"\ud83d\udd2c M\u00e9thodologie","text":"","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#1-analyse-exploratoire-des-donnees-eda","title":"1. Analyse exploratoire des donn\u00e9es (EDA)","text":"<pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\n\n# Chargement des donn\u00e9es\ndf = pd.read_json('sentiment_data.json')\n\n# Visualisation de la distribution\nplt.figure(figsize=(12, 6))\ndf['sentiment'].value_counts().plot(kind='bar')\nplt.title('Distribution des sentiments')\nplt.xlabel('Sentiment')\nplt.ylabel('Nombre d\\'\u00e9chantillons')\nplt.show()\n\n# Nuage de mots par sentiment\nfor sentiment in df['sentiment'].unique():\n    text = ' '.join(df[df['sentiment'] == sentiment]['text'])\n    wordcloud = WordCloud(width=800, height=400).generate(text)\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.title(f'Nuage de mots - {sentiment}')\n    plt.axis('off')\n    plt.show()\n</code></pre>","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#2-preprocessing","title":"2. Pr\u00e9processing","text":"<pre><code>import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# T\u00e9l\u00e9chargement des ressources NLTK\nnltk.download('stopwords')\nnltk.download('punkt')\n\ndef preprocess_text(text):\n    # Nettoyage du texte\n    text = re.sub(r'@\\w+|#\\w+', '', text)  # Suppression des mentions\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Suppression des URLs\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Suppression de la ponctuation\n    text = text.lower()  # Minuscules\n\n    # Tokenisation\n    tokens = word_tokenize(text)\n\n    # Suppression des stop words\n    stop_words = set(stopwords.words('french'))\n    tokens = [token for token in tokens if token not in stop_words]\n\n    return ' '.join(tokens)\n\n# Application du preprocessing\ndf['cleaned_text'] = df['text'].apply(preprocess_text)\n</code></pre>","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#3-modelisation-avec-bert","title":"3. Mod\u00e9lisation avec BERT","text":"<pre><code>from transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import TrainingArguments, Trainer\nimport torch\nfrom torch.utils.data import Dataset\n\nclass SentimentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n# Initialisation du mod\u00e8le BERT\nmodel_name = 'bert-base-multilingual-cased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertForSequenceClassification.from_pretrained(\n    model_name, \n    num_labels=3\n)\n\n# Pr\u00e9paration des donn\u00e9es\ntrain_dataset = SentimentDataset(\n    train_texts, train_labels, tokenizer\n)\nval_dataset = SentimentDataset(\n    val_texts, val_labels, tokenizer\n)\n</code></pre>","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#4-entrainement","title":"4. Entra\u00eenement","text":"<pre><code>from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        labels, predictions, average='weighted'\n    )\n    accuracy = accuracy_score(labels, predictions)\n\n    return {\n        'accuracy': accuracy,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }\n\n# Configuration de l'entra\u00eenement\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=100,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)\n\n# Entra\u00eeneur\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\n\n# Entra\u00eenement\ntrainer.train()\n</code></pre>","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#5-evaluation","title":"5. \u00c9valuation","text":"<pre><code># \u00c9valuation sur le jeu de test\ntest_results = trainer.evaluate(test_dataset)\nprint(f\"Accuracy: {test_results['eval_accuracy']:.3f}\")\nprint(f\"F1-Score: {test_results['eval_f1']:.3f}\")\n\n# Pr\u00e9dictions sur des exemples\ndef predict_sentiment(text, model, tokenizer):\n    inputs = tokenizer(\n        text, \n        return_tensors='pt', \n        truncation=True, \n        padding=True, \n        max_length=128\n    )\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n        predicted_class = torch.argmax(predictions, dim=-1).item()\n        confidence = predictions[0][predicted_class].item()\n\n    return predicted_class, confidence\n\n# Test sur des exemples\nexamples = [\n    \"J'adore ce produit, il est fantastique !\",\n    \"Service client d\u00e9cevant, je ne recommande pas.\",\n    \"Le produit est correct, rien de sp\u00e9cial.\"\n]\n\nfor example in examples:\n    pred_class, confidence = predict_sentiment(example, model, tokenizer)\n    sentiment = ['N\u00e9gatif', 'Neutre', 'Positif'][pred_class]\n    print(f\"Texte: {example}\")\n    print(f\"Sentiment: {sentiment} (Confiance: {confidence:.3f})\")\n    print()\n</code></pre>","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#resultats-et-metriques","title":"\ud83d\udcc8 R\u00e9sultats et M\u00e9triques","text":"","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#performance-du-modele","title":"Performance du mod\u00e8le","text":"M\u00e9trique Valeur Baseline Am\u00e9lioration Accuracy 94.5% 78.2% +16.3% Precision 94.1% 76.8% +17.3% Recall 94.8% 77.5% +17.3% F1-Score 94.4% 77.1% +17.3%","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#performance-par-classe","title":"Performance par classe","text":"Classe Precision Recall F1-Score Support Positif 95.2% 93.8% 94.5% 1,000 N\u00e9gatif 94.1% 96.3% 95.2% 1,000 Neutre 93.8% 94.2% 94.0% 1,000","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#metriques-de-performance","title":"M\u00e9triques de performance","text":"<ul> <li>Latence moyenne : 45ms</li> <li>Throughput : 1000 requ\u00eates/minute</li> <li>Disponibilit\u00e9 : 99.9%</li> </ul>","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#deploiement","title":"\ud83d\ude80 D\u00e9ploiement","text":"","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#architecture-de-deploiement","title":"Architecture de d\u00e9ploiement","text":"<ul> <li>Environnement : Docker + AWS ECS</li> <li>API : FastAPI avec documentation automatique</li> <li>Base de donn\u00e9es : Redis pour le cache</li> <li>Monitoring : CloudWatch + Custom metrics</li> <li>CI/CD : GitHub Actions</li> </ul>","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#code-de-deploiement","title":"Code de d\u00e9ploiement","text":"<pre><code>from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\nimport redis\nimport json\nimport time\n\napp = FastAPI(title=\"Sentiment Analysis API\")\n\n# Configuration Redis\nredis_client = redis.Redis(host='localhost', port=6379, db=0)\n\n# Chargement du mod\u00e8le\nmodel = BertForSequenceClassification.from_pretrained('./model')\ntokenizer = BertTokenizer.from_pretrained('./model')\nmodel.eval()\n\nclass TextInput(BaseModel):\n    text: str\n\nclass SentimentOutput(BaseModel):\n    sentiment: str\n    confidence: float\n    processing_time: float\n\n@app.post(\"/predict\", response_model=SentimentOutput)\nasync def predict_sentiment(input_data: TextInput):\n    start_time = time.time()\n\n    # V\u00e9rification du cache\n    cache_key = f\"sentiment:{hash(input_data.text)}\"\n    cached_result = redis_client.get(cache_key)\n\n    if cached_result:\n        result = json.loads(cached_result)\n        result['processing_time'] = time.time() - start_time\n        return SentimentOutput(**result)\n\n    # Pr\u00e9diction\n    inputs = tokenizer(\n        input_data.text, \n        return_tensors='pt', \n        truncation=True, \n        padding=True, \n        max_length=128\n    )\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n        predicted_class = torch.argmax(predictions, dim=-1).item()\n        confidence = predictions[0][predicted_class].item()\n\n    sentiment_labels = ['N\u00e9gatif', 'Neutre', 'Positif']\n    sentiment = sentiment_labels[predicted_class]\n\n    result = {\n        'sentiment': sentiment,\n        'confidence': float(confidence),\n        'processing_time': time.time() - start_time\n    }\n\n    # Mise en cache\n    redis_client.setex(cache_key, 3600, json.dumps(result))\n\n    return SentimentOutput(**result)\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\", \"timestamp\": time.time()}\n</code></pre>","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#monitoring","title":"Monitoring","text":"<pre><code>import logging\nfrom prometheus_client import Counter, Histogram, generate_latest\n\n# M\u00e9triques Prometheus\nREQUEST_COUNT = Counter('sentiment_requests_total', 'Total requests')\nREQUEST_LATENCY = Histogram('sentiment_request_duration_seconds', 'Request latency')\nPREDICTION_ACCURACY = Histogram('sentiment_prediction_confidence', 'Prediction confidence')\n\n@app.middleware(\"http\")\nasync def add_process_time_header(request, call_next):\n    start_time = time.time()\n    response = await call_next(request)\n    process_time = time.time() - start_time\n\n    REQUEST_COUNT.inc()\n    REQUEST_LATENCY.observe(process_time)\n\n    response.headers[\"X-Process-Time\"] = str(process_time)\n    return response\n\n@app.get(\"/metrics\")\nasync def metrics():\n    return Response(generate_latest(), media_type=\"text/plain\")\n</code></pre>","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#visualisations","title":"\ud83d\udcca Visualisations","text":"","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#matrice-de-confusion","title":"Matrice de confusion","text":"<pre><code>from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Matrice de confusion\ncm = confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['N\u00e9gatif', 'Neutre', 'Positif'],\n            yticklabels=['N\u00e9gatif', 'Neutre', 'Positif'])\nplt.title('Matrice de confusion')\nplt.xlabel('Pr\u00e9dictions')\nplt.ylabel('Vraies valeurs')\nplt.show()\n</code></pre>","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#analyse-des-erreurs","title":"Analyse des erreurs","text":"<pre><code># Analyse des erreurs de classification\nerrors = df[df['true_sentiment'] != df['predicted_sentiment']]\n\n# Mots les plus fr\u00e9quents dans les erreurs\nerror_words = []\nfor text in errors['text']:\n    words = text.split()\n    error_words.extend(words)\n\nfrom collections import Counter\nword_freq = Counter(error_words)\nprint(\"Mots les plus fr\u00e9quents dans les erreurs:\")\nprint(word_freq.most_common(10))\n</code></pre>","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#liens-et-ressources","title":"\ud83d\udd17 Liens et ressources","text":"","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#code-source","title":"Code source","text":"<ul> <li>Repository GitHub : github.com/loick-dernoncourt/sentiment-analysis</li> <li>Notebooks Jupyter : github.com/loick-dernoncourt/sentiment-analysis/tree/main/notebooks</li> </ul>","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#demonstrations","title":"D\u00e9monstrations","text":"<ul> <li>D\u00e9mo interactive : sentiment-demo.example.com</li> <li>API Documentation : sentiment-api.example.com/docs</li> <li>Dashboard : sentiment-dashboard.example.com</li> </ul>","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#documentation","title":"Documentation","text":"<ul> <li>Rapport technique : sentiment-report.example.com</li> <li>Pr\u00e9sentation : sentiment-slides.example.com</li> <li>Article de blog : blog.example.com/sentiment-analysis</li> </ul>","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#prochaines-etapes","title":"\ud83c\udfaf Prochaines \u00e9tapes","text":"","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#ameliorations-prevues","title":"Am\u00e9liorations pr\u00e9vues","text":"<ul> <li> Support multilingue (anglais, espagnol)</li> <li> Analyse d'\u00e9motions (joie, col\u00e8re, tristesse)</li> <li> Int\u00e9gration avec les r\u00e9seaux sociaux</li> <li> Dashboard de monitoring en temps r\u00e9el</li> </ul>","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/analyse-sentiment/#technologies-a-explorer","title":"Technologies \u00e0 explorer","text":"<ul> <li> RoBERTa pour de meilleures performances</li> <li> DistilBERT pour la latence</li> <li> ONNX pour l'optimisation</li> <li> Kafka pour le streaming</li> </ul> <p>Derni\u00e8re mise \u00e0 jour : October 22, 2025</p>","tags":["nlp","deep-learning","bert","transformers","sentiment-analysis","fastapi","real-time"]},{"location":"projects/exemples/classification-images/","title":"\ud83d\uddbc\ufe0f Classification d'images m\u00e9dicales avec CNN","text":"","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#contexte-et-objectifs","title":"\ud83c\udfaf Contexte et Objectifs","text":"","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#probleme-a-resoudre","title":"Probl\u00e8me \u00e0 r\u00e9soudre","text":"<p>D\u00e9veloppement d'un syst\u00e8me de classification automatique d'images m\u00e9dicales pour assister les radiologues dans le diagnostic de pathologies pulmonaires.</p>","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#objectifs","title":"Objectifs","text":"<ul> <li>Objectif principal : Classifier les images de radiographies pulmonaires en 4 cat\u00e9gories</li> <li>Objectifs secondaires : R\u00e9duire le temps de diagnostic de 50%</li> <li>M\u00e9triques de succ\u00e8s : Accuracy &gt; 90%, Pr\u00e9cision &gt; 85%</li> </ul>","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#contexte-metier","title":"Contexte m\u00e9tier","text":"<ul> <li>Secteur : Sant\u00e9 / Radiologie</li> <li>Utilisateurs : Radiologues, M\u00e9decins</li> <li>Impact attendu : Am\u00e9lioration de la pr\u00e9cision diagnostique</li> </ul>","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#donnees-et-sources","title":"\ud83d\udcca Donn\u00e9es et Sources","text":"","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#sources-de-donnees","title":"Sources de donn\u00e9es","text":"<ul> <li>Source principale : NIH Chest X-ray Dataset</li> <li>Format : Images PNG (1024x1024)</li> <li>Taille : 10,000 images</li> <li>P\u00e9riode : 2017-2020</li> <li>Fr\u00e9quence : Collecte continue</li> </ul>","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#qualite-des-donnees","title":"Qualit\u00e9 des donn\u00e9es","text":"<ul> <li>Compl\u00e9tude : 98% de compl\u00e9tude</li> <li>Coh\u00e9rence : Validation par radiologues experts</li> <li>Exactitude : Double validation des annotations</li> <li>Actualit\u00e9 : Donn\u00e9es r\u00e9centes et repr\u00e9sentatives</li> </ul>","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#classes-de-classification","title":"Classes de classification","text":"Classe Nombre Description Exemples Normal 2,500 Radiographie normale Poumons sains Pneumonie 3,000 Infection pulmonaire Opacit\u00e9s alv\u00e9olaires COVID-19 2,000 Infection COVID-19 Opacit\u00e9s en verre d\u00e9poli Autres 2,500 Autres pathologies Tuberculose, Cancer","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#methodologie","title":"\ud83d\udd2c M\u00e9thodologie","text":"","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#1-analyse-exploratoire-des-donnees-eda","title":"1. Analyse exploratoire des donn\u00e9es (EDA)","text":"<pre><code>import torch\nimport torchvision\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Chargement des donn\u00e9es\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                        std=[0.229, 0.224, 0.225])\n])\n\ndataset = torchvision.datasets.ImageFolder('data/', transform=transform)\n\n# Visualisation des distributions\nclass_counts = [2500, 3000, 2000, 2500]\nclasses = ['Normal', 'Pneumonie', 'COVID-19', 'Autres']\n\nplt.figure(figsize=(10, 6))\nplt.bar(classes, class_counts)\nplt.title('Distribution des classes')\nplt.xlabel('Classes')\nplt.ylabel('Nombre d\\'images')\nplt.show()\n</code></pre>","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#2-preprocessing","title":"2. Pr\u00e9processing","text":"<pre><code># Augmentation de donn\u00e9es\ntrain_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(degrees=15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                        std=[0.229, 0.224, 0.225])\n])\n\n# Division train/validation/test\ntrain_size = int(0.7 * len(dataset))\nval_size = int(0.15 * len(dataset))\ntest_size = len(dataset) - train_size - val_size\n\ntrain_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n    dataset, [train_size, val_size, test_size]\n)\n</code></pre>","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#3-architecture-du-modele","title":"3. Architecture du mod\u00e8le","text":"<pre><code>import torch.nn as nn\nimport torchvision.models as models\n\nclass MedicalCNN(nn.Module):\n    def __init__(self, num_classes=4):\n        super(MedicalCNN, self).__init__()\n\n        # Backbone ResNet50 pr\u00e9-entra\u00een\u00e9\n        self.backbone = models.resnet50(pretrained=True)\n\n        # Modification de la derni\u00e8re couche\n        num_features = self.backbone.fc.in_features\n        self.backbone.fc = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(num_features, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x):\n        return self.backbone(x)\n\n# Initialisation du mod\u00e8le\nmodel = MedicalCNN(num_classes=4)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n</code></pre>","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#4-entrainement","title":"4. Entra\u00eenement","text":"<pre><code>import torch.optim as optim\nfrom torch.utils.data import DataLoader\n\n# Configuration de l'entra\u00eenement\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n\n# Data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n# Boucle d'entra\u00eenement\ndef train_epoch(model, train_loader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = output.max(1)\n        total += target.size(0)\n        correct += predicted.eq(target).sum().item()\n\n    return running_loss / len(train_loader), 100. * correct / total\n</code></pre>","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#5-evaluation","title":"5. \u00c9valuation","text":"<pre><code>from sklearn.metrics import classification_report, confusion_matrix\nimport numpy as np\n\ndef evaluate_model(model, test_loader, device):\n    model.eval()\n    all_preds = []\n    all_targets = []\n\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            _, predicted = output.max(1)\n\n            all_preds.extend(predicted.cpu().numpy())\n            all_targets.extend(target.cpu().numpy())\n\n    return np.array(all_preds), np.array(all_targets)\n\n# \u00c9valuation finale\npredictions, targets = evaluate_model(model, test_loader, device)\nprint(classification_report(targets, predictions, target_names=classes))\n</code></pre>","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#resultats-et-metriques","title":"\ud83d\udcc8 R\u00e9sultats et M\u00e9triques","text":"","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#performance-du-modele","title":"Performance du mod\u00e8le","text":"M\u00e9trique Valeur Baseline Am\u00e9lioration Accuracy 95.2% 78.5% +16.7% Precision 94.8% 76.2% +18.6% Recall 95.1% 77.8% +17.3% F1-Score 94.9% 77.0% +17.9%","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#performance-par-classe","title":"Performance par classe","text":"Classe Precision Recall F1-Score Support Normal 96.2% 94.8% 95.5% 375 Pneumonie 94.1% 96.3% 95.2% 450 COVID-19 95.8% 93.2% 94.5% 300 Autres 94.9% 96.1% 95.5% 375","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#analyse-des-erreurs","title":"Analyse des erreurs","text":"<ul> <li>Faux positifs : 2.1% des pr\u00e9dictions</li> <li>Faux n\u00e9gatifs : 1.8% des pr\u00e9dictions</li> <li>Classes les plus difficiles : COVID-19 vs Pneumonie (confusion fr\u00e9quente)</li> </ul>","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#deploiement","title":"\ud83d\ude80 D\u00e9ploiement","text":"","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#architecture-de-deploiement","title":"Architecture de d\u00e9ploiement","text":"<ul> <li>Environnement : Docker + AWS ECS</li> <li>API : FastAPI avec documentation automatique</li> <li>Base de donn\u00e9es : PostgreSQL + Redis</li> <li>Monitoring : MLflow + CloudWatch</li> <li>CI/CD : GitHub Actions</li> </ul>","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#code-de-deploiement","title":"Code de d\u00e9ploiement","text":"<pre><code>from fastapi import FastAPI, File, UploadFile\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport io\n\napp = FastAPI(title=\"Medical Image Classification API\")\n\n# Chargement du mod\u00e8le\nmodel = torch.load('medical_cnn.pth', map_location='cpu')\nmodel.eval()\n\n# Transformations\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                        std=[0.229, 0.224, 0.225])\n])\n\n@app.post(\"/predict\")\nasync def predict(file: UploadFile = File(...)):\n    # Lecture de l'image\n    image = Image.open(io.BytesIO(await file.read()))\n\n    # Pr\u00e9processing\n    image_tensor = transform(image).unsqueeze(0)\n\n    # Pr\u00e9diction\n    with torch.no_grad():\n        outputs = model(image_tensor)\n        probabilities = torch.softmax(outputs, dim=1)\n        predicted_class = torch.argmax(probabilities, dim=1).item()\n        confidence = probabilities[0][predicted_class].item()\n\n    return {\n        \"predicted_class\": classes[predicted_class],\n        \"confidence\": float(confidence),\n        \"probabilities\": {\n            class_name: float(prob) \n            for class_name, prob in zip(classes, probabilities[0])\n        }\n    }\n</code></pre>","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#visualisations","title":"\ud83d\udcca Visualisations","text":"","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#matrice-de-confusion","title":"Matrice de confusion","text":"<pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Matrice de confusion\ncm = confusion_matrix(targets, predictions)\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=classes, yticklabels=classes)\nplt.title('Matrice de confusion')\nplt.xlabel('Pr\u00e9dictions')\nplt.ylabel('Vraies valeurs')\nplt.show()\n</code></pre>","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#courbe-roc","title":"Courbe ROC","text":"<pre><code>from sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\n\n# Binarisation des labels pour ROC\ny_bin = label_binarize(targets, classes=[0, 1, 2, 3])\ny_scores = torch.softmax(torch.tensor(predictions), dim=1).numpy()\n\n# Calcul des courbes ROC pour chaque classe\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\n\nfor i in range(4):\n    fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_scores[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Visualisation\nplt.figure(figsize=(12, 8))\nfor i in range(4):\n    plt.plot(fpr[i], tpr[i], label=f'{classes[i]} (AUC = {roc_auc[i]:.3f})')\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('Taux de faux positifs')\nplt.ylabel('Taux de vrais positifs')\nplt.title('Courbes ROC par classe')\nplt.legend()\nplt.show()\n</code></pre>","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#liens-et-ressources","title":"\ud83d\udd17 Liens et ressources","text":"","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#code-source","title":"Code source","text":"<ul> <li>Repository GitHub : github.com/loick-dernoncourt/medical-cnn</li> <li>Notebooks Jupyter : github.com/loick-dernoncourt/medical-cnn/tree/main/notebooks</li> </ul>","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#demonstrations","title":"D\u00e9monstrations","text":"<ul> <li>D\u00e9mo interactive : medical-demo.example.com</li> <li>API Documentation : medical-api.example.com/docs</li> <li>Dashboard : medical-dashboard.example.com</li> </ul>","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#documentation","title":"Documentation","text":"<ul> <li>Rapport technique : medical-report.example.com</li> <li>Pr\u00e9sentation : medical-slides.example.com</li> <li>Article de blog : blog.example.com/medical-cnn</li> </ul>","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#prochaines-etapes","title":"\ud83c\udfaf Prochaines \u00e9tapes","text":"","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#ameliorations-prevues","title":"Am\u00e9liorations pr\u00e9vues","text":"<ul> <li> Int\u00e9gration de donn\u00e9es 3D (CT scans)</li> <li> Am\u00e9lioration de l'explicabilit\u00e9 avec Grad-CAM</li> <li> Optimisation pour mobile (quantification)</li> <li> Int\u00e9gration avec PACS hospitalier</li> </ul>","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/classification-images/#technologies-a-explorer","title":"Technologies \u00e0 explorer","text":"<ul> <li> Vision Transformers (ViT)</li> <li> Self-supervised learning</li> <li> Federated learning</li> <li> Edge deployment avec ONNX</li> </ul> <p>Derni\u00e8re mise \u00e0 jour : October 22, 2025</p>","tags":["computer-vision","deep-learning","pytorch","cnn","classification","medical-imaging"]},{"location":"projects/exemples/prediction-prix/","title":"\ud83c\udfe0 Pr\u00e9diction de prix immobiliers avec XGBoost","text":"","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#contexte-et-objectifs","title":"\ud83c\udfaf Contexte et Objectifs","text":"","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#probleme-a-resoudre","title":"Probl\u00e8me \u00e0 r\u00e9soudre","text":"<p>D\u00e9veloppement d'un mod\u00e8le de pr\u00e9diction de prix immobiliers pour aider les acheteurs et vendeurs \u00e0 estimer la valeur d'un bien immobilier.</p>","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#objectifs","title":"Objectifs","text":"<ul> <li>Objectif principal : Pr\u00e9dire le prix d'un bien immobilier avec une erreur &lt; 20%</li> <li>Objectifs secondaires : Identifier les facteurs les plus influents sur le prix</li> <li>M\u00e9triques de succ\u00e8s : RMSE &lt; 0.2, R\u00b2 &gt; 0.85</li> </ul>","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#contexte-metier","title":"Contexte m\u00e9tier","text":"<ul> <li>Secteur : Immobilier / Fintech</li> <li>Utilisateurs : Acheteurs, Vendeurs, Agents immobiliers</li> <li>Impact attendu : R\u00e9duction de 30% du temps d'estimation</li> </ul>","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#donnees-et-sources","title":"\ud83d\udcca Donn\u00e9es et Sources","text":"","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#sources-de-donnees","title":"Sources de donn\u00e9es","text":"<ul> <li>Source principale : Donn\u00e9es publiques immobili\u00e8res</li> <li>Format : CSV (propri\u00e9t\u00e9s + prix)</li> <li>Taille : 50,000 propri\u00e9t\u00e9s</li> <li>P\u00e9riode : 2020-2024</li> <li>Fr\u00e9quence : Mise \u00e0 jour mensuelle</li> </ul>","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#qualite-des-donnees","title":"Qualit\u00e9 des donn\u00e9es","text":"<ul> <li>Compl\u00e9tude : 88% de compl\u00e9tude</li> <li>Coh\u00e9rence : Validation des prix avec les transactions</li> <li>Exactitude : V\u00e9rification avec les notaires</li> <li>Actualit\u00e9 : Donn\u00e9es r\u00e9centes et repr\u00e9sentatives</li> </ul>","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#variables-disponibles","title":"Variables disponibles","text":"Variable Type Description Importance surface Num\u00e9rique Surface en m\u00b2 Haute nb_pieces Num\u00e9rique Nombre de pi\u00e8ces Haute nb_chambres Num\u00e9rique Nombre de chambres Haute etage Num\u00e9rique \u00c9tage Moyenne ascenseur Binaire Pr\u00e9sence d'ascenseur Moyenne parking Binaire Place de parking Moyenne balcon Binaire Balcon/terrasse Faible quartier Cat\u00e9gorielle Quartier Haute type_bien Cat\u00e9gorielle Type de bien Haute","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#methodologie","title":"\ud83d\udd2c M\u00e9thodologie","text":"","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#1-analyse-exploratoire-des-donnees-eda","title":"1. Analyse exploratoire des donn\u00e9es (EDA)","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Chargement des donn\u00e9es\ndf = pd.read_csv('real_estate_data.csv')\n\n# Statistiques descriptives\nprint(df.describe())\n\n# Visualisation de la distribution des prix\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.hist(df['prix'], bins=50, alpha=0.7)\nplt.title('Distribution des prix')\nplt.xlabel('Prix (\u20ac)')\nplt.ylabel('Fr\u00e9quence')\n\nplt.subplot(1, 2, 2)\nplt.hist(np.log(df['prix']), bins=50, alpha=0.7)\nplt.title('Distribution des prix (log)')\nplt.xlabel('Log(Prix)')\nplt.ylabel('Fr\u00e9quence')\nplt.show()\n\n# Corr\u00e9lation avec la surface\nplt.figure(figsize=(10, 6))\nplt.scatter(df['surface'], df['prix'], alpha=0.5)\nplt.title('Prix vs Surface')\nplt.xlabel('Surface (m\u00b2)')\nplt.ylabel('Prix (\u20ac)')\nplt.show()\n</code></pre>","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#2-preprocessing","title":"2. Pr\u00e9processing","text":"<pre><code>from sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# Nettoyage des donn\u00e9es\ndf = df.dropna()\ndf = df[df['prix'] &gt; 0]  # Suppression des prix n\u00e9gatifs\ndf = df[df['surface'] &gt; 0]  # Suppression des surfaces n\u00e9gatives\n\n# Transformation logarithmique du prix\ndf['log_prix'] = np.log(df['prix'])\n\n# Encodage des variables cat\u00e9gorielles\nle_quartier = LabelEncoder()\nle_type = LabelEncoder()\n\ndf['quartier_encoded'] = le_quartier.fit_transform(df['quartier'])\ndf['type_encoded'] = le_type.fit_transform(df['type_bien'])\n\n# S\u00e9lection des features\nfeatures = ['surface', 'nb_pieces', 'nb_chambres', 'etage', \n           'ascenseur', 'parking', 'balcon', 'quartier_encoded', 'type_encoded']\nX = df[features]\ny = df['log_prix']\n\n# Division train/validation/test\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n</code></pre>","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#3-feature-engineering","title":"3. Feature Engineering","text":"<pre><code># Cr\u00e9ation de nouvelles features\ndef create_features(df):\n    # Prix au m\u00b2\n    df['prix_m2'] = df['prix'] / df['surface']\n\n    # Ratio chambres/pi\u00e8ces\n    df['ratio_chambres_pieces'] = df['nb_chambres'] / df['nb_pieces']\n\n    # Surface par pi\u00e8ce\n    df['surface_par_piece'] = df['surface'] / df['nb_pieces']\n\n    # Indicateur de luxe (surface &gt; 100m\u00b2 et \u00e9tage &gt; 5)\n    df['luxe'] = ((df['surface'] &gt; 100) &amp; (df['etage'] &gt; 5)).astype(int)\n\n    # Indicateur de r\u00e9novation (bien r\u00e9cent)\n    df['renove'] = (df['annee_construction'] &gt; 2010).astype(int)\n\n    return df\n\n# Application du feature engineering\ndf = create_features(df)\n\n# S\u00e9lection des features finales\nfinal_features = ['surface', 'nb_pieces', 'nb_chambres', 'etage', \n                 'ascenseur', 'parking', 'balcon', 'quartier_encoded', \n                 'type_encoded', 'prix_m2', 'ratio_chambres_pieces', \n                 'surface_par_piece', 'luxe', 'renove']\n</code></pre>","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#4-modelisation-avec-xgboost","title":"4. Mod\u00e9lisation avec XGBoost","text":"<pre><code>import xgboost as xgb\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport mlflow\nimport mlflow.xgboost\n\n# Configuration MLflow\nmlflow.set_experiment(\"real_estate_prediction\")\n\nwith mlflow.start_run():\n    # Configuration du mod\u00e8le\n    params = {\n        'n_estimators': 1000,\n        'max_depth': 6,\n        'learning_rate': 0.1,\n        'subsample': 0.8,\n        'colsample_bytree': 0.8,\n        'random_state': 42\n    }\n\n    # Entra\u00eenement du mod\u00e8le\n    model = xgb.XGBRegressor(**params)\n    model.fit(X_train, y_train)\n\n    # Pr\u00e9dictions\n    y_pred_train = model.predict(X_train)\n    y_pred_val = model.predict(X_val)\n    y_pred_test = model.predict(X_test)\n\n    # Calcul des m\u00e9triques\n    rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n    rmse_val = np.sqrt(mean_squared_error(y_val, y_pred_val))\n    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n\n    r2_train = r2_score(y_train, y_pred_train)\n    r2_val = r2_score(y_val, y_pred_val)\n    r2_test = r2_score(y_test, y_pred_test)\n\n    # Logging des m\u00e9triques\n    mlflow.log_params(params)\n    mlflow.log_metric(\"rmse_train\", rmse_train)\n    mlflow.log_metric(\"rmse_val\", rmse_val)\n    mlflow.log_metric(\"rmse_test\", rmse_test)\n    mlflow.log_metric(\"r2_train\", r2_train)\n    mlflow.log_metric(\"r2_val\", r2_val)\n    mlflow.log_metric(\"r2_test\", r2_test)\n\n    # Sauvegarde du mod\u00e8le\n    mlflow.xgboost.log_model(model, \"model\")\n\n    print(f\"RMSE Train: {rmse_train:.3f}\")\n    print(f\"RMSE Validation: {rmse_val:.3f}\")\n    print(f\"RMSE Test: {rmse_test:.3f}\")\n    print(f\"R\u00b2 Train: {r2_train:.3f}\")\n    print(f\"R\u00b2 Validation: {r2_val:.3f}\")\n    print(f\"R\u00b2 Test: {r2_test:.3f}\")\n</code></pre>","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#5-optimisation-des-hyperparametres","title":"5. Optimisation des hyperparam\u00e8tres","text":"<pre><code>from sklearn.model_selection import GridSearchCV\n\n# Grille de param\u00e8tres\nparam_grid = {\n    'n_estimators': [500, 1000, 1500],\n    'max_depth': [4, 6, 8],\n    'learning_rate': [0.05, 0.1, 0.15],\n    'subsample': [0.8, 0.9, 1.0],\n    'colsample_bytree': [0.8, 0.9, 1.0]\n}\n\n# Recherche par grille\ngrid_search = GridSearchCV(\n    xgb.XGBRegressor(random_state=42),\n    param_grid,\n    cv=5,\n    scoring='neg_mean_squared_error',\n    n_jobs=-1\n)\n\ngrid_search.fit(X_train, y_train)\n\n# Meilleurs param\u00e8tres\nprint(\"Meilleurs param\u00e8tres:\", grid_search.best_params_)\nprint(\"Meilleur score:\", grid_search.best_score_)\n\n# Mod\u00e8le optimis\u00e9\nbest_model = grid_search.best_estimator_\n</code></pre>","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#6-evaluation-et-interpretation","title":"6. \u00c9valuation et interpr\u00e9tation","text":"<pre><code># Importance des features\nfeature_importance = model.feature_importances_\nfeature_names = X.columns\n\n# Tri par importance\nimportance_df = pd.DataFrame({\n    'feature': feature_names,\n    'importance': feature_importance\n}).sort_values('importance', ascending=False)\n\n# Visualisation de l'importance\nplt.figure(figsize=(10, 8))\nsns.barplot(data=importance_df.head(10), x='importance', y='feature')\nplt.title('Importance des features')\nplt.xlabel('Importance')\nplt.show()\n\n# Pr\u00e9dictions sur des exemples\ndef predict_price(model, surface, nb_pieces, nb_chambres, etage, \n                 ascenseur, parking, balcon, quartier, type_bien):\n    # Cr\u00e9ation d'un DataFrame avec les features\n    data = pd.DataFrame({\n        'surface': [surface],\n        'nb_pieces': [nb_pieces],\n        'nb_chambres': [nb_chambres],\n        'etage': [etage],\n        'ascenseur': [ascenseur],\n        'parking': [parking],\n        'balcon': [balcon],\n        'quartier_encoded': [quartier],\n        'type_encoded': [type_bien]\n    })\n\n    # Pr\u00e9diction\n    log_price = model.predict(data)[0]\n    price = np.exp(log_price)\n\n    return price\n\n# Exemple de pr\u00e9diction\npredicted_price = predict_price(\n    model, surface=80, nb_pieces=3, nb_chambres=2, etage=3,\n    ascenseur=1, parking=1, balcon=0, quartier=5, type_bien=1\n)\nprint(f\"Prix pr\u00e9dit: {predicted_price:,.0f} \u20ac\")\n</code></pre>","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#resultats-et-metriques","title":"\ud83d\udcc8 R\u00e9sultats et M\u00e9triques","text":"","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#performance-du-modele","title":"Performance du mod\u00e8le","text":"M\u00e9trique Valeur Baseline Am\u00e9lioration RMSE 0.15 0.25 +40% R\u00b2 0.87 0.65 +22% MAE 0.12 0.20 +40% MAPE 15.2% 28.5% +13.3%","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#performance-par-type-de-bien","title":"Performance par type de bien","text":"Type de bien RMSE R\u00b2 Nombre d'\u00e9chantillons Appartement 0.14 0.89 30,000 Maison 0.16 0.85 15,000 Studio 0.13 0.91 5,000","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#importance-des-features","title":"Importance des features","text":"Feature Importance Description surface 0.35 Surface en m\u00b2 quartier 0.25 Quartier nb_pieces 0.15 Nombre de pi\u00e8ces type_bien 0.10 Type de bien etage 0.08 \u00c9tage parking 0.04 Place de parking ascenseur 0.03 Ascenseur","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#deploiement","title":"\ud83d\ude80 D\u00e9ploiement","text":"","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#architecture-de-deploiement","title":"Architecture de d\u00e9ploiement","text":"<ul> <li>Environnement : Docker + AWS ECS</li> <li>API : FastAPI avec documentation automatique</li> <li>Base de donn\u00e9es : PostgreSQL</li> <li>Monitoring : MLflow + CloudWatch</li> <li>CI/CD : GitHub Actions</li> </ul>","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#code-de-deploiement","title":"Code de d\u00e9ploiement","text":"<pre><code>from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport joblib\nimport numpy as np\nimport pandas as pd\n\napp = FastAPI(title=\"Real Estate Price Prediction API\")\n\n# Chargement du mod\u00e8le\nmodel = joblib.load('xgboost_model.pkl')\nscaler = joblib.load('scaler.pkl')\n\nclass PropertyInput(BaseModel):\n    surface: float\n    nb_pieces: int\n    nb_chambres: int\n    etage: int\n    ascenseur: bool\n    parking: bool\n    balcon: bool\n    quartier: str\n    type_bien: str\n\nclass PriceOutput(BaseModel):\n    predicted_price: float\n    confidence_interval: dict\n    feature_importance: dict\n\n@app.post(\"/predict\", response_model=PriceOutput)\nasync def predict_price(property_data: PropertyInput):\n    try:\n        # Pr\u00e9processing des donn\u00e9es\n        data = pd.DataFrame([property_data.dict()])\n\n        # Encodage des variables cat\u00e9gorielles\n        data['quartier_encoded'] = le_quartier.transform(data['quartier'])\n        data['type_encoded'] = le_type.transform(data['type_bien'])\n\n        # S\u00e9lection des features\n        features = ['surface', 'nb_pieces', 'nb_chambres', 'etage', \n                   'ascenseur', 'parking', 'balcon', 'quartier_encoded', 'type_encoded']\n        X = data[features]\n\n        # Pr\u00e9diction\n        log_price = model.predict(X)[0]\n        predicted_price = np.exp(log_price)\n\n        # Intervalle de confiance (approximatif)\n        confidence_interval = {\n            'lower': predicted_price * 0.85,\n            'upper': predicted_price * 1.15\n        }\n\n        # Importance des features pour cette pr\u00e9diction\n        feature_importance = dict(zip(features, model.feature_importances_))\n\n        return PriceOutput(\n            predicted_price=float(predicted_price),\n            confidence_interval=confidence_interval,\n            feature_importance=feature_importance\n        )\n\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=str(e))\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\", \"model_loaded\": True}\n</code></pre>","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#visualisations","title":"\ud83d\udcca Visualisations","text":"","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#distribution-des-erreurs","title":"Distribution des erreurs","text":"<pre><code># Calcul des erreurs\nerrors = y_test - y_pred_test\nerrors_percent = (errors / y_test) * 100\n\n# Visualisation des erreurs\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.hist(errors, bins=50, alpha=0.7)\nplt.title('Distribution des erreurs')\nplt.xlabel('Erreur (log prix)')\nplt.ylabel('Fr\u00e9quence')\n\nplt.subplot(1, 2, 2)\nplt.hist(errors_percent, bins=50, alpha=0.7)\nplt.title('Distribution des erreurs (%)')\nplt.xlabel('Erreur (%)')\nplt.ylabel('Fr\u00e9quence')\nplt.show()\n</code></pre>","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#predictions-vs-vraies-valeurs","title":"Pr\u00e9dictions vs Vraies valeurs","text":"<pre><code># Scatter plot des pr\u00e9dictions\nplt.figure(figsize=(10, 8))\nplt.scatter(y_test, y_pred_test, alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel('Vraies valeurs (log prix)')\nplt.ylabel('Pr\u00e9dictions (log prix)')\nplt.title('Pr\u00e9dictions vs Vraies valeurs')\nplt.show()\n</code></pre>","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#liens-et-ressources","title":"\ud83d\udd17 Liens et ressources","text":"","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#code-source","title":"Code source","text":"<ul> <li>Repository GitHub : github.com/loick-dernoncourt/real-estate-prediction</li> <li>Notebooks Jupyter : github.com/loick-dernoncourt/real-estate-prediction/tree/main/notebooks</li> </ul>","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#demonstrations","title":"D\u00e9monstrations","text":"<ul> <li>D\u00e9mo interactive : real-estate-demo.example.com</li> <li>API Documentation : real-estate-api.example.com/docs</li> <li>Dashboard : real-estate-dashboard.example.com</li> </ul>","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#documentation","title":"Documentation","text":"<ul> <li>Rapport technique : real-estate-report.example.com</li> <li>Pr\u00e9sentation : real-estate-slides.example.com</li> <li>Article de blog : blog.example.com/real-estate-prediction</li> </ul>","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#prochaines-etapes","title":"\ud83c\udfaf Prochaines \u00e9tapes","text":"","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#ameliorations-prevues","title":"Am\u00e9liorations pr\u00e9vues","text":"<ul> <li> Int\u00e9gration de donn\u00e9es g\u00e9ographiques (GIS)</li> <li> Mod\u00e8le de pr\u00e9diction des tendances</li> <li> Analyse de la valeur ajout\u00e9e des r\u00e9novations</li> <li> Pr\u00e9diction des prix de location</li> </ul>","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"projects/exemples/prediction-prix/#technologies-a-explorer","title":"Technologies \u00e0 explorer","text":"<ul> <li> LightGBM pour de meilleures performances</li> <li> SHAP pour l'explicabilit\u00e9</li> <li> Time series pour les tendances</li> <li> Deep learning pour les patterns complexes</li> </ul> <p>Derni\u00e8re mise \u00e0 jour : October 22, 2025</p>","tags":["machine-learning","regression","xgboost","feature-engineering","real-estate","mlflow"]},{"location":"skills/","title":"\ud83d\udee0\ufe0f Comp\u00e9tences Techniques","text":"<p>D\u00e9couvrez mes comp\u00e9tences en data science, machine learning et technologies associ\u00e9es.</p>"},{"location":"skills/#apercu-des-competences","title":"\ud83c\udfaf Aper\u00e7u des comp\u00e9tences","text":"<ul> <li> <p> Machine Learning</p> <p>Classification, R\u00e9gression, Clustering, Feature Engineering</p> </li> <li> <p> Deep Learning</p> <p>CNN, RNN, Transformers, PyTorch, TensorFlow</p> </li> <li> <p> Data Engineering</p> <p>ETL, Big Data, AWS, Docker, CI/CD</p> </li> <li> <p> Visualisation</p> <p>Matplotlib, Seaborn, Plotly, Tableau, Power BI</p> </li> </ul>"},{"location":"skills/#niveau-de-competences","title":"\ud83d\udcca Niveau de comp\u00e9tences","text":""},{"location":"skills/#langages-de-programmation","title":"Langages de programmation","text":"Langage Niveau Exp\u00e9rience Projets Python Expert 5+ ans 20+ projets R Interm\u00e9diaire 3 ans 5 projets SQL Expert 5+ ans 15+ projets JavaScript Interm\u00e9diaire 2 ans 3 projets"},{"location":"skills/#frameworks-et-bibliotheques","title":"Frameworks et biblioth\u00e8ques","text":"Framework Niveau Utilisation Derni\u00e8re version Scikit-learn Expert 4 ans 1.3.0 PyTorch Expert 3 ans 2.0.0 XGBoost Expert 3 ans 1.7.0 Pandas Expert 5 ans 2.0.0 NumPy Expert 5 ans 1.24.0 Matplotlib Expert 4 ans 3.7.0 Seaborn Expert 4 ans 0.12.0 Plotly Interm\u00e9diaire 2 ans 5.15.0"},{"location":"skills/#outils-et-plateformes","title":"Outils et plateformes","text":"Outil Niveau Exp\u00e9rience Certification AWS Expert 3 ans \u2705 Certified Docker Expert 3 ans - Kubernetes Interm\u00e9diaire 2 ans - Git Expert 5 ans - MLflow Expert 2 ans - Tableau Interm\u00e9diaire 2 ans - Power BI Interm\u00e9diaire 1 an -"},{"location":"skills/#certifications","title":"\ud83c\udf93 Certifications","text":""},{"location":"skills/#cloud-et-ml","title":"Cloud et ML","text":"<ul> <li>AWS Certified Machine Learning - Specialty (2023)</li> <li>Google Cloud Professional Machine Learning Engineer (2023)</li> <li>Microsoft Azure Data Scientist Associate (2022)</li> </ul>"},{"location":"skills/#data-science","title":"Data Science","text":"<ul> <li>IBM Data Science Professional Certificate (2021)</li> <li>Coursera Deep Learning Specialization (2020)</li> </ul>"},{"location":"skills/#projets-par-competence","title":"\ud83c\udfc6 Projets par comp\u00e9tence","text":""},{"location":"skills/#machine-learning-20-projets","title":"Machine Learning (20+ projets)","text":"<ul> <li>Classification : 8 projets (images, texte, tabulaire)</li> <li>R\u00e9gression : 6 projets (prix, pr\u00e9dictions)</li> <li>Clustering : 4 projets (segmentation, recommandation)</li> <li>Feature Engineering : 15+ projets</li> </ul>"},{"location":"skills/#deep-learning-10-projets","title":"Deep Learning (10+ projets)","text":"<ul> <li>Computer Vision : 5 projets (CNN, YOLO)</li> <li>NLP : 4 projets (BERT, Transformers)</li> <li>Time Series : 2 projets (LSTM, GRU)</li> <li>Reinforcement Learning : 1 projet</li> </ul>"},{"location":"skills/#data-engineering-8-projets","title":"Data Engineering (8+ projets)","text":"<ul> <li>ETL Pipelines : 5 projets</li> <li>Big Data : 3 projets (Spark, Hadoop)</li> <li>APIs : 6 projets (FastAPI, Flask)</li> <li>D\u00e9ploiement : 10+ projets (Docker, AWS)</li> </ul>"},{"location":"skills/#evolution-des-competences","title":"\ud83d\udcc8 \u00c9volution des comp\u00e9tences","text":""},{"location":"skills/#2024","title":"2024","text":"<ul> <li>Nouveau : LangChain, Ray, Weights &amp; Biases</li> <li>Am\u00e9lioration : MLOps, Kubernetes, ONNX</li> <li>Projets : 5 nouveaux projets</li> </ul>"},{"location":"skills/#2023","title":"2023","text":"<ul> <li>Nouveau : Transformers, BERT, GPT</li> <li>Am\u00e9lioration : PyTorch, Computer Vision</li> <li>Projets : 8 nouveaux projets</li> </ul>"},{"location":"skills/#2022","title":"2022","text":"<ul> <li>Nouveau : Deep Learning, AWS</li> <li>Am\u00e9lioration : Machine Learning, Python</li> <li>Projets : 6 nouveaux projets</li> </ul>"},{"location":"skills/#objectifs-dapprentissage","title":"\ud83c\udfaf Objectifs d'apprentissage","text":""},{"location":"skills/#court-terme-6-mois","title":"Court terme (6 mois)","text":"<ul> <li> Ma\u00eetrise de LangChain pour les applications LLM</li> <li> Expertise en Ray pour le ML distribu\u00e9</li> <li> Certification Kubernetes</li> <li> Projet de computer vision avanc\u00e9</li> </ul>"},{"location":"skills/#moyen-terme-1-an","title":"Moyen terme (1 an)","text":"<ul> <li> Expertise en MLOps avec Kubeflow</li> <li> Ma\u00eetrise de l'optimisation de mod\u00e8les (ONNX, TensorRT)</li> <li> Projet de recherche en deep learning</li> <li> Contribution \u00e0 des projets open source</li> </ul>"},{"location":"skills/#long-terme-2-ans","title":"Long terme (2 ans)","text":"<ul> <li> Expertise en recherche en IA</li> <li> Leadership technique</li> <li> Cr\u00e9ation d'un framework ML</li> <li> Publication d'articles scientifiques</li> </ul>"},{"location":"skills/#stack-technique-prefere","title":"\ud83d\udee0\ufe0f Stack technique pr\u00e9f\u00e9r\u00e9","text":""},{"location":"skills/#developpement","title":"D\u00e9veloppement","text":"<ul> <li>IDE : VS Code, Jupyter Lab</li> <li>Versioning : Git, GitHub</li> <li>Environnement : Conda, Docker</li> <li>Testing : pytest, unittest</li> </ul>"},{"location":"skills/#mlai","title":"ML/AI","text":"<ul> <li>Frameworks : PyTorch, Scikit-learn</li> <li>Optimisation : Optuna, Hyperopt</li> <li>Monitoring : MLflow, Weights &amp; Biases</li> <li>D\u00e9ploiement : FastAPI, Docker</li> </ul>"},{"location":"skills/#data","title":"Data","text":"<ul> <li>Processing : Pandas, NumPy, Spark</li> <li>Visualisation : Matplotlib, Seaborn, Plotly</li> <li>Databases : PostgreSQL, Redis, MongoDB</li> <li>Cloud : AWS (S3, EC2, ECS, Lambda)</li> </ul>"},{"location":"skills/#ressources-dapprentissage","title":"\ud83d\udcda Ressources d'apprentissage","text":""},{"location":"skills/#cours-et-formations","title":"Cours et formations","text":"<ul> <li>Coursera : Deep Learning Specialization (Andrew Ng)</li> <li>Udacity : Machine Learning Engineer Nanodegree</li> <li>Fast.ai : Practical Deep Learning for Coders</li> <li>Coursera : Machine Learning (Stanford)</li> </ul>"},{"location":"skills/#livres-recommandes","title":"Livres recommand\u00e9s","text":"<ul> <li>Hands-On Machine Learning (Aur\u00e9lien G\u00e9ron)</li> <li>Deep Learning (Ian Goodfellow)</li> <li>Pattern Recognition and Machine Learning (Christopher Bishop)</li> <li>The Elements of Statistical Learning (Hastie, Tibshirani, Friedman)</li> </ul>"},{"location":"skills/#blogs-et-ressources","title":"Blogs et ressources","text":"<ul> <li>Towards Data Science (Medium)</li> <li>Distill.pub (Visualisations ML)</li> <li>Papers with Code (Recherche)</li> <li>Kaggle Learn (Tutoriels pratiques)</li> </ul>"},{"location":"skills/#collaboration-et-contribution","title":"\ud83e\udd1d Collaboration et contribution","text":""},{"location":"skills/#open-source","title":"Open Source","text":"<ul> <li>Contributions : 15+ repositories</li> <li>Stars : 500+ sur mes projets</li> <li>Forks : 100+ sur mes projets</li> <li>Issues : 50+ r\u00e9solues</li> </ul>"},{"location":"skills/#communaute","title":"Communaut\u00e9","text":"<ul> <li>Meetups : 20+ participations</li> <li>Conf\u00e9rences : 5+ pr\u00e9sentations</li> <li>Mentoring : 10+ \u00e9tudiants</li> <li>Blog : 25+ articles techniques</li> </ul>"},{"location":"skills/#contact","title":"\ud83d\udcde Contact","text":"<p>Int\u00e9ress\u00e9 par une collaboration ? N'h\u00e9sitez pas \u00e0 me contacter !</p> <ul> <li>\ud83d\udce7 Email : loick.dernoncourt@example.com</li> <li>\ud83d\udcbc LinkedIn : linkedin.com/in/loick-dernoncourt</li> <li>\ud83d\udc19 GitHub : github.com/loick-dernoncourt</li> <li>\ud83d\udc26 Twitter : @loick_dernoncourt</li> </ul> <p>Derni\u00e8re mise \u00e0 jour : October 22, 2025</p>"},{"location":"skills/data-engineering/","title":"\ud83d\uddc4\ufe0f Data Engineering","text":"<p>Expertise en data engineering avec 3+ ann\u00e9es d'exp\u00e9rience et 8+ projets r\u00e9alis\u00e9s.</p>"},{"location":"skills/data-engineering/#competences-principales","title":"\ud83c\udfaf Comp\u00e9tences principales","text":""},{"location":"skills/data-engineering/#technologies-de-donnees","title":"Technologies de donn\u00e9es","text":"<ul> <li>Big Data : Apache Spark, Hadoop, Hive</li> <li>Databases : PostgreSQL, MongoDB, Redis, Elasticsearch</li> <li>Cloud : AWS (S3, EC2, ECS, Lambda), Google Cloud, Azure</li> <li>Streaming : Kafka, Apache Flink, Apache Storm</li> <li>Orchestration : Airflow, Prefect, Dagster</li> </ul>"},{"location":"skills/data-engineering/#outils-de-developpement","title":"Outils de d\u00e9veloppement","text":"<ul> <li>Conteneurisation : Docker, Kubernetes</li> <li>CI/CD : GitHub Actions, GitLab CI, Jenkins</li> <li>Monitoring : Prometheus, Grafana, ELK Stack</li> <li>Versioning : Git, DVC (Data Version Control)</li> </ul>"},{"location":"skills/data-engineering/#langages-et-frameworks","title":"Langages et frameworks","text":"<ul> <li>Python : Pandas, NumPy, PySpark, FastAPI</li> <li>SQL : PostgreSQL, MySQL, BigQuery</li> <li>Scala : Apache Spark</li> <li>Bash : Scripting et automation</li> </ul>"},{"location":"skills/data-engineering/#stack-technique","title":"\ud83d\udee0\ufe0f Stack technique","text":""},{"location":"skills/data-engineering/#frameworks-principaux","title":"Frameworks principaux","text":"<pre><code># Apache Spark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n# FastAPI\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport uvicorn\n\n# Airflow\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\n</code></pre>"},{"location":"skills/data-engineering/#outils-de-developpement_1","title":"Outils de d\u00e9veloppement","text":"<ul> <li>Docker : Conteneurisation des applications</li> <li>Kubernetes : Orchestration des conteneurs</li> <li>Terraform : Infrastructure as Code</li> <li>Ansible : Configuration management</li> </ul>"},{"location":"skills/data-engineering/#projets-realises","title":"\ud83d\udcca Projets r\u00e9alis\u00e9s","text":""},{"location":"skills/data-engineering/#pipeline-etl-pour-e-commerce","title":"Pipeline ETL pour e-commerce","text":"<p>Technologies : Apache Spark, PostgreSQL, Airflow R\u00e9sultat : Traitement de 10M+ enregistrements/jour Impact : R\u00e9duction de 60% du temps de traitement</p>"},{"location":"skills/data-engineering/#api-de-donnees-en-temps-reel","title":"API de donn\u00e9es en temps r\u00e9el","text":"<p>Technologies : FastAPI, Redis, Kafka R\u00e9sultat : 1000 requ\u00eates/seconde, 99.9% de disponibilit\u00e9 Impact : Am\u00e9lioration de 40% des performances</p>"},{"location":"skills/data-engineering/#data-lake-sur-aws","title":"Data Lake sur AWS","text":"<p>Technologies : S3, Glue, Athena, Redshift R\u00e9sultat : 100TB+ de donn\u00e9es stock\u00e9es Impact : R\u00e9duction de 50% des co\u00fbts de stockage</p>"},{"location":"skills/data-engineering/#methodologie","title":"\ud83d\udd2c M\u00e9thodologie","text":""},{"location":"skills/data-engineering/#1-architecture-de-donnees","title":"1. Architecture de donn\u00e9es","text":"<pre><code># Configuration Spark\nspark = SparkSession.builder \\\n    .appName(\"DataPipeline\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n    .getOrCreate()\n\n# Lecture des donn\u00e9es\ndf = spark.read \\\n    .format(\"parquet\") \\\n    .option(\"path\", \"s3://bucket/data/\") \\\n    .load()\n\n# Transformation des donn\u00e9es\ndf_transformed = df \\\n    .withColumn(\"date\", to_date(col(\"timestamp\"))) \\\n    .withColumn(\"hour\", hour(col(\"timestamp\"))) \\\n    .filter(col(\"amount\") &gt; 0) \\\n    .groupBy(\"date\", \"hour\") \\\n    .agg(sum(\"amount\").alias(\"total_amount\"))\n</code></pre>"},{"location":"skills/data-engineering/#2-pipeline-etl","title":"2. Pipeline ETL","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom datetime import datetime, timedelta\n\ndef extract_data():\n    \"\"\"Extraction des donn\u00e9es depuis la source\"\"\"\n    import pandas as pd\n    import requests\n\n    # Extraction depuis API\n    response = requests.get(\"https://api.example.com/data\")\n    data = response.json()\n\n    # Sauvegarde en local\n    df = pd.DataFrame(data)\n    df.to_parquet(\"/tmp/raw_data.parquet\")\n\n    return \"Data extracted successfully\"\n\ndef transform_data():\n    \"\"\"Transformation des donn\u00e9es\"\"\"\n    import pandas as pd\n    import numpy as np\n\n    # Lecture des donn\u00e9es\n    df = pd.read_parquet(\"/tmp/raw_data.parquet\")\n\n    # Nettoyage et transformation\n    df = df.dropna()\n    df['amount'] = df['amount'].astype(float)\n    df['date'] = pd.to_datetime(df['date'])\n\n    # Calculs m\u00e9tier\n    df['amount_category'] = np.where(\n        df['amount'] &gt; 1000, 'high', \n        np.where(df['amount'] &gt; 100, 'medium', 'low')\n    )\n\n    # Sauvegarde\n    df.to_parquet(\"/tmp/transformed_data.parquet\")\n\n    return \"Data transformed successfully\"\n\ndef load_data():\n    \"\"\"Chargement des donn\u00e9es vers la destination\"\"\"\n    import pandas as pd\n    import psycopg2\n\n    # Lecture des donn\u00e9es transform\u00e9es\n    df = pd.read_parquet(\"/tmp/transformed_data.parquet\")\n\n    # Connexion \u00e0 la base de donn\u00e9es\n    conn = psycopg2.connect(\n        host=\"localhost\",\n        database=\"analytics\",\n        user=\"user\",\n        password=\"password\"\n    )\n\n    # Insertion des donn\u00e9es\n    df.to_sql('transactions', conn, if_exists='append', index=False)\n\n    return \"Data loaded successfully\"\n\n# D\u00e9finition du DAG\ndefault_args = {\n    'owner': 'data_team',\n    'depends_on_past': False,\n    'start_date': datetime(2024, 1, 1),\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'data_pipeline',\n    default_args=default_args,\n    description='Pipeline ETL quotidien',\n    schedule_interval=timedelta(days=1),\n    catchup=False\n)\n\n# D\u00e9finition des t\u00e2ches\nextract_task = PythonOperator(\n    task_id='extract_data',\n    python_callable=extract_data,\n    dag=dag\n)\n\ntransform_task = PythonOperator(\n    task_id='transform_data',\n    python_callable=transform_data,\n    dag=dag\n)\n\nload_task = PythonOperator(\n    task_id='load_data',\n    python_callable=load_data,\n    dag=dag\n)\n\n# D\u00e9finition des d\u00e9pendances\nextract_task &gt;&gt; transform_task &gt;&gt; load_task\n</code></pre>"},{"location":"skills/data-engineering/#3-api-de-donnees","title":"3. API de donn\u00e9es","text":"<pre><code>from fastapi import FastAPI, HTTPException, Depends\nfrom pydantic import BaseModel\nimport redis\nimport json\nimport pandas as pd\nimport psycopg2\nfrom sqlalchemy import create_engine\n\napp = FastAPI(title=\"Data API\", version=\"1.0.0\")\n\n# Configuration Redis\nredis_client = redis.Redis(host='localhost', port=6379, db=0)\n\n# Configuration base de donn\u00e9es\nengine = create_engine('postgresql://user:password@localhost/analytics')\n\nclass DataRequest(BaseModel):\n    start_date: str\n    end_date: str\n    filters: dict = {}\n\nclass DataResponse(BaseModel):\n    data: list\n    total_records: int\n    execution_time: float\n\n@app.post(\"/data\", response_model=DataResponse)\nasync def get_data(request: DataRequest):\n    import time\n    start_time = time.time()\n\n    # V\u00e9rification du cache\n    cache_key = f\"data:{hash(str(request.dict()))}\"\n    cached_result = redis_client.get(cache_key)\n\n    if cached_result:\n        result = json.loads(cached_result)\n        result['execution_time'] = time.time() - start_time\n        return DataResponse(**result)\n\n    # Requ\u00eate \u00e0 la base de donn\u00e9es\n    query = f\"\"\"\n    SELECT * FROM transactions \n    WHERE date BETWEEN '{request.start_date}' AND '{request.end_date}'\n    \"\"\"\n\n    # Application des filtres\n    if request.filters:\n        for key, value in request.filters.items():\n            query += f\" AND {key} = '{value}'\"\n\n    # Ex\u00e9cution de la requ\u00eate\n    df = pd.read_sql(query, engine)\n\n    # Pr\u00e9paration de la r\u00e9ponse\n    result = {\n        'data': df.to_dict('records'),\n        'total_records': len(df),\n        'execution_time': time.time() - start_time\n    }\n\n    # Mise en cache\n    redis_client.setex(cache_key, 3600, json.dumps(result))\n\n    return DataResponse(**result)\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\", \"timestamp\": time.time()}\n</code></pre>"},{"location":"skills/data-engineering/#4-monitoring-et-alertes","title":"4. Monitoring et alertes","text":"<pre><code>import logging\nfrom prometheus_client import Counter, Histogram, generate_latest\nfrom flask import Flask, Response\n\n# M\u00e9triques Prometheus\nREQUEST_COUNT = Counter('data_requests_total', 'Total requests')\nREQUEST_LATENCY = Histogram('data_request_duration_seconds', 'Request latency')\nERROR_COUNT = Counter('data_errors_total', 'Total errors')\n\n# Configuration du logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('data_pipeline.log'),\n        logging.StreamHandler()\n    ]\n)\n\nlogger = logging.getLogger(__name__)\n\n@app.middleware(\"http\")\nasync def add_process_time_header(request, call_next):\n    start_time = time.time()\n\n    try:\n        response = await call_next(request)\n        REQUEST_COUNT.inc()\n        REQUEST_LATENCY.observe(time.time() - start_time)\n        return response\n    except Exception as e:\n        ERROR_COUNT.inc()\n        logger.error(f\"Error processing request: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/metrics\")\nasync def metrics():\n    return Response(generate_latest(), media_type=\"text/plain\")\n</code></pre>"},{"location":"skills/data-engineering/#metriques-de-performance","title":"\ud83d\udcc8 M\u00e9triques de performance","text":""},{"location":"skills/data-engineering/#pipeline-etl","title":"Pipeline ETL","text":"M\u00e9trique Valeur Description Throughput 10M records/day Volume trait\u00e9 par jour Latence 2 minutes Temps de traitement Disponibilit\u00e9 99.9% Uptime du pipeline Erreurs &lt; 0.1% Taux d'erreur"},{"location":"skills/data-engineering/#api-de-donnees","title":"API de donn\u00e9es","text":"M\u00e9trique Valeur Description QPS 1000 req/s Requ\u00eates par seconde Latence 50ms Temps de r\u00e9ponse Disponibilit\u00e9 99.9% Uptime de l'API Cache Hit Rate 85% Taux de cache"},{"location":"skills/data-engineering/#bonnes-pratiques","title":"\ud83c\udfaf Bonnes pratiques","text":""},{"location":"skills/data-engineering/#architecture","title":"Architecture","text":"<ul> <li>Scalabilit\u00e9 : Design horizontal</li> <li>R\u00e9silience : Gestion des erreurs</li> <li>Monitoring : M\u00e9triques et alertes</li> <li>Documentation : Code et architecture</li> </ul>"},{"location":"skills/data-engineering/#developpement","title":"D\u00e9veloppement","text":"<ul> <li>Versioning : Git et DVC</li> <li>Testing : Tests unitaires et d'int\u00e9gration</li> <li>CI/CD : Automatisation des d\u00e9ploiements</li> <li>Code Review : Validation par les pairs</li> </ul>"},{"location":"skills/data-engineering/#operations","title":"Op\u00e9rations","text":"<ul> <li>Monitoring : Prometheus, Grafana</li> <li>Logging : ELK Stack</li> <li>Alerting : PagerDuty, Slack</li> <li>Backup : Strat\u00e9gies de sauvegarde</li> </ul>"},{"location":"skills/data-engineering/#deploiement","title":"\ud83d\ude80 D\u00e9ploiement","text":""},{"location":"skills/data-engineering/#docker","title":"Docker","text":"<pre><code>FROM python:3.9-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nEXPOSE 8000\n\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre>"},{"location":"skills/data-engineering/#kubernetes","title":"Kubernetes","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: data-api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: data-api\n  template:\n    metadata:\n      labels:\n        app: data-api\n    spec:\n      containers:\n      - name: data-api\n        image: data-api:latest\n        ports:\n        - containerPort: 8000\n        env:\n        - name: DATABASE_URL\n          value: \"postgresql://user:password@db:5432/analytics\"\n        - name: REDIS_URL\n          value: \"redis://redis:6379\"\n</code></pre>"},{"location":"skills/data-engineering/#ressources-dapprentissage","title":"\ud83d\udcda Ressources d'apprentissage","text":""},{"location":"skills/data-engineering/#cours-recommandes","title":"Cours recommand\u00e9s","text":"<ul> <li>Udacity : Data Engineering Nanodegree</li> <li>Coursera : Big Data Specialization</li> <li>edX : Data Engineering with Python</li> </ul>"},{"location":"skills/data-engineering/#livres-essentiels","title":"Livres essentiels","text":"<ul> <li>Designing Data-Intensive Applications (Martin Kleppmann)</li> <li>Data Engineering Handbook (Data Engineering Team)</li> <li>Building Real-Time Data Pipelines (Ben Stopford)</li> </ul>"},{"location":"skills/data-engineering/#pratique","title":"Pratique","text":"<ul> <li>Apache Spark : Documentation officielle</li> <li>Airflow : Documentation officielle</li> <li>Docker : Documentation officielle</li> <li>Kubernetes : Documentation officielle</li> </ul> <p>Derni\u00e8re mise \u00e0 jour : October 22, 2025</p>"},{"location":"skills/deep-learning/","title":"\ud83e\udde0 Deep Learning","text":"<p>Expertise en deep learning avec 3+ ann\u00e9es d'exp\u00e9rience et 10+ projets r\u00e9alis\u00e9s.</p>"},{"location":"skills/deep-learning/#competences-principales","title":"\ud83c\udfaf Comp\u00e9tences principales","text":""},{"location":"skills/deep-learning/#architectures-de-reseaux","title":"Architectures de r\u00e9seaux","text":"<ul> <li>CNN : ResNet, VGG, EfficientNet, MobileNet</li> <li>RNN/LSTM : S\u00e9ries temporelles, NLP</li> <li>Transformers : BERT, GPT, Vision Transformer</li> <li>GANs : DCGAN, StyleGAN, CycleGAN</li> <li>Autoencoders : Variational Autoencoders (VAE)</li> </ul>"},{"location":"skills/deep-learning/#frameworks-et-outils","title":"Frameworks et outils","text":"<ul> <li>PyTorch : Framework principal (3 ans)</li> <li>TensorFlow/Keras : Framework secondaire (2 ans)</li> <li>Hugging Face : Transformers, Datasets</li> <li>Weights &amp; Biases : Exp\u00e9rimentation</li> <li>ONNX : Optimisation et d\u00e9ploiement</li> </ul>"},{"location":"skills/deep-learning/#domaines-dapplication","title":"Domaines d'application","text":"<ul> <li>Computer Vision : Classification, D\u00e9tection, Segmentation</li> <li>NLP : Sentiment Analysis, Text Classification, NER</li> <li>Time Series : Pr\u00e9diction, Anomaly Detection</li> <li>Reinforcement Learning : Q-Learning, Policy Gradient</li> </ul>"},{"location":"skills/deep-learning/#stack-technique","title":"\ud83d\udee0\ufe0f Stack technique","text":""},{"location":"skills/deep-learning/#frameworks-principaux","title":"Frameworks principaux","text":"<pre><code># PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\n\n# Hugging Face\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import TrainingArguments, Trainer\n\n# Computer Vision\nimport torchvision.models as models\nimport cv2\nfrom PIL import Image\n</code></pre>"},{"location":"skills/deep-learning/#outils-de-developpement","title":"Outils de d\u00e9veloppement","text":"<ul> <li>Jupyter Lab : D\u00e9veloppement interactif</li> <li>Weights &amp; Biases : Suivi des exp\u00e9riences</li> <li>MLflow : Gestion des mod\u00e8les</li> <li>Docker : Conteneurisation</li> </ul>"},{"location":"skills/deep-learning/#projets-realises","title":"\ud83d\udcca Projets r\u00e9alis\u00e9s","text":""},{"location":"skills/deep-learning/#classification-dimages-medicales","title":"Classification d'images m\u00e9dicales","text":"<p>Technologies : ResNet50, Transfer Learning, PyTorch R\u00e9sultat : 95.2% d'accuracy sur 10K images Impact : R\u00e9duction de 40% du temps de diagnostic</p>"},{"location":"skills/deep-learning/#analyse-de-sentiment-en-temps-reel","title":"Analyse de sentiment en temps r\u00e9el","text":"<p>Technologies : BERT, Transformers, FastAPI R\u00e9sultat : 94.5% d'accuracy, 50ms de latence Impact : API d\u00e9ploy\u00e9e avec 99.9% de disponibilit\u00e9</p>"},{"location":"skills/deep-learning/#detection-danomalies-industrielles","title":"D\u00e9tection d'anomalies industrielles","text":"<p>Technologies : CNN, OpenCV, YOLO R\u00e9sultat : 99.5% de pr\u00e9cision, 40% de r\u00e9duction des d\u00e9fauts Impact : Am\u00e9lioration significative de la qualit\u00e9</p>"},{"location":"skills/deep-learning/#methodologie","title":"\ud83d\udd2c M\u00e9thodologie","text":""},{"location":"skills/deep-learning/#1-preparation-des-donnees","title":"1. Pr\u00e9paration des donn\u00e9es","text":"<pre><code>import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom PIL import Image\n\nclass CustomDataset(Dataset):\n    def __init__(self, data, labels, transform=None):\n        self.data = data\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        image = self.data[idx]\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n# Transformations pour l'augmentation de donn\u00e9es\ntrain_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(degrees=15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                        std=[0.229, 0.224, 0.225])\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                        std=[0.229, 0.224, 0.225])\n])\n</code></pre>"},{"location":"skills/deep-learning/#2-architecture-de-modele","title":"2. Architecture de mod\u00e8le","text":"<pre><code>import torch.nn as nn\nimport torchvision.models as models\n\nclass CustomCNN(nn.Module):\n    def __init__(self, num_classes=10, pretrained=True):\n        super(CustomCNN, self).__init__()\n\n        # Backbone pr\u00e9-entra\u00een\u00e9\n        if pretrained:\n            self.backbone = models.resnet50(pretrained=True)\n            # Geler les premi\u00e8res couches\n            for param in self.backbone.parameters():\n                param.requires_grad = False\n\n            # D\u00e9geler les derni\u00e8res couches\n            for param in self.backbone.layer4.parameters():\n                param.requires_grad = True\n\n        # Classification head\n        num_features = self.backbone.fc.in_features\n        self.backbone.fc = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(num_features, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x):\n        return self.backbone(x)\n\n# Initialisation du mod\u00e8le\nmodel = CustomCNN(num_classes=10, pretrained=True)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n</code></pre>"},{"location":"skills/deep-learning/#3-entrainement","title":"3. Entra\u00eenement","text":"<pre><code>import torch.optim as optim\nfrom torch.utils.data import DataLoader\n\ndef train_epoch(model, train_loader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = output.max(1)\n        total += target.size(0)\n        correct += predicted.eq(target).sum().item()\n\n        if batch_idx % 100 == 0:\n            print(f'Batch {batch_idx}, Loss: {loss.item():.4f}')\n\n    return running_loss / len(train_loader), 100. * correct / total\n\ndef validate_epoch(model, val_loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for data, target in val_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            loss = criterion(output, target)\n\n            running_loss += loss.item()\n            _, predicted = output.max(1)\n            total += target.size(0)\n            correct += predicted.eq(target).sum().item()\n\n    return running_loss / len(val_loader), 100. * correct / total\n\n# Configuration de l'entra\u00eenement\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n\n# Boucle d'entra\u00eenement\nnum_epochs = 50\nbest_val_acc = 0\n\nfor epoch in range(num_epochs):\n    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n\n    scheduler.step(val_loss)\n\n    print(f'Epoch {epoch+1}/{num_epochs}:')\n    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n\n    # Sauvegarde du meilleur mod\u00e8le\n    if val_acc &gt; best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), 'best_model.pth')\n\n    print('-' * 50)\n</code></pre>"},{"location":"skills/deep-learning/#4-evaluation","title":"4. \u00c9valuation","text":"<pre><code>from sklearn.metrics import classification_report, confusion_matrix\nimport numpy as np\n\ndef evaluate_model(model, test_loader, device):\n    model.eval()\n    all_preds = []\n    all_targets = []\n    all_probs = []\n\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            probs = torch.softmax(output, dim=1)\n            _, predicted = output.max(1)\n\n            all_preds.extend(predicted.cpu().numpy())\n            all_targets.extend(target.cpu().numpy())\n            all_probs.extend(probs.cpu().numpy())\n\n    return np.array(all_preds), np.array(all_targets), np.array(all_probs)\n\n# \u00c9valuation finale\npredictions, targets, probabilities = evaluate_model(model, test_loader, device)\nprint(classification_report(targets, predictions))\n</code></pre>"},{"location":"skills/deep-learning/#metriques-de-performance","title":"\ud83d\udcc8 M\u00e9triques de performance","text":""},{"location":"skills/deep-learning/#computer-vision","title":"Computer Vision","text":"M\u00e9trique Valeur Description Accuracy 95.2% Pr\u00e9cision globale Precision 94.8% Pr\u00e9cision par classe Recall 95.1% Rappel par classe F1-Score 94.9% Score F1 harmonique AUC-ROC 0.98 Aire sous la courbe ROC"},{"location":"skills/deep-learning/#nlp","title":"NLP","text":"M\u00e9trique Valeur Description Accuracy 94.5% Pr\u00e9cision globale Precision 94.1% Pr\u00e9cision par classe Recall 94.8% Rappel par classe F1-Score 94.4% Score F1 harmonique Latence 50ms Temps de pr\u00e9diction"},{"location":"skills/deep-learning/#bonnes-pratiques","title":"\ud83c\udfaf Bonnes pratiques","text":""},{"location":"skills/deep-learning/#architecture","title":"Architecture","text":"<ul> <li>Transfer Learning : Utiliser des mod\u00e8les pr\u00e9-entra\u00een\u00e9s</li> <li>Regularization : Dropout, Batch Normalization</li> <li>Data Augmentation : Rotation, Flip, Color Jitter</li> <li>Ensemble Methods : Combinaison de mod\u00e8les</li> </ul>"},{"location":"skills/deep-learning/#entrainement","title":"Entra\u00eenement","text":"<ul> <li>Learning Rate Scheduling : ReduceLROnPlateau</li> <li>Early Stopping : \u00c9viter le surapprentissage</li> <li>Gradient Clipping : Stabiliser l'entra\u00eenement</li> <li>Mixed Precision : Acc\u00e9l\u00e9rer l'entra\u00eenement</li> </ul>"},{"location":"skills/deep-learning/#evaluation","title":"\u00c9valuation","text":"<ul> <li>Cross-Validation : Validation robuste</li> <li>Ablation Studies : Analyse des composants</li> <li>Error Analysis : Compr\u00e9hension des erreurs</li> <li>Visualization : Grad-CAM, t-SNE</li> </ul>"},{"location":"skills/deep-learning/#deploiement","title":"\ud83d\ude80 D\u00e9ploiement","text":""},{"location":"skills/deep-learning/#optimisation","title":"Optimisation","text":"<pre><code>import torch.onnx\nimport onnx\nimport onnxruntime\n\n# Conversion ONNX\ndummy_input = torch.randn(1, 3, 224, 224)\ntorch.onnx.export(\n    model, dummy_input, \"model.onnx\",\n    export_params=True, opset_version=11,\n    do_constant_folding=True,\n    input_names=['input'], output_names=['output']\n)\n\n# Validation ONNX\nonnx_model = onnx.load(\"model.onnx\")\nonnx.checker.check_model(onnx_model)\n</code></pre>"},{"location":"skills/deep-learning/#api-de-prediction","title":"API de pr\u00e9diction","text":"<pre><code>from fastapi import FastAPI, File, UploadFile\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport io\n\napp = FastAPI(title=\"Deep Learning Prediction API\")\n\n# Chargement du mod\u00e8le\nmodel = torch.load('best_model.pth', map_location='cpu')\nmodel.eval()\n\n# Transformations\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                        std=[0.229, 0.224, 0.225])\n])\n\n@app.post(\"/predict\")\nasync def predict(file: UploadFile = File(...)):\n    # Lecture de l'image\n    image = Image.open(io.BytesIO(await file.read()))\n\n    # Pr\u00e9processing\n    image_tensor = transform(image).unsqueeze(0)\n\n    # Pr\u00e9diction\n    with torch.no_grad():\n        outputs = model(image_tensor)\n        probabilities = torch.softmax(outputs, dim=1)\n        predicted_class = torch.argmax(probabilities, dim=1).item()\n        confidence = probabilities[0][predicted_class].item()\n\n    return {\n        \"predicted_class\": predicted_class,\n        \"confidence\": float(confidence),\n        \"probabilities\": probabilities[0].tolist()\n    }\n</code></pre>"},{"location":"skills/deep-learning/#ressources-dapprentissage","title":"\ud83d\udcda Ressources d'apprentissage","text":""},{"location":"skills/deep-learning/#cours-recommandes","title":"Cours recommand\u00e9s","text":"<ul> <li>Fast.ai : Practical Deep Learning for Coders</li> <li>Coursera : Deep Learning Specialization (Andrew Ng)</li> <li>Udacity : Deep Learning Nanodegree</li> </ul>"},{"location":"skills/deep-learning/#livres-essentiels","title":"Livres essentiels","text":"<ul> <li>Deep Learning (Ian Goodfellow)</li> <li>Hands-On Machine Learning (Aur\u00e9lien G\u00e9ron)</li> <li>Pattern Recognition and Machine Learning (Christopher Bishop)</li> </ul>"},{"location":"skills/deep-learning/#pratique","title":"Pratique","text":"<ul> <li>Papers with Code : Recherche et impl\u00e9mentations</li> <li>Hugging Face : Mod\u00e8les et datasets</li> <li>PyTorch : Documentation officielle</li> <li>Weights &amp; Biases : Exp\u00e9rimentation</li> </ul> <p>Derni\u00e8re mise \u00e0 jour : October 22, 2025</p>"},{"location":"skills/machine-learning/","title":"\ud83e\udd16 Machine Learning","text":"<p>Expertise en machine learning avec 5+ ann\u00e9es d'exp\u00e9rience et 20+ projets r\u00e9alis\u00e9s.</p>"},{"location":"skills/machine-learning/#competences-principales","title":"\ud83c\udfaf Comp\u00e9tences principales","text":""},{"location":"skills/machine-learning/#algorithmes-supervises","title":"Algorithmes supervis\u00e9s","text":"<ul> <li>Classification : Random Forest, SVM, XGBoost, LightGBM</li> <li>R\u00e9gression : Linear Regression, Ridge, Lasso, Elastic Net</li> <li>Ensemble Methods : Bagging, Boosting, Stacking</li> <li>Feature Selection : Recursive Feature Elimination, L1 Regularization</li> </ul>"},{"location":"skills/machine-learning/#algorithmes-non-supervises","title":"Algorithmes non-supervis\u00e9s","text":"<ul> <li>Clustering : K-Means, DBSCAN, Hierarchical Clustering</li> <li>Dimensionality Reduction : PCA, t-SNE, UMAP</li> <li>Anomaly Detection : Isolation Forest, One-Class SVM</li> <li>Association Rules : Apriori, FP-Growth</li> </ul>"},{"location":"skills/machine-learning/#optimisation-et-validation","title":"Optimisation et validation","text":"<ul> <li>Hyperparameter Tuning : Grid Search, Random Search, Bayesian Optimization</li> <li>Cross-Validation : K-Fold, Stratified, Time Series Split</li> <li>Model Selection : AIC, BIC, Cross-Validation</li> <li>Feature Engineering : Polynomial Features, Interaction Terms</li> </ul>"},{"location":"skills/machine-learning/#stack-technique","title":"\ud83d\udee0\ufe0f Stack technique","text":""},{"location":"skills/machine-learning/#frameworks-principaux","title":"Frameworks principaux","text":"<pre><code># Scikit-learn\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\n# XGBoost\nimport xgboost as xgb\nfrom xgboost import XGBClassifier, XGBRegressor\n\n# LightGBM\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier, LGBMRegressor\n</code></pre>"},{"location":"skills/machine-learning/#outils-de-developpement","title":"Outils de d\u00e9veloppement","text":"<ul> <li>Jupyter Notebooks : Exploration et prototypage</li> <li>MLflow : Gestion des exp\u00e9riences</li> <li>Optuna : Optimisation des hyperparam\u00e8tres</li> <li>SHAP : Explicabilit\u00e9 des mod\u00e8les</li> </ul>"},{"location":"skills/machine-learning/#projets-realises","title":"\ud83d\udcca Projets r\u00e9alis\u00e9s","text":""},{"location":"skills/machine-learning/#classification-dimages-medicales","title":"Classification d'images m\u00e9dicales","text":"<p>Technologies : Random Forest, Feature Engineering R\u00e9sultat : 95.2% d'accuracy sur 10K images Impact : R\u00e9duction de 40% du temps de diagnostic</p>"},{"location":"skills/machine-learning/#prediction-de-prix-immobiliers","title":"Pr\u00e9diction de prix immobiliers","text":"<p>Technologies : XGBoost, Feature Engineering R\u00e9sultat : RMSE 0.15, R\u00b2 0.87 Impact : Am\u00e9lioration de 30% de la pr\u00e9cision</p>"},{"location":"skills/machine-learning/#segmentation-de-clients","title":"Segmentation de clients","text":"<p>Technologies : K-Means, PCA, Feature Engineering R\u00e9sultat : 4 segments identifi\u00e9s avec 85% de coh\u00e9rence Impact : +25% de conversion marketing</p>"},{"location":"skills/machine-learning/#methodologie","title":"\ud83d\udd2c M\u00e9thodologie","text":""},{"location":"skills/machine-learning/#1-analyse-exploratoire","title":"1. Analyse exploratoire","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Chargement et exploration\ndf = pd.read_csv('data.csv')\nprint(df.info())\nprint(df.describe())\n\n# Visualisation des distributions\nplt.figure(figsize=(12, 8))\ndf.hist(bins=50, figsize=(12, 8))\nplt.show()\n\n# Matrice de corr\u00e9lation\ncorrelation_matrix = df.corr()\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\nplt.show()\n</code></pre>"},{"location":"skills/machine-learning/#2-preprocessing","title":"2. Pr\u00e9processing","text":"<pre><code>from sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# Nettoyage des donn\u00e9es\ndf = df.dropna()\ndf = df.drop_duplicates()\n\n# Encodage des variables cat\u00e9gorielles\nle = LabelEncoder()\ndf['categorical_var'] = le.fit_transform(df['categorical_var'])\n\n# Normalisation\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Division train/test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n</code></pre>"},{"location":"skills/machine-learning/#3-feature-engineering","title":"3. Feature Engineering","text":"<pre><code># Cr\u00e9ation de nouvelles features\ndf['feature_ratio'] = df['var1'] / df['var2']\ndf['feature_interaction'] = df['var1'] * df['var2']\ndf['feature_polynomial'] = df['var1'] ** 2\n\n# S\u00e9lection des features\nfrom sklearn.feature_selection import SelectKBest, f_classif\nselector = SelectKBest(f_classif, k=10)\nX_selected = selector.fit_transform(X, y)\n\n# Importance des features\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\nfeature_importance = rf.feature_importances_\n</code></pre>"},{"location":"skills/machine-learning/#4-modelisation","title":"4. Mod\u00e9lisation","text":"<pre><code>from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\n\n# Mod\u00e8les \u00e0 comparer\nmodels = {\n    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n    'SVM': SVC(random_state=42),\n    'Logistic Regression': LogisticRegression(random_state=42),\n    'XGBoost': xgb.XGBClassifier(random_state=42)\n}\n\n# Entra\u00eenement et \u00e9valuation\nresults = {}\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    results[name] = accuracy\n    print(f\"{name}: {accuracy:.3f}\")\n</code></pre>"},{"location":"skills/machine-learning/#5-optimisation-des-hyperparametres","title":"5. Optimisation des hyperparam\u00e8tres","text":"<pre><code>from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer, f1_score\n\n# Grille de param\u00e8tres pour XGBoost\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [3, 6, 9],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'subsample': [0.8, 0.9, 1.0]\n}\n\n# Recherche par grille\ngrid_search = GridSearchCV(\n    xgb.XGBClassifier(random_state=42),\n    param_grid,\n    cv=5,\n    scoring='f1_weighted',\n    n_jobs=-1\n)\n\ngrid_search.fit(X_train, y_train)\nprint(\"Meilleurs param\u00e8tres:\", grid_search.best_params_)\nprint(\"Meilleur score:\", grid_search.best_score_)\n</code></pre>"},{"location":"skills/machine-learning/#6-evaluation-et-validation","title":"6. \u00c9valuation et validation","text":"<pre><code>from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import cross_val_score\n\n# M\u00e9triques de performance\ny_pred = grid_search.best_estimator_.predict(X_test)\nprint(classification_report(y_test, y_pred))\n\n# Matrice de confusion\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title('Matrice de confusion')\nplt.show()\n\n# Validation crois\u00e9e\ncv_scores = cross_val_score(\n    grid_search.best_estimator_, X, y, cv=5, scoring='f1_weighted'\n)\nprint(f\"CV Score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n</code></pre>"},{"location":"skills/machine-learning/#metriques-de-performance","title":"\ud83d\udcc8 M\u00e9triques de performance","text":""},{"location":"skills/machine-learning/#classification","title":"Classification","text":"M\u00e9trique Valeur Description Accuracy 94.2% Pr\u00e9cision globale Precision 93.8% Pr\u00e9cision par classe Recall 94.1% Rappel par classe F1-Score 93.9% Score F1 harmonique AUC-ROC 0.96 Aire sous la courbe ROC"},{"location":"skills/machine-learning/#regression","title":"R\u00e9gression","text":"M\u00e9trique Valeur Description RMSE 0.15 Racine de l'erreur quadratique MAE 0.12 Erreur absolue moyenne R\u00b2 0.87 Coefficient de d\u00e9termination MAPE 15.2% Erreur absolue en pourcentage"},{"location":"skills/machine-learning/#bonnes-pratiques","title":"\ud83c\udfaf Bonnes pratiques","text":""},{"location":"skills/machine-learning/#preprocessing","title":"Pr\u00e9processing","text":"<ul> <li>Gestion des valeurs manquantes : Imputation intelligente</li> <li>Normalisation : StandardScaler pour la plupart des algorithmes</li> <li>Encodage : LabelEncoder pour les variables ordinales</li> <li>Feature Engineering : Cr\u00e9ation de features m\u00e9tier</li> </ul>"},{"location":"skills/machine-learning/#modelisation","title":"Mod\u00e9lisation","text":"<ul> <li>Validation crois\u00e9e : Toujours utiliser CV pour l'\u00e9valuation</li> <li>Hyperparameter tuning : Optimisation syst\u00e9matique</li> <li>Ensemble methods : Combinaison de mod\u00e8les</li> <li>Feature selection : R\u00e9duction de la dimensionnalit\u00e9</li> </ul>"},{"location":"skills/machine-learning/#evaluation","title":"\u00c9valuation","text":"<ul> <li>M\u00e9triques appropri\u00e9es : Choix selon le probl\u00e8me</li> <li>Validation temporelle : Pour les donn\u00e9es temporelles</li> <li>Test A/B : Validation en conditions r\u00e9elles</li> <li>Monitoring : Suivi des performances en production</li> </ul>"},{"location":"skills/machine-learning/#deploiement","title":"\ud83d\ude80 D\u00e9ploiement","text":""},{"location":"skills/machine-learning/#mlops","title":"MLOps","text":"<pre><code>import mlflow\nimport mlflow.sklearn\nimport joblib\n\n# Sauvegarde du mod\u00e8le\nmlflow.sklearn.log_model(\n    grid_search.best_estimator_, \n    \"model\",\n    registered_model_name=\"best_model\"\n)\n\n# Sauvegarde des pr\u00e9processeurs\njoblib.dump(scaler, 'scaler.pkl')\njoblib.dump(le, 'label_encoder.pkl')\n</code></pre>"},{"location":"skills/machine-learning/#api-de-prediction","title":"API de pr\u00e9diction","text":"<pre><code>from fastapi import FastAPI\nimport joblib\nimport pandas as pd\n\napp = FastAPI()\n\n# Chargement du mod\u00e8le\nmodel = joblib.load('best_model.pkl')\nscaler = joblib.load('scaler.pkl')\n\n@app.post(\"/predict\")\nasync def predict(data: dict):\n    # Pr\u00e9processing\n    df = pd.DataFrame([data])\n    df_scaled = scaler.transform(df)\n\n    # Pr\u00e9diction\n    prediction = model.predict(df_scaled)\n    probability = model.predict_proba(df_scaled)\n\n    return {\n        \"prediction\": int(prediction[0]),\n        \"probability\": float(probability[0].max())\n    }\n</code></pre>"},{"location":"skills/machine-learning/#ressources-dapprentissage","title":"\ud83d\udcda Ressources d'apprentissage","text":""},{"location":"skills/machine-learning/#cours-recommandes","title":"Cours recommand\u00e9s","text":"<ul> <li>Coursera : Machine Learning (Stanford)</li> <li>Udacity : Machine Learning Engineer Nanodegree</li> <li>Fast.ai : Practical Machine Learning for Coders</li> </ul>"},{"location":"skills/machine-learning/#livres-essentiels","title":"Livres essentiels","text":"<ul> <li>Hands-On Machine Learning (Aur\u00e9lien G\u00e9ron)</li> <li>The Elements of Statistical Learning (Hastie, Tibshirani, Friedman)</li> <li>Pattern Recognition and Machine Learning (Christopher Bishop)</li> </ul>"},{"location":"skills/machine-learning/#pratique","title":"Pratique","text":"<ul> <li>Kaggle : Comp\u00e9titions et datasets</li> <li>Scikit-learn : Documentation officielle</li> <li>Papers with Code : Recherche et impl\u00e9mentations</li> </ul> <p>Derni\u00e8re mise \u00e0 jour : October 22, 2025</p>"},{"location":"skills/visualisation/","title":"\ud83d\udcca Visualisation de donn\u00e9es","text":"<p>Expertise en visualisation de donn\u00e9es avec 4+ ann\u00e9es d'exp\u00e9rience et 15+ projets r\u00e9alis\u00e9s.</p>"},{"location":"skills/visualisation/#competences-principales","title":"\ud83c\udfaf Comp\u00e9tences principales","text":""},{"location":"skills/visualisation/#bibliotheques-python","title":"Biblioth\u00e8ques Python","text":"<ul> <li>Matplotlib : Visualisations statiques et interactives</li> <li>Seaborn : Visualisations statistiques avanc\u00e9es</li> <li>Plotly : Graphiques interactifs et dashboards</li> <li>Bokeh : Visualisations web interactives</li> <li>Altair : Grammaire de graphiques</li> </ul>"},{"location":"skills/visualisation/#outils-de-bi","title":"Outils de BI","text":"<ul> <li>Tableau : Dashboards et rapports</li> <li>Power BI : Visualisations Microsoft</li> <li>Grafana : Monitoring et m\u00e9triques</li> <li>D3.js : Visualisations web personnalis\u00e9es</li> </ul>"},{"location":"skills/visualisation/#types-de-visualisations","title":"Types de visualisations","text":"<ul> <li>Statistiques : Histogrammes, Box plots, Scatter plots</li> <li>Temporelles : Time series, Gantt charts</li> <li>G\u00e9ographiques : Cartes, Heat maps</li> <li>Hi\u00e9rarchiques : Treemaps, Sunburst</li> <li>R\u00e9seaux : Graph networks, Sankey diagrams</li> </ul>"},{"location":"skills/visualisation/#stack-technique","title":"\ud83d\udee0\ufe0f Stack technique","text":""},{"location":"skills/visualisation/#bibliotheques-python_1","title":"Biblioth\u00e8ques Python","text":"<pre><code># Matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom matplotlib.patches import Rectangle\n\n# Seaborn\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_palette(\"husl\")\n\n# Plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Pandas\nimport pandas as pd\nimport numpy as np\n</code></pre>"},{"location":"skills/visualisation/#outils-de-developpement","title":"Outils de d\u00e9veloppement","text":"<ul> <li>Jupyter Lab : D\u00e9veloppement interactif</li> <li>Streamlit : Applications web rapides</li> <li>Dash : Dashboards interactifs</li> <li>Bokeh : Visualisations web</li> </ul>"},{"location":"skills/visualisation/#projets-realises","title":"\ud83d\udcca Projets r\u00e9alis\u00e9s","text":""},{"location":"skills/visualisation/#dashboard-de-ventes-e-commerce","title":"Dashboard de ventes e-commerce","text":"<p>Technologies : Plotly, Dash, PostgreSQL R\u00e9sultat : Dashboard interactif avec 10+ graphiques Impact : Am\u00e9lioration de 25% de l'analyse des ventes</p>"},{"location":"skills/visualisation/#visualisation-de-donnees-geographiques","title":"Visualisation de donn\u00e9es g\u00e9ographiques","text":"<p>Technologies : Folium, GeoPandas, OpenStreetMap R\u00e9sultat : Cartes interactives avec 50K+ points Impact : Identification de 5 zones \u00e0 fort potentiel</p>"},{"location":"skills/visualisation/#analyse-de-sentiment-en-temps-reel","title":"Analyse de sentiment en temps r\u00e9el","text":"<p>Technologies : Matplotlib, Seaborn, Real-time data R\u00e9sultat : Graphiques temps r\u00e9el avec 1K+ points/seconde Impact : Monitoring en temps r\u00e9el des opinions</p>"},{"location":"skills/visualisation/#methodologie","title":"\ud83d\udd2c M\u00e9thodologie","text":""},{"location":"skills/visualisation/#1-exploration-des-donnees","title":"1. Exploration des donn\u00e9es","text":"<pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Chargement des donn\u00e9es\ndf = pd.read_csv('data.csv')\n\n# Statistiques descriptives\nprint(df.describe())\n\n# Visualisation de la distribution\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Histogramme\naxes[0, 0].hist(df['price'], bins=50, alpha=0.7)\naxes[0, 0].set_title('Distribution des prix')\naxes[0, 0].set_xlabel('Prix')\naxes[0, 0].set_ylabel('Fr\u00e9quence')\n\n# Box plot\naxes[0, 1].boxplot(df['price'])\naxes[0, 1].set_title('Box plot des prix')\naxes[0, 1].set_ylabel('Prix')\n\n# Scatter plot\naxes[1, 0].scatter(df['area'], df['price'], alpha=0.5)\naxes[1, 0].set_title('Prix vs Surface')\naxes[1, 0].set_xlabel('Surface')\naxes[1, 0].set_ylabel('Prix')\n\n# Corr\u00e9lation\ncorrelation_matrix = df.corr()\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', ax=axes[1, 1])\naxes[1, 1].set_title('Matrice de corr\u00e9lation')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"skills/visualisation/#2-visualisations-avancees","title":"2. Visualisations avanc\u00e9es","text":"<pre><code>import plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Graphique interactif avec Plotly\nfig = px.scatter(df, x='area', y='price', color='district',\n                 size='rooms', hover_data=['price', 'area'],\n                 title='Prix vs Surface par district')\n\nfig.update_layout(\n    title_font_size=20,\n    xaxis_title=\"Surface (m\u00b2)\",\n    yaxis_title=\"Prix (\u20ac)\",\n    legend_title=\"District\"\n)\n\nfig.show()\n\n# Graphique en barres empil\u00e9es\nfig = px.bar(df, x='district', y='price', color='type',\n             title='Prix par district et type de bien')\n\nfig.update_layout(\n    xaxis_title=\"District\",\n    yaxis_title=\"Prix moyen (\u20ac)\",\n    legend_title=\"Type de bien\"\n)\n\nfig.show()\n</code></pre>"},{"location":"skills/visualisation/#3-dashboards-interactifs","title":"3. Dashboards interactifs","text":"<pre><code>import dash\nfrom dash import dcc, html, Input, Output\nimport plotly.express as px\nimport pandas as pd\n\n# Initialisation de l'application Dash\napp = dash.Dash(__name__)\n\n# Layout du dashboard\napp.layout = html.Div([\n    html.H1(\"Dashboard de ventes\", style={'textAlign': 'center'}),\n\n    # Filtres\n    html.Div([\n        html.Label(\"S\u00e9lectionner le district:\"),\n        dcc.Dropdown(\n            id='district-dropdown',\n            options=[{'label': i, 'value': i} for i in df['district'].unique()],\n            value=df['district'].unique()[0]\n        )\n    ], style={'width': '30%', 'display': 'inline-block'}),\n\n    # Graphiques\n    dcc.Graph(id='price-chart'),\n    dcc.Graph(id='area-chart')\n])\n\n# Callbacks pour l'interactivit\u00e9\n@app.callback(\n    [Output('price-chart', 'figure'),\n     Output('area-chart', 'figure')],\n    [Input('district-dropdown', 'value')]\n)\ndef update_charts(selected_district):\n    filtered_df = df[df['district'] == selected_district]\n\n    # Graphique des prix\n    price_fig = px.histogram(filtered_df, x='price', nbins=30,\n                           title=f'Distribution des prix - {selected_district}')\n\n    # Graphique des surfaces\n    area_fig = px.scatter(filtered_df, x='area', y='price',\n                         title=f'Prix vs Surface - {selected_district}')\n\n    return price_fig, area_fig\n\nif __name__ == '__main__':\n    app.run_server(debug=True)\n</code></pre>"},{"location":"skills/visualisation/#4-visualisations-geographiques","title":"4. Visualisations g\u00e9ographiques","text":"<pre><code>import folium\nimport geopandas as gpd\nfrom folium import plugins\n\n# Cr\u00e9ation d'une carte interactive\nm = folium.Map(location=[48.8566, 2.3522], zoom_start=12)\n\n# Ajout de marqueurs\nfor idx, row in df.iterrows():\n    folium.Marker(\n        location=[row['latitude'], row['longitude']],\n        popup=f\"Prix: {row['price']}\u20ac&lt;br&gt;Surface: {row['area']}m\u00b2\",\n        icon=folium.Icon(color='red', icon='home')\n    ).add_to(m)\n\n# Heatmap\nheat_data = [[row['latitude'], row['longitude']] for idx, row in df.iterrows()]\nplugins.HeatMap(heat_data).add_to(m)\n\n# Sauvegarde de la carte\nm.save('map.html')\n</code></pre>"},{"location":"skills/visualisation/#types-de-visualisations_1","title":"\ud83d\udcc8 Types de visualisations","text":""},{"location":"skills/visualisation/#visualisations-statistiques","title":"Visualisations statistiques","text":"<pre><code># Histogramme avec distribution normale\nfig, ax = plt.subplots(figsize=(10, 6))\nax.hist(df['price'], bins=50, alpha=0.7, density=True, label='Donn\u00e9es')\nax.axvline(df['price'].mean(), color='red', linestyle='--', label='Moyenne')\nax.axvline(df['price'].median(), color='green', linestyle='--', label='M\u00e9diane')\nax.set_xlabel('Prix (\u20ac)')\nax.set_ylabel('Densit\u00e9')\nax.set_title('Distribution des prix')\nax.legend()\nplt.show()\n\n# Box plot par groupe\nfig, ax = plt.subplots(figsize=(12, 6))\nsns.boxplot(data=df, x='district', y='price', ax=ax)\nax.set_title('Distribution des prix par district')\nax.set_xlabel('District')\nax.set_ylabel('Prix (\u20ac)')\nplt.xticks(rotation=45)\nplt.show()\n</code></pre>"},{"location":"skills/visualisation/#visualisations-temporelles","title":"Visualisations temporelles","text":"<pre><code># Time series\nfig, ax = plt.subplots(figsize=(15, 6))\ndf_time = df.groupby('date')['price'].mean().reset_index()\nax.plot(df_time['date'], df_time['price'], linewidth=2)\nax.set_title('\u00c9volution du prix moyen dans le temps')\nax.set_xlabel('Date')\nax.set_ylabel('Prix moyen (\u20ac)')\nplt.xticks(rotation=45)\nplt.show()\n\n# Graphique en aires empil\u00e9es\nfig = px.area(df, x='date', y='price', color='district',\n              title='\u00c9volution des prix par district')\nfig.show()\n</code></pre>"},{"location":"skills/visualisation/#visualisations-geographiques","title":"Visualisations g\u00e9ographiques","text":"<pre><code># Carte de chaleur\nfig = px.density_mapbox(df, lat='latitude', lon='longitude', z='price',\n                        radius=10, center=dict(lat=48.8566, lon=2.3522),\n                        zoom=10, mapbox_style=\"open-street-map\")\nfig.update_layout(title=\"Carte de chaleur des prix\")\nfig.show()\n</code></pre>"},{"location":"skills/visualisation/#bonnes-pratiques","title":"\ud83c\udfaf Bonnes pratiques","text":""},{"location":"skills/visualisation/#design","title":"Design","text":"<ul> <li>Couleurs : Palette coh\u00e9rente et accessible</li> <li>Typographie : Lisibilit\u00e9 et hi\u00e9rarchie</li> <li>Espacement : \u00c9quilibre et clart\u00e9</li> <li>Interactivit\u00e9 : Engagement utilisateur</li> </ul>"},{"location":"skills/visualisation/#performance","title":"Performance","text":"<ul> <li>Optimisation : R\u00e9duction de la complexit\u00e9</li> <li>Caching : Mise en cache des donn\u00e9es</li> <li>Lazy loading : Chargement \u00e0 la demande</li> <li>Responsive : Adaptation aux \u00e9crans</li> </ul>"},{"location":"skills/visualisation/#accessibilite","title":"Accessibilit\u00e9","text":"<ul> <li>Contraste : Respect des standards WCAG</li> <li>Couleurs : Support daltonien</li> <li>Navigation : Clavier et lecteurs d'\u00e9cran</li> <li>Texte : Descriptions et l\u00e9gendes</li> </ul>"},{"location":"skills/visualisation/#deploiement","title":"\ud83d\ude80 D\u00e9ploiement","text":""},{"location":"skills/visualisation/#streamlit","title":"Streamlit","text":"<pre><code>import streamlit as st\nimport plotly.express as px\nimport pandas as pd\n\n# Configuration de la page\nst.set_page_config(page_title=\"Dashboard\", layout=\"wide\")\n\n# Titre\nst.title(\"Dashboard de ventes\")\n\n# Sidebar avec filtres\nst.sidebar.header(\"Filtres\")\ndistrict = st.sidebar.selectbox(\"District\", df['district'].unique())\nprice_range = st.sidebar.slider(\"Fourchette de prix\", \n                               int(df['price'].min()), \n                               int(df['price'].max()),\n                               (int(df['price'].min()), int(df['price'].max())))\n\n# Filtrage des donn\u00e9es\nfiltered_df = df[(df['district'] == district) &amp; \n                 (df['price'] &gt;= price_range[0]) &amp; \n                 (df['price'] &lt;= price_range[1])]\n\n# M\u00e9triques\ncol1, col2, col3, col4 = st.columns(4)\nwith col1:\n    st.metric(\"Nombre de biens\", len(filtered_df))\nwith col2:\n    st.metric(\"Prix moyen\", f\"{filtered_df['price'].mean():,.0f}\u20ac\")\nwith col3:\n    st.metric(\"Surface moyenne\", f\"{filtered_df['area'].mean():.0f}m\u00b2\")\nwith col4:\n    st.metric(\"Prix au m\u00b2\", f\"{filtered_df['price'].mean()/filtered_df['area'].mean():.0f}\u20ac/m\u00b2\")\n\n# Graphiques\nfig = px.scatter(filtered_df, x='area', y='price', color='type',\n                 title='Prix vs Surface par type de bien')\nst.plotly_chart(fig, use_container_width=True)\n</code></pre>"},{"location":"skills/visualisation/#dash","title":"Dash","text":"<pre><code>import dash\nfrom dash import dcc, html, Input, Output\nimport plotly.express as px\n\napp = dash.Dash(__name__)\n\napp.layout = html.Div([\n    html.H1(\"Dashboard de ventes\"),\n    dcc.Graph(id='scatter-plot'),\n    dcc.Slider(\n        id='price-slider',\n        min=df['price'].min(),\n        max=df['price'].max(),\n        value=df['price'].max(),\n        marks={str(price): str(price) for price in df['price'].unique()},\n        step=None\n    )\n])\n\n@app.callback(\n    Output('scatter-plot', 'figure'),\n    Input('price-slider', 'value')\n)\ndef update_figure(selected_price):\n    filtered_df = df[df['price'] &lt;= selected_price]\n    fig = px.scatter(filtered_df, x='area', y='price', color='type')\n    return fig\n\nif __name__ == '__main__':\n    app.run_server(debug=True)\n</code></pre>"},{"location":"skills/visualisation/#ressources-dapprentissage","title":"\ud83d\udcda Ressources d'apprentissage","text":""},{"location":"skills/visualisation/#cours-recommandes","title":"Cours recommand\u00e9s","text":"<ul> <li>Coursera : Data Visualization with Python</li> <li>Udacity : Data Visualization Nanodegree</li> <li>edX : Data Visualization Fundamentals</li> </ul>"},{"location":"skills/visualisation/#livres-essentiels","title":"Livres essentiels","text":"<ul> <li>The Visual Display of Quantitative Information (Edward Tufte)</li> <li>Storytelling with Data (Cole Nussbaumer Knaflic)</li> <li>Data Visualization (Kieran Healy)</li> </ul>"},{"location":"skills/visualisation/#pratique","title":"Pratique","text":"<ul> <li>Matplotlib : Documentation officielle</li> <li>Seaborn : Documentation officielle</li> <li>Plotly : Documentation officielle</li> <li>D3.js : Documentation officielle</li> </ul> <p>Derni\u00e8re mise \u00e0 jour : October 22, 2025</p>"}]}