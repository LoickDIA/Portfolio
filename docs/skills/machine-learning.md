# ü§ñ Machine Learning

Expertise en machine learning avec 5+ ann√©es d'exp√©rience et 20+ projets r√©alis√©s.

## üéØ Comp√©tences principales

### Algorithmes supervis√©s
- **Classification** : Random Forest, SVM, XGBoost, LightGBM
- **R√©gression** : Linear Regression, Ridge, Lasso, Elastic Net
- **Ensemble Methods** : Bagging, Boosting, Stacking
- **Feature Selection** : Recursive Feature Elimination, L1 Regularization

### Algorithmes non-supervis√©s
- **Clustering** : K-Means, DBSCAN, Hierarchical Clustering
- **Dimensionality Reduction** : PCA, t-SNE, UMAP
- **Anomaly Detection** : Isolation Forest, One-Class SVM
- **Association Rules** : Apriori, FP-Growth

### Optimisation et validation
- **Hyperparameter Tuning** : Grid Search, Random Search, Bayesian Optimization
- **Cross-Validation** : K-Fold, Stratified, Time Series Split
- **Model Selection** : AIC, BIC, Cross-Validation
- **Feature Engineering** : Polynomial Features, Interaction Terms

## üõ†Ô∏è Stack technique

### Frameworks principaux
```python
# Scikit-learn
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler, LabelEncoder

# XGBoost
import xgboost as xgb
from xgboost import XGBClassifier, XGBRegressor

# LightGBM
import lightgbm as lgb
from lightgbm import LGBMClassifier, LGBMRegressor
```

### Outils de d√©veloppement
- **Jupyter Notebooks** : Exploration et prototypage
- **MLflow** : Gestion des exp√©riences
- **Optuna** : Optimisation des hyperparam√®tres
- **SHAP** : Explicabilit√© des mod√®les

## üìä Projets r√©alis√©s

### Classification d'images m√©dicales
**Technologies** : Random Forest, Feature Engineering  
**R√©sultat** : 95.2% d'accuracy sur 10K images  
**Impact** : R√©duction de 40% du temps de diagnostic

### Pr√©diction de prix immobiliers
**Technologies** : XGBoost, Feature Engineering  
**R√©sultat** : RMSE 0.15, R¬≤ 0.87  
**Impact** : Am√©lioration de 30% de la pr√©cision

### Segmentation de clients
**Technologies** : K-Means, PCA, Feature Engineering  
**R√©sultat** : 4 segments identifi√©s avec 85% de coh√©rence  
**Impact** : +25% de conversion marketing

## üî¨ M√©thodologie

### 1. Analyse exploratoire
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Chargement et exploration
df = pd.read_csv('data.csv')
print(df.info())
print(df.describe())

# Visualisation des distributions
plt.figure(figsize=(12, 8))
df.hist(bins=50, figsize=(12, 8))
plt.show()

# Matrice de corr√©lation
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.show()
```

### 2. Pr√©processing
```python
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split

# Nettoyage des donn√©es
df = df.dropna()
df = df.drop_duplicates()

# Encodage des variables cat√©gorielles
le = LabelEncoder()
df['categorical_var'] = le.fit_transform(df['categorical_var'])

# Normalisation
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Division train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
```

### 3. Feature Engineering
```python
# Cr√©ation de nouvelles features
df['feature_ratio'] = df['var1'] / df['var2']
df['feature_interaction'] = df['var1'] * df['var2']
df['feature_polynomial'] = df['var1'] ** 2

# S√©lection des features
from sklearn.feature_selection import SelectKBest, f_classif
selector = SelectKBest(f_classif, k=10)
X_selected = selector.fit_transform(X, y)

# Importance des features
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
feature_importance = rf.feature_importances_
```

### 4. Mod√©lisation
```python
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
import xgboost as xgb

# Mod√®les √† comparer
models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42),
    'SVM': SVC(random_state=42),
    'Logistic Regression': LogisticRegression(random_state=42),
    'XGBoost': xgb.XGBClassifier(random_state=42)
}

# Entra√Ænement et √©valuation
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    results[name] = accuracy
    print(f"{name}: {accuracy:.3f}")
```

### 5. Optimisation des hyperparam√®tres
```python
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, f1_score

# Grille de param√®tres pour XGBoost
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 0.9, 1.0]
}

# Recherche par grille
grid_search = GridSearchCV(
    xgb.XGBClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='f1_weighted',
    n_jobs=-1
)

grid_search.fit(X_train, y_train)
print("Meilleurs param√®tres:", grid_search.best_params_)
print("Meilleur score:", grid_search.best_score_)
```

### 6. √âvaluation et validation
```python
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.model_selection import cross_val_score

# M√©triques de performance
y_pred = grid_search.best_estimator_.predict(X_test)
print(classification_report(y_test, y_pred))

# Matrice de confusion
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Matrice de confusion')
plt.show()

# Validation crois√©e
cv_scores = cross_val_score(
    grid_search.best_estimator_, X, y, cv=5, scoring='f1_weighted'
)
print(f"CV Score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")
```

## üìà M√©triques de performance

### Classification
| M√©trique | Valeur | Description |
|----------|--------|-------------|
| Accuracy | 94.2% | Pr√©cision globale |
| Precision | 93.8% | Pr√©cision par classe |
| Recall | 94.1% | Rappel par classe |
| F1-Score | 93.9% | Score F1 harmonique |
| AUC-ROC | 0.96 | Aire sous la courbe ROC |

### R√©gression
| M√©trique | Valeur | Description |
|----------|--------|-------------|
| RMSE | 0.15 | Racine de l'erreur quadratique |
| MAE | 0.12 | Erreur absolue moyenne |
| R¬≤ | 0.87 | Coefficient de d√©termination |
| MAPE | 15.2% | Erreur absolue en pourcentage |

## üéØ Bonnes pratiques

### Pr√©processing
- **Gestion des valeurs manquantes** : Imputation intelligente
- **Normalisation** : StandardScaler pour la plupart des algorithmes
- **Encodage** : LabelEncoder pour les variables ordinales
- **Feature Engineering** : Cr√©ation de features m√©tier

### Mod√©lisation
- **Validation crois√©e** : Toujours utiliser CV pour l'√©valuation
- **Hyperparameter tuning** : Optimisation syst√©matique
- **Ensemble methods** : Combinaison de mod√®les
- **Feature selection** : R√©duction de la dimensionnalit√©

### √âvaluation
- **M√©triques appropri√©es** : Choix selon le probl√®me
- **Validation temporelle** : Pour les donn√©es temporelles
- **Test A/B** : Validation en conditions r√©elles
- **Monitoring** : Suivi des performances en production

## üöÄ D√©ploiement

### MLOps
```python
import mlflow
import mlflow.sklearn
import joblib

# Sauvegarde du mod√®le
mlflow.sklearn.log_model(
    grid_search.best_estimator_, 
    "model",
    registered_model_name="best_model"
)

# Sauvegarde des pr√©processeurs
joblib.dump(scaler, 'scaler.pkl')
joblib.dump(le, 'label_encoder.pkl')
```

### API de pr√©diction
```python
from fastapi import FastAPI
import joblib
import pandas as pd

app = FastAPI()

# Chargement du mod√®le
model = joblib.load('best_model.pkl')
scaler = joblib.load('scaler.pkl')

@app.post("/predict")
async def predict(data: dict):
    # Pr√©processing
    df = pd.DataFrame([data])
    df_scaled = scaler.transform(df)
    
    # Pr√©diction
    prediction = model.predict(df_scaled)
    probability = model.predict_proba(df_scaled)
    
    return {
        "prediction": int(prediction[0]),
        "probability": float(probability[0].max())
    }
```

## üìö Ressources d'apprentissage

### Cours recommand√©s
- **Coursera** : Machine Learning (Stanford)
- **Udacity** : Machine Learning Engineer Nanodegree
- **Fast.ai** : Practical Machine Learning for Coders

### Livres essentiels
- **Hands-On Machine Learning** (Aur√©lien G√©ron)
- **The Elements of Statistical Learning** (Hastie, Tibshirani, Friedman)
- **Pattern Recognition and Machine Learning** (Christopher Bishop)

### Pratique
- **Kaggle** : Comp√©titions et datasets
- **Scikit-learn** : Documentation officielle
- **Papers with Code** : Recherche et impl√©mentations

---

*Derni√®re mise √† jour : {{ git_revision_date_localized }}*
