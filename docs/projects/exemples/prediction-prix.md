---
tags:
  - machine-learning
  - regression
  - xgboost
  - feature-engineering
  - real-estate
  - mlflow
---

# üè† Pr√©diction de prix immobiliers avec XGBoost

![Badge de statut](https://img.shields.io/badge/statut-termin√©-success)
![Badge Technologies](https://img.shields.io/badge/xgboost-1.7.0-orange)
![Badge Performance](https://img.shields.io/badge/rmse-0.15-green)
![Badge Dataset](https://img.shields.io/badge/dataset-50K%20properties-blue)

## üéØ Contexte et Objectifs

### Probl√®me √† r√©soudre
D√©veloppement d'un mod√®le de pr√©diction de prix immobiliers pour aider les acheteurs et vendeurs √† estimer la valeur d'un bien immobilier.

### Objectifs
- **Objectif principal** : Pr√©dire le prix d'un bien immobilier avec une erreur < 20%
- **Objectifs secondaires** : Identifier les facteurs les plus influents sur le prix
- **M√©triques de succ√®s** : RMSE < 0.2, R¬≤ > 0.85

### Contexte m√©tier
- **Secteur** : Immobilier / Fintech
- **Utilisateurs** : Acheteurs, Vendeurs, Agents immobiliers
- **Impact attendu** : R√©duction de 30% du temps d'estimation

## üìä Donn√©es et Sources

### Sources de donn√©es
- **Source principale** : Donn√©es publiques immobili√®res
- **Format** : CSV (propri√©t√©s + prix)
- **Taille** : 50,000 propri√©t√©s
- **P√©riode** : 2020-2024
- **Fr√©quence** : Mise √† jour mensuelle

### Qualit√© des donn√©es
- **Compl√©tude** : 88% de compl√©tude
- **Coh√©rence** : Validation des prix avec les transactions
- **Exactitude** : V√©rification avec les notaires
- **Actualit√©** : Donn√©es r√©centes et repr√©sentatives

### Variables disponibles
| Variable | Type | Description | Importance |
|----------|------|-------------|------------|
| surface | Num√©rique | Surface en m¬≤ | Haute |
| nb_pieces | Num√©rique | Nombre de pi√®ces | Haute |
| nb_chambres | Num√©rique | Nombre de chambres | Haute |
| etage | Num√©rique | √âtage | Moyenne |
| ascenseur | Binaire | Pr√©sence d'ascenseur | Moyenne |
| parking | Binaire | Place de parking | Moyenne |
| balcon | Binaire | Balcon/terrasse | Faible |
| quartier | Cat√©gorielle | Quartier | Haute |
| type_bien | Cat√©gorielle | Type de bien | Haute |

## üî¨ M√©thodologie

### 1. Analyse exploratoire des donn√©es (EDA)
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Chargement des donn√©es
df = pd.read_csv('real_estate_data.csv')

# Statistiques descriptives
print(df.describe())

# Visualisation de la distribution des prix
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.hist(df['prix'], bins=50, alpha=0.7)
plt.title('Distribution des prix')
plt.xlabel('Prix (‚Ç¨)')
plt.ylabel('Fr√©quence')

plt.subplot(1, 2, 2)
plt.hist(np.log(df['prix']), bins=50, alpha=0.7)
plt.title('Distribution des prix (log)')
plt.xlabel('Log(Prix)')
plt.ylabel('Fr√©quence')
plt.show()

# Corr√©lation avec la surface
plt.figure(figsize=(10, 6))
plt.scatter(df['surface'], df['prix'], alpha=0.5)
plt.title('Prix vs Surface')
plt.xlabel('Surface (m¬≤)')
plt.ylabel('Prix (‚Ç¨)')
plt.show()
```

### 2. Pr√©processing
```python
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split

# Nettoyage des donn√©es
df = df.dropna()
df = df[df['prix'] > 0]  # Suppression des prix n√©gatifs
df = df[df['surface'] > 0]  # Suppression des surfaces n√©gatives

# Transformation logarithmique du prix
df['log_prix'] = np.log(df['prix'])

# Encodage des variables cat√©gorielles
le_quartier = LabelEncoder()
le_type = LabelEncoder()

df['quartier_encoded'] = le_quartier.fit_transform(df['quartier'])
df['type_encoded'] = le_type.fit_transform(df['type_bien'])

# S√©lection des features
features = ['surface', 'nb_pieces', 'nb_chambres', 'etage', 
           'ascenseur', 'parking', 'balcon', 'quartier_encoded', 'type_encoded']
X = df[features]
y = df['log_prix']

# Division train/validation/test
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
```

### 3. Feature Engineering
```python
# Cr√©ation de nouvelles features
def create_features(df):
    # Prix au m¬≤
    df['prix_m2'] = df['prix'] / df['surface']
    
    # Ratio chambres/pi√®ces
    df['ratio_chambres_pieces'] = df['nb_chambres'] / df['nb_pieces']
    
    # Surface par pi√®ce
    df['surface_par_piece'] = df['surface'] / df['nb_pieces']
    
    # Indicateur de luxe (surface > 100m¬≤ et √©tage > 5)
    df['luxe'] = ((df['surface'] > 100) & (df['etage'] > 5)).astype(int)
    
    # Indicateur de r√©novation (bien r√©cent)
    df['renove'] = (df['annee_construction'] > 2010).astype(int)
    
    return df

# Application du feature engineering
df = create_features(df)

# S√©lection des features finales
final_features = ['surface', 'nb_pieces', 'nb_chambres', 'etage', 
                 'ascenseur', 'parking', 'balcon', 'quartier_encoded', 
                 'type_encoded', 'prix_m2', 'ratio_chambres_pieces', 
                 'surface_par_piece', 'luxe', 'renove']
```

### 4. Mod√©lisation avec XGBoost
```python
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score
import mlflow
import mlflow.xgboost

# Configuration MLflow
mlflow.set_experiment("real_estate_prediction")

with mlflow.start_run():
    # Configuration du mod√®le
    params = {
        'n_estimators': 1000,
        'max_depth': 6,
        'learning_rate': 0.1,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'random_state': 42
    }
    
    # Entra√Ænement du mod√®le
    model = xgb.XGBRegressor(**params)
    model.fit(X_train, y_train)
    
    # Pr√©dictions
    y_pred_train = model.predict(X_train)
    y_pred_val = model.predict(X_val)
    y_pred_test = model.predict(X_test)
    
    # Calcul des m√©triques
    rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))
    rmse_val = np.sqrt(mean_squared_error(y_val, y_pred_val))
    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))
    
    r2_train = r2_score(y_train, y_pred_train)
    r2_val = r2_score(y_val, y_pred_val)
    r2_test = r2_score(y_test, y_pred_test)
    
    # Logging des m√©triques
    mlflow.log_params(params)
    mlflow.log_metric("rmse_train", rmse_train)
    mlflow.log_metric("rmse_val", rmse_val)
    mlflow.log_metric("rmse_test", rmse_test)
    mlflow.log_metric("r2_train", r2_train)
    mlflow.log_metric("r2_val", r2_val)
    mlflow.log_metric("r2_test", r2_test)
    
    # Sauvegarde du mod√®le
    mlflow.xgboost.log_model(model, "model")
    
    print(f"RMSE Train: {rmse_train:.3f}")
    print(f"RMSE Validation: {rmse_val:.3f}")
    print(f"RMSE Test: {rmse_test:.3f}")
    print(f"R¬≤ Train: {r2_train:.3f}")
    print(f"R¬≤ Validation: {r2_val:.3f}")
    print(f"R¬≤ Test: {r2_test:.3f}")
```

### 5. Optimisation des hyperparam√®tres
```python
from sklearn.model_selection import GridSearchCV

# Grille de param√®tres
param_grid = {
    'n_estimators': [500, 1000, 1500],
    'max_depth': [4, 6, 8],
    'learning_rate': [0.05, 0.1, 0.15],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}

# Recherche par grille
grid_search = GridSearchCV(
    xgb.XGBRegressor(random_state=42),
    param_grid,
    cv=5,
    scoring='neg_mean_squared_error',
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

# Meilleurs param√®tres
print("Meilleurs param√®tres:", grid_search.best_params_)
print("Meilleur score:", grid_search.best_score_)

# Mod√®le optimis√©
best_model = grid_search.best_estimator_
```

### 6. √âvaluation et interpr√©tation
```python
# Importance des features
feature_importance = model.feature_importances_
feature_names = X.columns

# Tri par importance
importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': feature_importance
}).sort_values('importance', ascending=False)

# Visualisation de l'importance
plt.figure(figsize=(10, 8))
sns.barplot(data=importance_df.head(10), x='importance', y='feature')
plt.title('Importance des features')
plt.xlabel('Importance')
plt.show()

# Pr√©dictions sur des exemples
def predict_price(model, surface, nb_pieces, nb_chambres, etage, 
                 ascenseur, parking, balcon, quartier, type_bien):
    # Cr√©ation d'un DataFrame avec les features
    data = pd.DataFrame({
        'surface': [surface],
        'nb_pieces': [nb_pieces],
        'nb_chambres': [nb_chambres],
        'etage': [etage],
        'ascenseur': [ascenseur],
        'parking': [parking],
        'balcon': [balcon],
        'quartier_encoded': [quartier],
        'type_encoded': [type_bien]
    })
    
    # Pr√©diction
    log_price = model.predict(data)[0]
    price = np.exp(log_price)
    
    return price

# Exemple de pr√©diction
predicted_price = predict_price(
    model, surface=80, nb_pieces=3, nb_chambres=2, etage=3,
    ascenseur=1, parking=1, balcon=0, quartier=5, type_bien=1
)
print(f"Prix pr√©dit: {predicted_price:,.0f} ‚Ç¨")
```

## üìà R√©sultats et M√©triques

### Performance du mod√®le
| M√©trique | Valeur | Baseline | Am√©lioration |
|----------|--------|----------|--------------|
| RMSE | 0.15 | 0.25 | +40% |
| R¬≤ | 0.87 | 0.65 | +22% |
| MAE | 0.12 | 0.20 | +40% |
| MAPE | 15.2% | 28.5% | +13.3% |

### Performance par type de bien
| Type de bien | RMSE | R¬≤ | Nombre d'√©chantillons |
|--------------|------|----|---------------------|
| Appartement | 0.14 | 0.89 | 30,000 |
| Maison | 0.16 | 0.85 | 15,000 |
| Studio | 0.13 | 0.91 | 5,000 |

### Importance des features
| Feature | Importance | Description |
|---------|------------|-------------|
| surface | 0.35 | Surface en m¬≤ |
| quartier | 0.25 | Quartier |
| nb_pieces | 0.15 | Nombre de pi√®ces |
| type_bien | 0.10 | Type de bien |
| etage | 0.08 | √âtage |
| parking | 0.04 | Place de parking |
| ascenseur | 0.03 | Ascenseur |

## üöÄ D√©ploiement

### Architecture de d√©ploiement
- **Environnement** : Docker + AWS ECS
- **API** : FastAPI avec documentation automatique
- **Base de donn√©es** : PostgreSQL
- **Monitoring** : MLflow + CloudWatch
- **CI/CD** : GitHub Actions

### Code de d√©ploiement
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import joblib
import numpy as np
import pandas as pd

app = FastAPI(title="Real Estate Price Prediction API")

# Chargement du mod√®le
model = joblib.load('xgboost_model.pkl')
scaler = joblib.load('scaler.pkl')

class PropertyInput(BaseModel):
    surface: float
    nb_pieces: int
    nb_chambres: int
    etage: int
    ascenseur: bool
    parking: bool
    balcon: bool
    quartier: str
    type_bien: str

class PriceOutput(BaseModel):
    predicted_price: float
    confidence_interval: dict
    feature_importance: dict

@app.post("/predict", response_model=PriceOutput)
async def predict_price(property_data: PropertyInput):
    try:
        # Pr√©processing des donn√©es
        data = pd.DataFrame([property_data.dict()])
        
        # Encodage des variables cat√©gorielles
        data['quartier_encoded'] = le_quartier.transform(data['quartier'])
        data['type_encoded'] = le_type.transform(data['type_bien'])
        
        # S√©lection des features
        features = ['surface', 'nb_pieces', 'nb_chambres', 'etage', 
                   'ascenseur', 'parking', 'balcon', 'quartier_encoded', 'type_encoded']
        X = data[features]
        
        # Pr√©diction
        log_price = model.predict(X)[0]
        predicted_price = np.exp(log_price)
        
        # Intervalle de confiance (approximatif)
        confidence_interval = {
            'lower': predicted_price * 0.85,
            'upper': predicted_price * 1.15
        }
        
        # Importance des features pour cette pr√©diction
        feature_importance = dict(zip(features, model.feature_importances_))
        
        return PriceOutput(
            predicted_price=float(predicted_price),
            confidence_interval=confidence_interval,
            feature_importance=feature_importance
        )
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.get("/health")
async def health_check():
    return {"status": "healthy", "model_loaded": True}
```

## üìä Visualisations

### Distribution des erreurs
```python
# Calcul des erreurs
errors = y_test - y_pred_test
errors_percent = (errors / y_test) * 100

# Visualisation des erreurs
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.hist(errors, bins=50, alpha=0.7)
plt.title('Distribution des erreurs')
plt.xlabel('Erreur (log prix)')
plt.ylabel('Fr√©quence')

plt.subplot(1, 2, 2)
plt.hist(errors_percent, bins=50, alpha=0.7)
plt.title('Distribution des erreurs (%)')
plt.xlabel('Erreur (%)')
plt.ylabel('Fr√©quence')
plt.show()
```

### Pr√©dictions vs Vraies valeurs
```python
# Scatter plot des pr√©dictions
plt.figure(figsize=(10, 8))
plt.scatter(y_test, y_pred_test, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Vraies valeurs (log prix)')
plt.ylabel('Pr√©dictions (log prix)')
plt.title('Pr√©dictions vs Vraies valeurs')
plt.show()
```

## üîó Liens et ressources

### Code source
- **Repository GitHub** : [github.com/loick-dernoncourt/real-estate-prediction](https://github.com/loick-dernoncourt/real-estate-prediction)
- **Notebooks Jupyter** : [github.com/loick-dernoncourt/real-estate-prediction/tree/main/notebooks](https://github.com/loick-dernoncourt/real-estate-prediction/tree/main/notebooks)

### D√©monstrations
- **D√©mo interactive** : [real-estate-demo.example.com](https://real-estate-demo.example.com)
- **API Documentation** : [real-estate-api.example.com/docs](https://real-estate-api.example.com/docs)
- **Dashboard** : [real-estate-dashboard.example.com](https://real-estate-dashboard.example.com)

### Documentation
- **Rapport technique** : [real-estate-report.example.com](https://real-estate-report.example.com)
- **Pr√©sentation** : [real-estate-slides.example.com](https://real-estate-slides.example.com)
- **Article de blog** : [blog.example.com/real-estate-prediction](https://blog.example.com/real-estate-prediction)

## üéØ Prochaines √©tapes

### Am√©liorations pr√©vues
- [ ] Int√©gration de donn√©es g√©ographiques (GIS)
- [ ] Mod√®le de pr√©diction des tendances
- [ ] Analyse de la valeur ajout√©e des r√©novations
- [ ] Pr√©diction des prix de location

### Technologies √† explorer
- [ ] LightGBM pour de meilleures performances
- [ ] SHAP pour l'explicabilit√©
- [ ] Time series pour les tendances
- [ ] Deep learning pour les patterns complexes

---

*Derni√®re mise √† jour : {{ git_revision_date_localized }}*
